LTX-2 is an open-source artificial intelligence video foundation model released by Lightricks in October 2025. It creates videos based on user prompts and was preceded by LTX Video, which was released in 2024 as the company's first text-to-video model.


== History ==


=== Origins: LTX Video (2024–2025) ===
In November 2024 Lightricks publicly released its first text-to-video model, LTX Video. It was a 2-billion parameter model, available as open source.
In May 2025 Lightricks launched LTXV-13b, a version with 13-billion parameters. Two months later, the model broke the 60 second barrier for generated video.


=== Release of LTX-2 (2025) ===
In October 2025 Lightricks announced its latest model, and renamed it LTX-2. The model was described as capable of generating synchronized audio and video at native 4K resolution and up to 50 frames per second (fps), using a variety of conditions and prompts, including text-to-video and image-to-video.
Google highlighted the fact that LTX-2 was trained on its infrastructure, and saying it was "The first open source AI video generation model, powered by Google Cloud".
Upon its release it was ranked in the top-3 models for image-to-video creation by Artificial Analysis, behind Kling 3.5 by Kling AI and Veo 3.1 by Google. Its text-to-image option was ranked 7th


== Technical features ==


=== Advancements over LTX Video ===
LTX-2 builds upon the LTX Video architecture with several major improvements:

Unified audio-video generation producing synchronized dialogue, ambience, and motion
Native 4K rendering
50-fps output for cinematic motion
Three operational modes (Fast, Pro, Ultra)
More efficient diffusion pipelines enabling high fidelity on consumer GPUs


=== Core capabilities ===
Text-to-video generation
Image-to-video generation
Multimodal audiovisual synthesis
High-resolution spatial and temporal coherence
Configurable quality/performance settings
Open-source distribution of weights and datasets


== Reception ==
Initial reception to LTX-2 was broadly positive, with several technology and media outlets highlighting its open-source approach and multimodal capabilities. Open Source For You described LTX-2 as “one of the first AI video systems to combine 4K output, synchronized audio, and an open model release,” noting that it positioned Lightricks as a significant competitor to proprietary systems such as OpenAI's Sora and Google's Veo.
IEA Green said that the model “could rewrite the AI filmmaking game,” emphasizing that its 50-fps rendering and unified audio-video generation made it suitable for professional studios and independent creators alike.
AI News characterized LTX-2 as a “major step forward in the democratization of cinematic-quality video generation,” praising its consumer-grade hardware efficiency and multi-tier generation modes, while also noting ongoing challenges in long-form temporal stability.
FinancialContent reported strong interest among creative agencies, attributing the attention to Lightricks’ decision to release model weights and datasets, which reviewers said enabled “a level of transparency not typically seen in commercial AI video models.”
Some early reviewers also pointed out quality limitations. The Ray3 technical review noted occasional inconsistencies in lip-sync and motion tracking during long scenes, though it stated these were “in line with the challenges faced by all current AI video diffusion models” and expected to improve with continued iteration.


== See also ==
Sora (text-to-video model)
Veo (text-to-video model)
Generative artificial intelligence


== References ==


== External links ==
Official website
Official Documentation of LTX-2
Official Github Repo