Semantic similarity is a metric defined over a set of documents or terms, where the idea of distance between items is based on the likeness of their meaning or semantic content as opposed to lexicographical similarity. These are mathematical tools used to estimate the strength of the semantic relationship between units of language, concepts or instances, through a numerical description obtained according to the comparison of information supporting their meaning or describing their nature. The term semantic similarity is often confused with semantic relatedness. Semantic relatedness includes any relation between two terms, while semantic similarity only includes "is a" relations.
For example, "car" is similar to "bus", but is also related to "road" and "driving".
Computationally, semantic similarity can be estimated by defining a topological similarity, by using ontologies to define the distance between terms/concepts. For example, a naive metric for the comparison of concepts ordered in a partially ordered set and represented as nodes of a directed acyclic graph (e.g., a taxonomy), would be the shortest-path linking the two concept nodes. Based on text analyses, semantic relatedness between units of language (e.g., words, sentences) can also be estimated using statistical means such as a vector space model to correlate words and textual contexts from a suitable text corpus. The evaluation of the proposed semantic similarity / relatedness measures are evaluated through two main ways. The former is based on the use of datasets designed by experts and composed of word pairs with semantic similarity / relatedness degree estimation. The second way is based on the integration of the measures inside specific applications such as information retrieval, recommender systems, natural language processing, etc.


== Terminology ==
The concept of semantic similarity is more specific than semantic relatedness, as the latter includes concepts as antonymy and meronymy, while similarity does not. However, much of the literature uses these terms interchangeably, along with terms like semantic distance. In essence, semantic similarity, semantic distance, and semantic relatedness all mean, "How much does term A have to do with term B?" The answer to this question is usually a number between −1 and 1, or between 0 and 1, where 1 signifies extremely high similarity.


== Visualization ==
An intuitive way of visualizing the semantic similarity of terms is by grouping together terms which are closely related and spacing wider apart the ones which are distantly related. This is also common in practice for mind maps and concept maps.
A more direct way of visualizing the semantic similarity of two linguistic items can be seen with the Semantic Folding approach. In this approach a linguistic item such as a term or a text can be represented by generating a pixel for each of its active semantic features in e.g. a 128 x 128 grid. This allows for a direct visual comparison of the semantics of two items by comparing image representations of their respective feature sets.


== Applications ==


=== In biomedical informatics ===
Semantic similarity measures have been applied and developed in biomedical ontologies.
They are mainly used to compare genes and proteins based on the similarity of their functions rather than on their sequence similarity,
but they are also being extended to other bioentities, such as diseases.
These comparisons can be done using tools freely available on the web:

ProteInOn can be used to find interacting proteins, find assigned GO terms and calculate the functional semantic similarity of UniProt proteins and to get the information content and calculate the functional semantic similarity of GO terms.
CMPSim provides a functional similarity measure between chemical compounds and metabolic pathways using ChEBI based semantic similarity measures.
CESSM provides a tool for the automated evaluation of GO-based semantic similarity measures.


=== In geoinformatics ===
Similarity is also applied in geoinformatics to find similar geographic features or feature types:

SIM-DL similarity server can be used to compute similarities between concepts stored in geographic feature type ontologies.
Similarity Calculator can be used to compute how well related two geographic concepts are in the Geo-Net-PT ontology.
The OSM semantic network can be used to compute the semantic similarity of tags in OpenStreetMap.


=== In computational linguistics ===
Several metrics use WordNet, a manually constructed lexical database of English words. Despite the advantages of having human supervision in constructing the database, since the words are not automatically learned the database cannot measure relatedness between multi-word term, non-incremental vocabulary.


=== In natural language processing ===
Natural language processing (NLP) is a field of computer science and linguistics. Sentiment analysis, Natural language understanding and Machine translation (Automatically translate text from one human language to another) are a few of the major areas where it is being used. For example, knowing one information resource in the internet, it is often of immediate interest to find similar resources. The Semantic Web provides semantic extensions to find similar data by content and not just by arbitrary descriptors. Deep learning methods have become an accurate way to gauge semantic similarity between two text passages, in which each passage is first embedded into a continuous vector representation.


=== In ontology matching ===
Semantic similarity plays a crucial role in ontology alignment, which aims to establish correspondences between entities from different ontologies. It involves quantifying the degree of similarity between concepts or terms using the information present in the ontology for each entity, such as labels, descriptions, and hierarchical relations to other entities. Traditional metrics used in ontology matching are based on a lexical similarity between features of the entities, such as using the Levenshtein distance to measure the edit distance between entity labels. However, it is difficult to capture the semantic similarity between entities using these metrics. For example, when comparing two ontologies describing conferences, the entities "Contribution" and "Paper" may have high semantic similarity since they share the same meaning. Nonetheless, due to their lexical differences, lexicographical similarity alone cannot establish this alignment. To capture these semantic similarities, embeddings are being adopted in ontology matching. By encoding semantic relationships and contextual information, embeddings enable the calculation of similarity scores between entities based on the proximity of their vector representations in the embedding space. This approach allows for efficient and accurate matching of ontologies since embeddings can model semantic differences in entity naming, such as homonymy, by assigning different embeddings to the same word based on different contexts.


== Measures ==


=== Topological similarity ===
There are essentially two types of approaches that calculate topological similarity between ontological concepts:

Edge-based: which use the edges and their types as the data source;
Node-based: in which the main data sources are the nodes and their properties.
Other measures calculate the similarity between ontological instances:

Pairwise: measure functional similarity between two instances by combining the semantic similarities of the concepts they represent
Groupwise: calculate the similarity directly not combining the semantic similarities of the concepts they represent
Some examples:


==== Edge-based ====
Pekar et al.
Cheng and Cline
Wu et al.
Del Pozo et al.
IntelliGO: Benabderrahmane et al.


==== Node-based ====
Resnik
based on the notion of information content. The information content of a concept (term or word) is the logarithm of the probability of finding the concept in a given corpus.
only considers the information content of lowest common subsumer (lcs). A lowest common subsumer is a concept in a lexical taxonomy (e.g. WordNet), which has the shortest distance from the two concepts compared. For example, animal and mammal both are the subsumers of cat and dog, but mammal is lower subsumer than animal for them.
Lin
based on Resnik's similarity.
considers the information content of lowest common subsumer (lcs) and the two compared concepts.
Maguitman, Menczer, Roinestad and Vespignani
Generalizes Lin's similarity to arbitrary ontologies (graphs).
Jiang and Conrath
based on Resnik's similarity.
considers the information content of lowest common subsumer (lcs) and the two compared concepts to calculate the distance between the two concepts. The distance is later used in computing the similarity measure.
Align, Disambiguate, and Walk: Random walks on Semantic Networks


==== Node-and-relation-content-based ====
applicable to ontology
consider properties (content) of nodes
consider types (content) of relations
based on eTVSM
based on Resnik's similarity


==== Pairwise ====
maximum of the pairwise similarities
composite average in which only the best-matching pairs are considered (best-match average)


==== Groupwise ====
Jaccard index


=== Statistical similarity ===
Statistical similarity approaches can be learned from data, or predefined. Similarity learning can often outperform predefined similarity measures. Broadly speaking, these approaches build a statistical model of documents, and use it to estimate similarity.

LSA (latent semantic analysis): (+) vector-based, adds vectors to measure multi-word terms; (−) non-incremental vocabulary, long pre-processing times
PMI (pointwise mutual information): (+) large vocab, because it uses any search engine (like Google); (−) cannot measure relatedness between whole sentences or documents
SOC-PMI (second-order co-occurrence pointwise mutual information): (+) sort lists of important neighbor words from a large corpus; (−) cannot measure relatedness between whole sentences or documents
GLSA (generalized latent semantic analysis): (+) vector-based, adds vectors to measure multi-word terms; (−) non-incremental vocabulary, long pre-processing times
ICAN (incremental construction of an associative network): (+) incremental, network-based measure, good for spreading activation, accounts for second-order relatedness; (−) cannot measure relatedness between multi-word terms, long pre-processing times
NGD (normalized Google distance): (+) large vocab, because it uses any search engine (like Google); (−) can measure relatedness between whole sentences or documents but the larger the sentence or document, the more ingenuity is required (Cilibrasi & Vitanyi, 2007).
TSS (Twitter semantic similarity): large vocab, because it use online tweets from Twitter to compute the similarity. It has high temporary resolution that allows to capture high frequency events. Open source
NCD (normalized compression distance)
ESA (explicit semantic analysis) based on Wikipedia and the ODP
SSA (salient semantic analysis) which indexes terms using salient concepts found in their immediate context.
n° of Wikipedia (noW), inspired by the game Six Degrees of Wikipedia, is a distance metric based on the hierarchical structure of Wikipedia. A directed-acyclic graph is first constructed and later, Dijkstra's shortest path algorithm is employed to determine the noW value between two terms as the geodesic distance between the corresponding topics (i.e. nodes) in the graph.
VGEM (vector generation of an explicitly defined multidimensional semantic space): (+) incremental vocab, can compare multi-word terms (−) performance depends on choosing specific dimensions
SimRank
NASARI: Sparse vector representations constructed by applying the hypergeometric distribution over the Wikipedia corpus in combination with BabelNet taxonomy. Cross-lingual similarity is currently also possible thanks to the multilingual and unified extension.


=== Semantics-based similarity ===
Marker passing: Combining lexical decomposition for automated ontology creation and marker passing, the approach of Fähndrich et al. introduces a new type of semantic similarity measure. Here markers are passed from the two target concepts carrying an amount of activation. This activation might increase or decrease depending on the relations weight with which the concepts are connected. This combines edge and node based approaches and includes connectionist reasoning with symbolic information.
Good common subsumer (GCS)-based semantic similarity measure


=== Semantics similarity networks ===
A semantic similarity network (SSN) is a special form of semantic network. designed to represent concepts and their semantic similarity. Its main contribution is reducing the complexity of calculating semantic distances. Bendeck (2004, 2008) introduced the concept of semantic similarity networks (SSN) as the specialization of a semantic network to measure semantic similarity from ontological representations. Implementations include genetic information handling.


=== Gold standards ===
Researchers have collected datasets with similarity judgements on pairs of words, which are used to evaluate the cognitive plausibility of computational measures. The golden standard up to today is an old 65 word list where humans have judged the word similarity.

RG65
MC30
WordSim353


== See also ==

Analogy
Componential analysis
Coherence (linguistics)
Levenshtein distance
Semantic differential
Semantic similarity network
Terminology extraction
Word2vec
tf-idf – Estimate of the importance of a word in a documentPages displaying short descriptions of redirect targets


== References ==


== Sources ==
Chicco, D; Masseroli, M (2015). "Software suite for gene and protein annotation prediction and similarity search". IEEE/ACM Transactions on Computational Biology and Bioinformatics. 12 (4): 837–843. Bibcode:2015ITCBB..12..837C. doi:10.1109/TCBB.2014.2382127. hdl:11311/959408. PMID 26357324. S2CID 14714823.
Cilibrasi, R.L. & Vitanyi, P.M.B. (2007). "The Google Similarity Distance". IEEE Transactions on Knowledge and Data Engineering. 19 (3): 370–383. arXiv:cs/0412098. Bibcode:2007ITKDE..19..370C. doi:10.1109/TKDE.2007.48. S2CID 59777.
Dumais, S (2003). "Data-driven approaches to information access". Cognitive Science. 27 (3): 491–524. doi:10.1207/s15516709cog2703_7.
Gabrilovich, E. and Markovitch, S. (2007). Computing Semantic Relatedness using Wikipedia-based Explicit Semantic Analysis, Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI), Hyderabad, India, January 2007.
Lee, M. D., Pincombe, B., & Welsh, M. (2005). An empirical evaluation of models of text document similarity. In B. G. Bara & L. Barsalou & M. Bucciarelli (Eds.), 27th Annual Meeting of the Cognitive Science Society, CogSci2005 (pp. 1254–1259). Austin, Tx: The Cognitive Science Society, Inc.
Lemaire, B., & Denhiére, G. (2004). Incremental construction of an associative network from a corpus. In K. D. Forbus & D. Gentner & T. Regier (Eds.), 26th Annual Meeting of the Cognitive Science Society, CogSci2004. Hillsdale, NJ: Lawrence Erlbaum Publisher.
Lindsey, R.; Veksler, V.D.; Grintsvayg, A.; Gray, W.D. (2007). "The Effects of Corpus Selection on Measuring Semantic Relatedness" (PDF). Proceedings of the 8th International Conference on Cognitive Modeling, Ann Arbor, MI.
Navigli, R., Lapata, M. (2010). "An Experimental Study of Graph Connectivity for Unsupervised Word Sense Disambiguation". IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 32(4), IEEE Press, 2010, pp. 678–692.
Veksler, V.D. & Gray, W.D. (2006). "Test Case Selection for Evaluating Measures of Semantic Distance" (PDF). Proceedings of the 28th Annual Meeting of the Cognitive Science Society, CogSci2006.
Wong, W., Liu, W. & Bennamoun, M. (2008) Featureless Data Clustering. In: M. Song and Y. Wu; Handbook of Research on Text and Web Mining Technologies; IGI Global. ISBN 978-1-59904-990-8 (the use of NGD and noW for term and URI clustering)


== External links ==
List of related literature


=== Survey articles ===
Conference article: C. d'Amato, S. Staab, N. Fanizzi. 2008. On the Influence of Description Logics Ontologies on Conceptual Similarity. In Proceedings of the 16th international conference on Knowledge Engineering: Practice and Patterns Pages 48 – 63. Acitrezza, Italy, Springer-Verlag
Journal article on the more general topic of relatedness, also including similarity: Z. Zhang, A. Gentile, F. Ciravegna. 2013. Recent advances in methods of lexical semantic relatedness – a survey. Natural Language Engineering 19 (4), 411–479, Cambridge University Press
Book: S. Harispe, S. Ranwez, S. Janaqi, J. Montmain. 2015. Semantic Similarity from Natural Language and Ontology Analysis, Morgan & Claypool Publishers.