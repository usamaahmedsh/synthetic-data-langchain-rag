In probability theory and mathematical physics, a random matrix is a matrix-valued random variable—that is, a matrix in which some or all of its entries are sampled randomly from a probability distribution. Random matrix theory (RMT) is the study of properties of random matrices, often as they become large. RMT provides techniques like mean-field theory, diagrammatic methods, the cavity method, or the replica method to compute quantities like traces, spectral densities, or scalar products between eigenvectors. Many physical phenomena, such as the spectrum of nuclei of heavy atoms, the thermal conductivity of a lattice, or the emergence of quantum chaos, can be modeled mathematically as problems concerning large, random matrices.


== History ==

Random matrix theory first gained attention beyond mathematics literature in the context of nuclear physics. Experiments by Enrico Fermi and others demonstrated evidence that individual nucleons cannot be approximated to move independently, leading Niels Bohr to formulate the idea of a compound nucleus. Because there was no knowledge of direct nucleon-nucleon interactions, Eugene Wigner and Leonard Eisenbud approximated that the nuclear Hamiltonian could be modeled as a random matrix. For larger atoms, the distribution of the energy eigenvalues of the Hamiltonian could be computed in order to approximate scattering cross sections by invoking the Wishart distribution.


== Applications ==


=== Physics ===
In nuclear physics, random matrices were introduced by Eugene Wigner to model the nuclei of heavy atoms. Wigner postulated that the spacings between the lines in the spectrum of a heavy atom nucleus should resemble the spacings between the eigenvalues of a random matrix, and should depend only on the symmetry class of the underlying evolution. In solid-state physics, random matrices model the behaviour of large disordered Hamiltonians in the mean-field approximation.
In quantum chaos, the Bohigas–Giannoni–Schmit (BGS) conjecture asserts that the spectral statistics of quantum systems whose classical counterparts exhibit chaotic behaviour are described by random matrix theory.
In quantum optics, transformations described by random unitary matrices are crucial for demonstrating the advantage of quantum over classical computation (see, e.g., the boson sampling model). Moreover, such random unitary transformations can be directly implemented in an optical circuit, by mapping their parameters to optical circuit components (that is beam splitters and phase shifters).


=== Mathematical statistics and numerical analysis ===
In multivariate statistics, random matrices were introduced by John Wishart, who sought to estimate covariance matrices of large samples.  Chernoff-, Bernstein-, and Hoeffding-type inequalities can typically be strengthened when applied to the maximal eigenvalue (i.e. the eigenvalue of largest magnitude) of a finite sum of random Hermitian matrices. Random matrix theory is used to study the spectral properties of random matrices—such as sample covariance matrices—which is of particular interest in high-dimensional statistics. Random matrix theory also saw applications in neural networks and deep learning, with recent work utilizing random matrices to show that hyper-parameter tunings can be cheaply transferred between large neural networks without the need for re-training.
In numerical analysis, random matrices have been used since the work of John von Neumann and Herman Goldstine to describe computation errors in operations such as matrix multiplication. Although random entries are traditional "generic" inputs to an algorithm, the concentration of measure associated with random matrix distributions implies that random matrices will not test large portions of an algorithm's input space.


=== Number theory ===
In number theory, the distribution of zeros of the Riemann zeta function (and other L-functions) is modeled by the distribution of eigenvalues of certain random matrices. The connection was first discovered by Hugh Montgomery and Freeman Dyson. It is connected to the Hilbert–Pólya conjecture.


=== Free probability ===
The relation of free probability with random matrices is a key reason for the wide use of free probability in other subjects. Voiculescu introduced the concept of freeness around 1983 in an operator algebraic context; at the beginning there was no relation at all with random matrices. This connection was only revealed later in 1991 by Voiculescu; he was motivated by the fact that the limit distribution which he found in his free central limit theorem had appeared before in Wigner's semi-circle law in the random matrix context. 


=== Computational neuroscience ===
In the field of computational neuroscience, random matrices are increasingly used to model the network of synaptic connections between neurons in the brain.  Dynamical models of neuronal networks with random connectivity matrix were shown to exhibit a phase transition to chaos when the variance of the synaptic weights crosses a critical value, at the limit of infinite system size.  Results on random matrices have also shown that the dynamics of random-matrix models are insensitive to mean connection strength.  Instead, the stability of fluctuations depends on connection strength variation and time to synchrony depends on network topology.
In the analysis of massive data such as fMRI, random matrix theory has been applied in order to perform dimension reduction. When applying an algorithm such as PCA, it is important to be able to select the number of significant components. The criteria for selecting components can be multiple (based on explained variance, Kaiser's method, eigenvalue, etc.). Random matrix theory in this content has its representative the Marchenko-Pastur distribution, which guarantees the theoretical high and low limits of the eigenvalues associated with a random variable covariance matrix. This matrix calculated in this way becomes the null hypothesis that allows one to find the eigenvalues (and their eigenvectors) that deviate from the theoretical random range. The components thus excluded become the reduced dimensional space (see examples in fMRI ).


=== Optimal control ===
In optimal control theory, the evolution of n state variables through time depends at any time on their own values and on the values of k control variables. With linear evolution, matrices of coefficients appear in the state equation (equation of evolution). In some problems the values of the parameters in these matrices are not known with certainty, in which case there are random matrices in the state equation and the problem is known as one of stochastic control. A key result in the case of linear-quadratic control with stochastic matrices is that the certainty equivalence principle does not apply: while in the absence of multiplier uncertainty (that is, with only additive uncertainty) the optimal policy with a quadratic loss function coincides with what would be decided if the uncertainty were ignored, the optimal policy may differ if the state equation contains random coefficients.


=== Computational mechanics ===
In computational mechanics, epistemic uncertainties underlying the lack of knowledge about the physics of the modeled system give rise to mathematical operators associated with the computational model, which are deficient in a certain sense. Such operators lack certain properties linked to unmodeled physics. When such operators are discretized to perform computational simulations, their accuracy is limited by the missing physics. To compensate for this deficiency of the mathematical operator, it is not enough to make the model parameters random, it is necessary to consider a mathematical operator that is random and can thus generate families of computational models in the hope that one of these captures the missing physics. Random matrices have been used in this sense, with applications in vibroacoustics, wave propagations, materials science, fluid mechanics, heat transfer, etc.


=== Engineering ===
Random matrix theory can be applied to the electrical and communications engineering research efforts to study, model and develop Massive Multiple-Input Multiple-Output (MIMO) radio systems.


== Types ==


=== Gaussian ensembles ===

The most-commonly studied random matrix distributions are the Gaussian ensembles: GOE, GUE and GSE. They are often denoted by their Dyson index, β = 1 for GOE, β = 2 for GUE, and β = 4 for GSE. This index counts the number of real components per matrix element.


==== Definitions ====
The Gaussian unitary ensemble 
  
    
      
        
          GUE
        
        (
        n
        )
      
    
    {\displaystyle {\text{GUE}}(n)}
  
 is described by the Gaussian measure with density

  
    
      
        
          
            1
            
              Z
              
                
                  GUE
                
                (
                n
                )
              
            
          
        
        
          e
          
            −
            
              
                n
                2
              
            
            
              t
              r
            
            
              H
              
                2
              
            
          
        
      
    
    {\displaystyle {\frac {1}{Z_{{\text{GUE}}(n)}}}e^{-{\frac {n}{2}}\mathrm {tr} H^{2}}}
  

on the space of 
  
    
      
        n
        ×
        n
      
    
    {\displaystyle n\times n}
  
 Hermitian matrices 
  
    
      
        H
        =
        (
        
          H
          
            i
            j
          
        
        
          )
          
            i
            ,
            j
            =
            1
          
          
            n
          
        
      
    
    {\displaystyle H=(H_{ij})_{i,j=1}^{n}}
  
. Here

  
    
      
        
          Z
          
            
              GUE
            
            (
            n
            )
          
        
        =
        
          2
          
            n
            
              /
            
            2
          
        
        
          
            (
            
              
                π
                n
              
            
            )
          
          
            
              
                1
                2
              
            
            
              n
              
                2
              
            
          
        
      
    
    {\displaystyle Z_{{\text{GUE}}(n)}=2^{n/2}\left({\frac {\pi }{n}}\right)^{{\frac {1}{2}}n^{2}}}
  

is a normalization constant, chosen so that the integral of the density is equal to one. The term unitary refers to the fact that the distribution is invariant under unitary conjugation. The Gaussian unitary ensemble models Hamiltonians lacking time-reversal symmetry.
The Gaussian orthogonal ensemble 
  
    
      
        
          GOE
        
        (
        n
        )
      
    
    {\displaystyle {\text{GOE}}(n)}
  
 is described by the Gaussian measure with density

  
    
      
        
          
            1
            
              Z
              
                
                  GOE
                
                (
                n
                )
              
            
          
        
        
          e
          
            −
            
              
                n
                4
              
            
            
              t
              r
            
            
              H
              
                2
              
            
          
        
      
    
    {\displaystyle {\frac {1}{Z_{{\text{GOE}}(n)}}}e^{-{\frac {n}{4}}\mathrm {tr} H^{2}}}
  

on the space of n × n real symmetric matrices H = (Hij)ni,j=1. Its distribution is invariant under orthogonal conjugation, and it models Hamiltonians with time-reversal symmetry. Equivalently, it is generated by 
  
    
      
        H
        =
        (
        G
        +
        
          G
          
            T
          
        
        )
        
          /
        
        
          
            2
            n
          
        
      
    
    {\displaystyle H=(G+G^{T})/{\sqrt {2n}}}
  
, where 
  
    
      
        G
      
    
    {\displaystyle G}
  
 is an 
  
    
      
        n
        ×
        n
      
    
    {\displaystyle n\times n}
  
 matrix with IID samples from the standard normal distribution.
The Gaussian symplectic ensemble 
  
    
      
        
          GSE
        
        (
        n
        )
      
    
    {\displaystyle {\text{GSE}}(n)}
  
 is described by the Gaussian measure with density

  
    
      
        
          
            1
            
              Z
              
                
                  GSE
                
                (
                n
                )
              
            
          
        
        
          e
          
            −
            n
            
              t
              r
            
            
              H
              
                2
              
            
          
        
      
    
    {\displaystyle {\frac {1}{Z_{{\text{GSE}}(n)}}}e^{-n\mathrm {tr} H^{2}}}
  

on the space of n × n Hermitian quaternionic matrices, e.g. symmetric square matrices composed of quaternions, H = (Hij)ni,j=1. Its distribution is invariant under conjugation by the symplectic group, and it models Hamiltonians with time-reversal symmetry but no rotational symmetry.


==== Basic properties ====
Point correlation functions The ensembles as defined here have Gaussian distributed matrix elements with mean ⟨Hij⟩ = 0, and two-point correlations given by

  
    
      
        ⟨
        
          H
          
            i
            j
          
        
        
          H
          
            m
            n
          
          
            ∗
          
        
        ⟩
        =
        ⟨
        
          H
          
            i
            j
          
        
        
          H
          
            n
            m
          
        
        ⟩
        =
        
          
            1
            n
          
        
        
          δ
          
            i
            m
          
        
        
          δ
          
            j
            n
          
        
        +
        
          
            
              2
              −
              β
            
            
              n
              β
            
          
        
        
          δ
          
            i
            n
          
        
        
          δ
          
            j
            m
          
        
        ,
      
    
    {\displaystyle \langle H_{ij}H_{mn}^{*}\rangle =\langle H_{ij}H_{nm}\rangle ={\frac {1}{n}}\delta _{im}\delta _{jn}+{\frac {2-\beta }{n\beta }}\delta _{in}\delta _{jm},}
  
from which all higher correlations follow by Isserlis' theorem.
The moment generating function for the GOE is
  
    
      
        E
        [
        
          e
          
            t
            r
            (
            V
            H
            )
          
        
        ]
        =
        
          e
          
            
              
                1
                
                  4
                  N
                
              
            
            ‖
            V
            +
            
              V
              
                T
              
            
            
              ‖
              
                F
              
              
                2
              
            
          
        
      
    
    {\displaystyle E[e^{tr(VH)}]=e^{{\frac {1}{4N}}\|V+V^{T}\|_{F}^{2}}}
  
where 
  
    
      
        ‖
        ⋅
        
          ‖
          
            F
          
        
      
    
    {\displaystyle \|\cdot \|_{F}}
  
 is the Frobenius norm.


==== Spectral distribution ====

The joint probability density for the eigenvalues λ1, λ2, ..., λn of GUE/GOE/GSE is given by

where Zβ,n is a normalization constant which can be explicitly computed, see Selberg integral. In the case of GUE (β = 2), the formula (1) describes a determinantal point process. Eigenvalues repel as the joint probability density has a zero (of 
  
    
      
        β
      
    
    {\displaystyle \beta }
  
th order) for coinciding eigenvalues 
  
    
      
        
          λ
          
            j
          
        
        =
        
          λ
          
            i
          
        
      
    
    {\displaystyle \lambda _{j}=\lambda _{i}}
  
, and 
  
    
      
        
          Z
          
            2
            ,
            n
          
        
        =
        (
        2
        π
        
          )
          
            n
            
              /
            
            2
          
        
        
          ∏
          
            k
            =
            1
          
          
            n
          
        
        k
        !
      
    
    {\displaystyle Z_{2,n}=(2\pi )^{n/2}\prod _{k=1}^{n}k!}
  
. 
More succinctly, 
  
    
      
        
          
            1
            
              Z
              
                β
                ,
                n
              
            
          
        
        
          e
          
            −
            
              
                β
                4
              
            
            ‖
            λ
            
              ‖
              
                2
              
              
                2
              
            
          
        
        
          |
        
        
          Δ
          
            n
          
        
        (
        λ
        )
        
          
            |
          
          
            β
          
        
      
    
    {\displaystyle {\frac {1}{Z_{\beta ,n}}}e^{-{\frac {\beta }{4}}\|\lambda \|_{2}^{2}}|\Delta _{n}(\lambda )|^{\beta }}
  
where 
  
    
      
        
          Δ
          
            n
          
        
      
    
    {\displaystyle \Delta _{n}}
  
 is the Vandermonde determinant.
The distribution of the largest eigenvalue for GOE, and GUE, are explicitly solvable. They converge to the Tracy–Widom distribution after shifting and scaling appropriately.
The spectrum, divided by 
  
    
      
        
          
            N
            
              σ
              
                2
              
            
          
        
      
    
    {\displaystyle {\sqrt {N\sigma ^{2}}}}
  
, converges in distribution to the semicircular distribution on the interval 
  
    
      
        [
        −
        2
        ,
        +
        2
        ]
      
    
    {\displaystyle [-2,+2]}
  
: 
  
    
      
        ρ
        (
        x
        )
        =
        
          
            1
            
              2
              π
            
          
        
        
          
            4
            −
            
              x
              
                2
              
            
          
        
      
    
    {\displaystyle \rho (x)={\frac {1}{2\pi }}{\sqrt {4-x^{2}}}}
  
. Here 
  
    
      
        
          σ
          
            2
          
        
      
    
    {\displaystyle \sigma ^{2}}
  
 is the variance of off-diagonal entries. The variance of the on-diagonal entries do not matter.


=== Wishart matrices ===

Wishart matrices are n × n random matrices of the form H = X X*, where X is an n × m random matrix (m ≥ n) with independent entries, and X* is its conjugate transpose. In the important special case considered by Wishart, the entries of X are identically distributed Gaussian random variables (either real or complex).
The limit of the empirical spectral measure of Wishart matrices was found by Vladimir Marchenko and Leonid Pastur.  


=== Random band matrix ===

Random band matrices are random matrices with the property that all entries outside a certain band are zero. They can be used to roughly model systems of interacting particles arranged roughly in a grid such that each particle is only allowed to interact with its neighbors, which is an improvement on the mean field model.
In one dimension, this means that 
  
    
      
        
          H
          
            i
            j
          
        
        =
        0
      
    
    {\textstyle H_{ij}=0}
  
 if 
  
    
      
        
          |
        
        i
        −
        j
        
          |
        
        >
        W
      
    
    {\textstyle |i-j|>W}
  
, where W is the band width. Physically, this means that the amount by which particles i and j interact is 0 if their separation is over W. In more than one dimension, i and j are no longer integers but nd vectors with integer components, and 
  
    
      
        
          H
          
            i
            j
          
        
        =
        0
      
    
    {\textstyle H_{ij}=0}
  
 if 
  
    
      
        
          |
        
        i
        −
        j
        
          
            |
          
          
            
              L
              
                1
              
            
          
        
      
    
    {\displaystyle |i-j|_{L^{1}}}
  
, where 
  
    
      
        
          |
        
        ⋅
        
          
            |
          
          
            
              L
              
                1
              
            
          
        
      
    
    {\displaystyle |\cdot |_{L^{1}}}
  
 indicates the taxicab distance between the two locations. 
  
    
      
        
          H
          
            i
            j
          
        
        =
        
          H
          
            j
            i
          
        
      
    
    {\textstyle H_{ij}=H_{ji}}
  
 for all i,j and nonzero values of 
  
    
      
        
          H
          
            i
            j
          
        
      
    
    {\textstyle H_{ij}}
  
 have variances 
  
    
      
        
          σ
          
            i
            j
          
          
            2
          
        
      
    
    {\displaystyle \sigma _{ij}^{2}}
  
 of the same order of magnitude, normalized such that 
  
    
      
        
          ∑
          
            j
          
        
        
          σ
          
            i
            j
          
          
            2
          
        
        =
        1
      
    
    {\textstyle \sum _{j}\sigma _{ij}^{2}=1}
  
 for each value of j.


=== Random unitary matrices ===


=== Non-Hermitian random matrices ===


== Spectral theory ==
The spectral theory of random matrices studies the distribution of the eigenvalues as the size of the matrix goes to infinity.


=== Empirical spectral measure ===
The empirical spectral measure 
  
    
      
        
          μ
          
            H
          
        
      
    
    {\displaystyle \mu _{H}}
  
 of 
  
    
      
        H
      
    
    {\displaystyle H}
  
 is defined by
  
    
      
        
          μ
          
            H
          
        
        (
        A
        )
        =
        
          
            1
            n
          
        
        
        #
        
          {
          
            
              eigenvalues of 
            
            H
            
               in 
            
            A
          
          }
        
        =
        
          N
          
            
              1
              
                A
              
            
            ,
            H
          
        
        ,
        
        A
        ⊂
        
          R
        
        .
      
    
    {\displaystyle \mu _{H}(A)={\frac {1}{n}}\,\#\left\{{\text{eigenvalues of }}H{\text{ in }}A\right\}=N_{1_{A},H},\quad A\subset \mathbb {R} .}
  

or more succinctly, if 
  
    
      
        
          λ
          
            1
          
        
        ,
        …
        ,
        
          λ
          
            n
          
        
      
    
    {\displaystyle \lambda _{1},\ldots ,\lambda _{n}}
  
 are the eigenvalues of 
  
    
      
        H
      
    
    {\displaystyle H}
  

  
    
      
        
          μ
          
            H
          
        
        (
        d
        λ
        )
        =
        
          
            1
            n
          
        
        
          ∑
          
            i
          
        
        
          δ
          
            
              λ
              
                i
              
            
          
        
        (
        d
        λ
        )
        .
      
    
    {\displaystyle \mu _{H}(d\lambda )={\frac {1}{n}}\sum _{i}\delta _{\lambda _{i}}(d\lambda ).}
  

Usually, the limit of 
  
    
      
        
          μ
          
            H
          
        
      
    
    {\displaystyle \mu _{H}}
  
 is a deterministic measure; this is a particular case of self-averaging. The cumulative distribution function of the limiting measure is called the integrated density of states and is denoted N(λ). If the integrated density of states is differentiable, its derivative is called the density of states and is denoted ρ(λ).


=== Types of convergence ===
Given a matrix ensemble, we say that its spectral measures converge weakly to 
  
    
      
        ρ
      
    
    {\displaystyle \rho }
  
 iff for any measurable set 
  
    
      
        A
      
    
    {\displaystyle A}
  
, the ensemble-average converges:
  
    
      
        
          lim
          
            n
            →
            ∞
          
        
        
          
            E
          
          
            H
          
        
        [
        
          μ
          
            H
          
        
        (
        A
        )
        ]
        =
        ρ
        (
        A
        )
      
    
    {\displaystyle \lim _{n\to \infty }\mathbb {E} _{H}[\mu _{H}(A)]=\rho (A)}
  
Convergence weakly almost surely: If we sample 
  
    
      
        
          H
          
            1
          
        
        ,
        
          H
          
            2
          
        
        ,
        
          H
          
            3
          
        
        ,
        …
      
    
    {\displaystyle H_{1},H_{2},H_{3},\dots }
  
 independently from the ensemble, then with probability 1,
  
    
      
        
          lim
          
            n
            →
            ∞
          
        
        
          μ
          
            
              H
              
                n
              
            
          
        
        (
        A
        )
        =
        ρ
        (
        A
        )
      
    
    {\displaystyle \lim _{n\to \infty }\mu _{H_{n}}(A)=\rho (A)}
  
for any measurable set 
  
    
      
        A
      
    
    {\displaystyle A}
  
. 
In another sense, weak almost sure convergence means that we sample 
  
    
      
        
          H
          
            1
          
        
        ,
        
          H
          
            2
          
        
        ,
        
          H
          
            3
          
        
        ,
        …
      
    
    {\displaystyle H_{1},H_{2},H_{3},\dots }
  
, not independently, but by "growing" (a stochastic process), then with probability 1, 
  
    
      
        
          lim
          
            n
            →
            ∞
          
        
        
          μ
          
            
              H
              
                n
              
            
          
        
        (
        A
        )
        =
        ρ
        (
        A
        )
      
    
    {\displaystyle \lim _{n\to \infty }\mu _{H_{n}}(A)=\rho (A)}
  
 for any measurable set 
  
    
      
        A
      
    
    {\displaystyle A}
  
.
For example, we can "grow" a sequence of matrices from the Gaussian ensemble as follows:

Sample an infinite doubly infinite sequence of standard random variables 
  
    
      
        {
        
          G
          
            i
            ,
            j
          
        
        
          }
          
            i
            ,
            j
            =
            1
            ,
            2
            ,
            3
            ,
            …
          
        
      
    
    {\displaystyle \{G_{i,j}\}_{i,j=1,2,3,\dots }}
  
.
Define each 
  
    
      
        
          H
          
            n
          
        
        =
        (
        
          G
          
            n
          
        
        +
        
          G
          
            n
          
          
            T
          
        
        )
        
          /
        
        
          
            2
            n
          
        
      
    
    {\displaystyle H_{n}=(G_{n}+G_{n}^{T})/{\sqrt {2n}}}
  
 where 
  
    
      
        
          G
          
            n
          
        
      
    
    {\displaystyle G_{n}}
  
 is the matrix made of entries 
  
    
      
        {
        
          G
          
            i
            ,
            j
          
        
        
          }
          
            i
            ,
            j
            =
            1
            ,
            2
            ,
            …
            ,
            n
          
        
      
    
    {\displaystyle \{G_{i,j}\}_{i,j=1,2,\dots ,n}}
  
.
Note that generic matrix ensembles do not allow us to grow, but most of the common ones, such as the three Gaussian ensembles, do allow us to grow.


=== Global regime ===
In the global regime, one is interested in the distribution of linear statistics of the form 
  
    
      
        
          N
          
            f
            ,
            H
          
        
        =
        
          n
          
            −
            1
          
        
        
          tr
        
        f
        (
        H
        )
      
    
    {\displaystyle N_{f,H}=n^{-1}{\text{tr}}f(H)}
  
.
The limit of the empirical spectral measure for Wigner matrices was described by Eugene Wigner; see Wigner semicircle distribution and Wigner surmise. As far as sample covariance matrices are concerned, a theory was developed by Marčenko and Pastur.
The limit of the empirical spectral measure of invariant matrix ensembles is described by a certain integral equation which arises from potential theory.


==== Fluctuations ====
For the linear statistics Nf,H = n−1 Σ f(λj), one is also interested in the fluctuations about ∫ f(λ) dN(λ). For many classes of random matrices, a central limit theorem of the form

  
    
      
        
          
            
              
                N
                
                  f
                  ,
                  H
                
              
              −
              ∫
              f
              (
              λ
              )
              
              d
              N
              (
              λ
              )
            
            
              σ
              
                f
                ,
                n
              
            
          
        
        
          
            ⟶
            D
          
        
        N
        (
        0
        ,
        1
        )
      
    
    {\displaystyle {\frac {N_{f,H}-\int f(\lambda )\,dN(\lambda )}{\sigma _{f,n}}}{\overset {D}{\longrightarrow }}N(0,1)}
  

is known.


==== The variational problem for the unitary ensembles ====
Consider the measure

  
    
      
        
          d
        
        
          μ
          
            N
          
        
        (
        μ
        )
        =
        
          
            1
            
              
                
                  
                    Z
                    ~
                  
                
              
              
                N
              
            
          
        
        
          e
          
            −
            
              H
              
                N
              
            
            (
            λ
            )
          
        
        
          d
        
        λ
        ,
        
        
          H
          
            N
          
        
        (
        λ
        )
        =
        −
        
          ∑
          
            j
            ≠
            k
          
        
        ln
        ⁡
        
          |
        
        
          λ
          
            j
          
        
        −
        
          λ
          
            k
          
        
        
          |
        
        +
        N
        
          ∑
          
            j
            =
            1
          
          
            N
          
        
        Q
        (
        
          λ
          
            j
          
        
        )
        ,
      
    
    {\displaystyle \mathrm {d} \mu _{N}(\mu )={\frac {1}{{\widetilde {Z}}_{N}}}e^{-H_{N}(\lambda )}\mathrm {d} \lambda ,\qquad H_{N}(\lambda )=-\sum \limits _{j\neq k}\ln |\lambda _{j}-\lambda _{k}|+N\sum \limits _{j=1}^{N}Q(\lambda _{j}),}
  

where 
  
    
      
        Q
        (
        M
        )
      
    
    {\displaystyle Q(M)}
  
 is the potential of the ensemble and let 
  
    
      
        ν
      
    
    {\displaystyle \nu }
  
 be the empirical spectral measure.
We can rewrite 
  
    
      
        
          H
          
            N
          
        
        (
        λ
        )
      
    
    {\displaystyle H_{N}(\lambda )}
  
 with 
  
    
      
        ν
      
    
    {\displaystyle \nu }
  
 as

  
    
      
        
          H
          
            N
          
        
        (
        λ
        )
        =
        
          N
          
            2
          
        
        
          [
          
            −
            ∫
            
              ∫
              
                x
                ≠
                y
              
            
            ln
            ⁡
            
              |
            
            x
            −
            y
            
              |
            
            
              d
            
            ν
            (
            x
            )
            
              d
            
            ν
            (
            y
            )
            +
            ∫
            Q
            (
            x
            )
            
              d
            
            ν
            (
            x
            )
          
          ]
        
        ,
      
    
    {\displaystyle H_{N}(\lambda )=N^{2}\left[-\int \int _{x\neq y}\ln |x-y|\mathrm {d} \nu (x)\mathrm {d} \nu (y)+\int Q(x)\mathrm {d} \nu (x)\right],}
  

the probability measure is now of the form

  
    
      
        
          d
        
        
          μ
          
            N
          
        
        (
        μ
        )
        =
        
          
            1
            
              
                
                  
                    Z
                    ~
                  
                
              
              
                N
              
            
          
        
        
          e
          
            −
            
              N
              
                2
              
            
            
              I
              
                Q
              
            
            (
            ν
            )
          
        
        
          d
        
        λ
        ,
      
    
    {\displaystyle \mathrm {d} \mu _{N}(\mu )={\frac {1}{{\widetilde {Z}}_{N}}}e^{-N^{2}I_{Q}(\nu )}\mathrm {d} \lambda ,}
  

where 
  
    
      
        
          I
          
            Q
          
        
        (
        ν
        )
      
    
    {\displaystyle I_{Q}(\nu )}
  
 is the above functional inside the squared brackets.
Let now

  
    
      
        
          M
          
            1
          
        
        (
        
          R
        
        )
        =
        
          {
          
            ν
            :
            ν
            ≥
            0
            ,
             
            
              ∫
              
                
                  R
                
              
            
            
              d
            
            ν
            =
            1
          
          }
        
      
    
    {\displaystyle M_{1}(\mathbb {R} )=\left\{\nu :\nu \geq 0,\ \int _{\mathbb {R} }\mathrm {d} \nu =1\right\}}
  

be the space of one-dimensional probability measures and consider the minimizer

  
    
      
        
          E
          
            Q
          
        
        =
        
          inf
          
            ν
            ∈
            
              M
              
                1
              
            
            (
            
              R
            
            )
          
        
        −
        ∫
        
          ∫
          
            x
            ≠
            y
          
        
        ln
        ⁡
        
          |
        
        x
        −
        y
        
          |
        
        
          d
        
        ν
        (
        x
        )
        
          d
        
        ν
        (
        y
        )
        +
        ∫
        Q
        (
        x
        )
        
          d
        
        ν
        (
        x
        )
        .
      
    
    {\displaystyle E_{Q}=\inf \limits _{\nu \in M_{1}(\mathbb {R} )}-\int \int _{x\neq y}\ln |x-y|\mathrm {d} \nu (x)\mathrm {d} \nu (y)+\int Q(x)\mathrm {d} \nu (x).}
  

For 
  
    
      
        
          E
          
            Q
          
        
      
    
    {\displaystyle E_{Q}}
  
 there exists a unique equilibrium measure 
  
    
      
        
          ν
          
            Q
          
        
      
    
    {\displaystyle \nu _{Q}}
  
 through the Euler-Lagrange variational conditions for some real constant 
  
    
      
        l
      
    
    {\displaystyle l}
  

  
    
      
        2
        
          ∫
          
            
              R
            
          
        
        log
        ⁡
        
          |
        
        x
        −
        y
        
          |
        
        
          d
        
        ν
        (
        y
        )
        −
        Q
        (
        x
        )
        =
        l
        ,
        
        x
        ∈
        J
      
    
    {\displaystyle 2\int _{\mathbb {R} }\log |x-y|\mathrm {d} \nu (y)-Q(x)=l,\quad x\in J}
  

  
    
      
        2
        
          ∫
          
            
              R
            
          
        
        log
        ⁡
        
          |
        
        x
        −
        y
        
          |
        
        
          d
        
        ν
        (
        y
        )
        −
        Q
        (
        x
        )
        ≤
        l
        ,
        
        x
        ∈
        
          R
        
        ∖
        J
      
    
    {\displaystyle 2\int _{\mathbb {R} }\log |x-y|\mathrm {d} \nu (y)-Q(x)\leq l,\quad x\in \mathbb {R} \setminus J}
  

where 
  
    
      
        J
        =
        
          ⋃
          
            j
            =
            1
          
          
            q
          
        
        [
        
          a
          
            j
          
        
        ,
        
          b
          
            j
          
        
        ]
      
    
    {\displaystyle J=\bigcup \limits _{j=1}^{q}[a_{j},b_{j}]}
  
 is the support of the measure and define

  
    
      
        q
        (
        x
        )
        =
        −
        
          
            (
            
              
                
                  
                    Q
                    ′
                  
                  (
                  x
                  )
                
                2
              
            
            )
          
          
            2
          
        
        +
        ∫
        
          
            
              
                Q
                ′
              
              (
              x
              )
              −
              
                Q
                ′
              
              (
              y
              )
            
            
              x
              −
              y
            
          
        
        
          d
        
        
          ν
          
            Q
          
        
        (
        y
        )
      
    
    {\displaystyle q(x)=-\left({\frac {Q'(x)}{2}}\right)^{2}+\int {\frac {Q'(x)-Q'(y)}{x-y}}\mathrm {d} \nu _{Q}(y)}
  
.
The equilibrium measure 
  
    
      
        
          ν
          
            Q
          
        
      
    
    {\displaystyle \nu _{Q}}
  
 has the following Radon–Nikodym density

  
    
      
        
          
            
              
                d
              
              
                ν
                
                  Q
                
              
              (
              x
              )
            
            
              
                d
              
              x
            
          
        
        =
        
          
            1
            π
          
        
        
          
            q
            (
            x
            )
          
        
        .
      
    
    {\displaystyle {\frac {\mathrm {d} \nu _{Q}(x)}{\mathrm {d} x}}={\frac {1}{\pi }}{\sqrt {q(x)}}.}
  


=== Mesoscopic regime ===
 The typical statement of the Wigner semicircular law is equivalent to the following statement: For each fixed interval 
  
    
      
        [
        
          λ
          
            0
          
        
        −
        Δ
        λ
        ,
        
          λ
          
            0
          
        
        +
        Δ
        λ
        ]
      
    
    {\displaystyle [\lambda _{0}-\Delta \lambda ,\lambda _{0}+\Delta \lambda ]}
  
 centered at a point 
  
    
      
        
          λ
          
            0
          
        
      
    
    {\displaystyle \lambda _{0}}
  
, as 
  
    
      
        N
      
    
    {\displaystyle N}
  
, the number of dimensions of the gaussian ensemble increases, the proportion of the eigenvalues falling within the interval converges to 
  
    
      
        
          ∫
          
            [
            
              λ
              
                0
              
            
            −
            Δ
            λ
            ,
            
              λ
              
                0
              
            
            +
            Δ
            λ
            ]
          
        
        ρ
        (
        t
        )
        d
        t
      
    
    {\displaystyle \int _{[\lambda _{0}-\Delta \lambda ,\lambda _{0}+\Delta \lambda ]}\rho (t)dt}
  
, where 
  
    
      
        ρ
        (
        t
        )
      
    
    {\displaystyle \rho (t)}
  
 is the density of the semicircular distribution.
If 
  
    
      
        Δ
        λ
      
    
    {\displaystyle \Delta \lambda }
  
 can be allowed to decrease as 
  
    
      
        N
      
    
    {\displaystyle N}
  
 increases, then we obtain strictly stronger theorems, named "local laws" or "mesoscopic regime".
The mesoscopic regime is intermediate between the local and the global. In the mesoscopic regime, one is interested in the limit distribution of eigenvalues in a set that shrinks to zero, but slow enough, such that the number of eigenvalues inside 
  
    
      
        →
        ∞
      
    
    {\displaystyle \to \infty }
  
.
For example, the Ginibre ensemble has a mesoscopic law: For any sequence of shrinking disks with areas 
  
    
      
        u
      
    
    {\displaystyle u}
  
inside the unite disk, if the disks have area 
  
    
      
        
          A
          
            n
          
        
        =
        O
        (
        
          n
          
            −
            1
            +
            ϵ
          
        
        )
      
    
    {\displaystyle A_{n}=O(n^{-1+\epsilon })}
  
, the conditional distribution of the spectrum inside the disks also converges to a uniform distribution. That is, if we cut the shrinking disks along with the spectrum falling inside the disks, then scale the disks up to unit area, we would see the spectra converging to a flat distribution in the disks.


=== Local regime ===
In the local regime, one is interested in the limit distribution of eigenvalues in a set that shrinks so fast that the number of eigenvalues remains 
  
    
      
        O
        (
        1
        )
      
    
    {\displaystyle O(1)}
  
.
Typically this means the study of spacings between eigenvalues, and, more generally, in the joint distribution of eigenvalues in an interval of length of order 1/n. One distinguishes between bulk statistics, pertaining to intervals inside the support of the limiting spectral measure, and edge statistics, pertaining to intervals near the boundary of the support.


==== Bulk statistics ====
Formally, fix 
  
    
      
        
          λ
          
            0
          
        
      
    
    {\displaystyle \lambda _{0}}
  
 in the interior of the support of 
  
    
      
        N
        (
        λ
        )
      
    
    {\displaystyle N(\lambda )}
  
. Then consider the point process

  
    
      
        Ξ
        (
        
          λ
          
            0
          
        
        )
        =
        
          ∑
          
            j
          
        
        δ
        
          
            (
          
        
        
          ⋅
        
        −
        n
        ρ
        (
        
          λ
          
            0
          
        
        )
        (
        
          λ
          
            j
          
        
        −
        
          λ
          
            0
          
        
        )
        
          
            )
          
        
         
        ,
      
    
    {\displaystyle \Xi (\lambda _{0})=\sum _{j}\delta {\Big (}{\cdot }-n\rho (\lambda _{0})(\lambda _{j}-\lambda _{0}){\Big )}~,}
  

where 
  
    
      
        
          λ
          
            j
          
        
      
    
    {\displaystyle \lambda _{j}}
  
 are the eigenvalues of the random matrix.
The point process 
  
    
      
        Ξ
        (
        
          λ
          
            0
          
        
        )
      
    
    {\displaystyle \Xi (\lambda _{0})}
  
 captures the statistical properties of eigenvalues in the vicinity of 
  
    
      
        
          λ
          
            0
          
        
      
    
    {\displaystyle \lambda _{0}}
  
. For the Gaussian ensembles, the limit of 
  
    
      
        Ξ
        (
        
          λ
          
            0
          
        
        )
      
    
    {\displaystyle \Xi (\lambda _{0})}
  
 is known; thus, for GUE it is a determinantal point process with the kernel

  
    
      
        K
        (
        x
        ,
        y
        )
        =
        
          
            
              sin
              ⁡
              π
              (
              x
              −
              y
              )
            
            
              π
              (
              x
              −
              y
              )
            
          
        
      
    
    {\displaystyle K(x,y)={\frac {\sin \pi (x-y)}{\pi (x-y)}}}
  

(the sine kernel).
The universality principle postulates that the limit of 
  
    
      
        Ξ
        (
        
          λ
          
            0
          
        
        )
      
    
    {\displaystyle \Xi (\lambda _{0})}
  
 as 
  
    
      
        n
        →
        ∞
      
    
    {\displaystyle n\to \infty }
  
 should depend only on the symmetry class of the random matrix (and neither on the specific model of random matrices nor on 
  
    
      
        
          λ
          
            0
          
        
      
    
    {\displaystyle \lambda _{0}}
  
). Rigorous proofs of universality are known for invariant matrix ensembles and Wigner matrices.  


==== Edge statistics ====

One example of edge statistics is the Tracy–Widom distribution.
As another example, consider the Ginibre ensemble. It can be real or complex. The real Ginibre ensemble has i.i.d. standard Gaussian entries 
  
    
      
        
          
            N
          
        
        (
        0
        ,
        1
        )
      
    
    {\displaystyle {\mathcal {N}}(0,1)}
  
, and the complex Ginibre ensemble has i.i.d. standard complex Gaussian entries 
  
    
      
        
          
            N
          
        
        (
        0
        ,
        1
        
          /
        
        2
        )
        +
        i
        
          
            N
          
        
        (
        0
        ,
        1
        
          /
        
        2
        )
      
    
    {\displaystyle {\mathcal {N}}(0,1/2)+i{\mathcal {N}}(0,1/2)}
  
. 
Now let 
  
    
      
        
          G
          
            n
          
        
      
    
    {\displaystyle G_{n}}
  
 be sampled from the real or complex ensemble, and let 
  
    
      
        ρ
        (
        
          G
          
            n
          
        
        )
      
    
    {\displaystyle \rho (G_{n})}
  
 be the absolute value of its maximal eigenvalue:
  
    
      
        ρ
        (
        
          G
          
            n
          
        
        )
        :=
        
          max
          
            j
          
        
        
          |
        
        
          λ
          
            j
          
        
        
          |
        
      
    
    {\displaystyle \rho (G_{n}):=\max _{j}|\lambda _{j}|}
  
We have the following theorem for the edge statistics: 

This theorem refines the circular law of the Ginibre ensemble. In words, the circular law says that the spectrum of 
  
    
      
        
          
            1
            
              n
            
          
        
        
          G
          
            n
          
        
      
    
    {\displaystyle {\frac {1}{\sqrt {n}}}G_{n}}
  
 almost surely falls uniformly on the unit disc.  and the edge statistics theorem states that the radius of the almost-unit-disk is about 
  
    
      
        1
        −
        
          
            
              
                γ
                
                  n
                
              
              
                4
                n
              
            
          
        
      
    
    {\displaystyle 1-{\sqrt {\frac {\gamma _{n}}{4n}}}}
  
, and fluctuates on a scale of 
  
    
      
        
          
            1
            
              4
              n
              
                γ
                
                  n
                
              
            
          
        
      
    
    {\displaystyle {\frac {1}{\sqrt {4n\gamma _{n}}}}}
  
, according to the Gumbel law.


=== Spectral rigidity ===
The phenomenon of spectral rigidity states that the eigenvalues from most commonly used matrix ensembles tend to be more uniformly distributed than they would be if they were sampled independently at random. That is, they together clump less than a purely Poisson point process. It is also called eigenvalue rigidity or level repulsion.
More quantitatively, suppose that a matrix ensemble has limit spectral density measure 
  
    
      
        μ
      
    
    {\displaystyle \mu }
  
. Fix some subset 
  
    
      
        S
      
    
    {\displaystyle S}
  
 such that 
  
    
      
        0
        <
        μ
        (
        S
        )
        <
        1
      
    
    {\displaystyle 0<\mu (S)<1}
  
. This is the proportion of eigenvalues that falls within 
  
    
      
        S
      
    
    {\displaystyle S}
  
 at the limit of large 
  
    
      
        N
      
    
    {\displaystyle N}
  
, so the expected number of eigenvalues falling within 
  
    
      
        S
      
    
    {\displaystyle S}
  
 is 
  
    
      
        N
        μ
        (
        S
        )
      
    
    {\displaystyle N\mu (S)}
  
. Now, a purely Poisson point process would have meant that the actual number of 
  
    
      
        N
        μ
        (
        S
        )
        +
        O
        (
        
          
            N
            μ
            (
            S
            )
            (
            1
            −
            μ
            (
            S
            )
            )
          
        
        )
      
    
    {\displaystyle N\mu (S)+O({\sqrt {N\mu (S)(1-\mu (S))}})}
  
, since 
  
    
      
        
          
            N
            μ
            (
            S
            )
            (
            1
            −
            μ
            (
            S
            )
            )
          
        
      
    
    {\displaystyle {\sqrt {N\mu (S)(1-\mu (S))}}}
  
 is the standard deviation of the number of points falling within 
  
    
      
        S
      
    
    {\displaystyle S}
  
 when the points are completely independent of each other. Conversely, if the points are completely rigid, then the actual number would be equal to 
  
    
      
        N
        μ
        (
        S
        )
      
    
    {\displaystyle N\mu (S)}
  
 without fluctuation. Now, it turns out that in many matrix ensembles, the number of points falling within 
  
    
      
        S
      
    
    {\displaystyle S}
  
 is 
  
    
      
        N
        μ
        (
        S
        )
        +
        O
        (
        
          
            ln
            ⁡
            N
          
        
        )
      
    
    {\displaystyle N\mu (S)+O({\sqrt {\ln N}})}
  
, i.e. not completely rigid, but very close to it. Spectral rigidity has been numerically observed in the zeros of the Riemann zeta function.


=== Correlation functions ===
The joint probability density of the eigenvalues of 
  
    
      
        n
        ×
        n
      
    
    {\displaystyle n\times n}
  
 random Hermitian matrices 
  
    
      
        M
        ∈
        
          
            H
          
          
            n
            ×
            n
          
        
      
    
    {\displaystyle M\in \mathbf {H} ^{n\times n}}
  
, with partition functions of the form

  
    
      
        
          Z
          
            n
          
        
        =
        
          ∫
          
            M
            ∈
            
              
                H
              
              
                n
                ×
                n
              
            
          
        
        d
        
          μ
          
            0
          
        
        (
        M
        )
        
          e
          
            
              tr
            
            (
            V
            (
            M
            )
            )
          
        
      
    
    {\displaystyle Z_{n}=\int _{M\in \mathbf {H} ^{n\times n}}d\mu _{0}(M)e^{{\text{tr}}(V(M))}}
  

where

  
    
      
        V
        (
        x
        )
        :=
        
          ∑
          
            j
            =
            1
          
          
            ∞
          
        
        
          v
          
            j
          
        
        
          x
          
            j
          
        
      
    
    {\displaystyle V(x):=\sum _{j=1}^{\infty }v_{j}x^{j}}
  

and 
  
    
      
        d
        
          μ
          
            0
          
        
        (
        M
        )
      
    
    {\displaystyle d\mu _{0}(M)}
  
 is the standard Lebesgue measure on the space 
  
    
      
        
          
            H
          
          
            n
            ×
            n
          
        
      
    
    {\displaystyle \mathbf {H} ^{n\times n}}
  
 of Hermitian 
  
    
      
        n
        ×
        n
      
    
    {\displaystyle n\times n}
  
 matrices, is given by

  
    
      
        
          p
          
            n
            ,
            V
          
        
        (
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
        )
        =
        
          
            1
            
              Z
              
                n
                ,
                V
              
            
          
        
        
          ∏
          
            i
            <
            j
          
        
        (
        
          x
          
            i
          
        
        −
        
          x
          
            j
          
        
        
          )
          
            2
          
        
        
          e
          
            −
            
              ∑
              
                i
              
            
            V
            (
            
              x
              
                i
              
            
            )
          
        
        .
      
    
    {\displaystyle p_{n,V}(x_{1},\dots ,x_{n})={\frac {1}{Z_{n,V}}}\prod _{i<j}(x_{i}-x_{j})^{2}e^{-\sum _{i}V(x_{i})}.}
  

The 
  
    
      
        k
      
    
    {\displaystyle k}
  
-point correlation functions (or marginal distributions) 
are defined as

  
    
      
        
          R
          
            n
            ,
            V
          
          
            (
            k
            )
          
        
        (
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            k
          
        
        )
        =
        
          
            
              n
              !
            
            
              (
              n
              −
              k
              )
              !
            
          
        
        
          ∫
          
            
              R
            
          
        
        d
        
          x
          
            k
            +
            1
          
        
        ⋯
        
          ∫
          
            
              R
            
          
        
        d
        
          x
          
            n
          
        
        
        
          p
          
            n
            ,
            V
          
        
        (
        
          x
          
            1
          
        
        ,
        
          x
          
            2
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
        )
        ,
      
    
    {\displaystyle R_{n,V}^{(k)}(x_{1},\dots ,x_{k})={\frac {n!}{(n-k)!}}\int _{\mathbf {R} }dx_{k+1}\cdots \int _{\mathbb {R} }dx_{n}\,p_{n,V}(x_{1},x_{2},\dots ,x_{n}),}
  

which are skew symmetric functions of their variables. 
In particular, the one-point correlation function, or density of states, is 

  
    
      
        
          R
          
            n
            ,
            V
          
          
            (
            1
            )
          
        
        (
        
          x
          
            1
          
        
        )
        =
        n
        
          ∫
          
            
              R
            
          
        
        d
        
          x
          
            2
          
        
        ⋯
        
          ∫
          
            
              R
            
          
        
        d
        
          x
          
            n
          
        
        
        
          p
          
            n
            ,
            V
          
        
        (
        
          x
          
            1
          
        
        ,
        
          x
          
            2
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
        )
        .
      
    
    {\displaystyle R_{n,V}^{(1)}(x_{1})=n\int _{\mathbb {R} }dx_{2}\cdots \int _{\mathbf {R} }dx_{n}\,p_{n,V}(x_{1},x_{2},\dots ,x_{n}).}
  

Its integral over a Borel set 
  
    
      
        B
        ⊂
        
          R
        
      
    
    {\displaystyle B\subset \mathbf {R} }
  
 gives the expected number of eigenvalues contained in 
  
    
      
        B
      
    
    {\displaystyle B}
  
:

  
    
      
        
          ∫
          
            B
          
        
        
          R
          
            n
            ,
            V
          
          
            (
            1
            )
          
        
        (
        x
        )
        d
        x
        =
        
          E
        
        
          (
          
            #
            {
            
              eigenvalues in 
            
            B
            }
          
          )
        
        .
      
    
    {\displaystyle \int _{B}R_{n,V}^{(1)}(x)dx=\mathbf {E} \left(\#\{{\text{eigenvalues in }}B\}\right).}
  

The following result expresses these correlation functions as determinants of the matrices formed from evaluating the appropriate integral kernel at the pairs 
  
    
      
        (
        
          x
          
            i
          
        
        ,
        
          x
          
            j
          
        
        )
      
    
    {\displaystyle (x_{i},x_{j})}
  
 of points appearing within the correlator.
Theorem [Dyson-Mehta] 
For any  
  
    
      
        k
      
    
    {\displaystyle k}
  
, 
  
    
      
        1
        ≤
        k
        ≤
        n
      
    
    {\displaystyle 1\leq k\leq n}
  
 the 
  
    
      
        k
      
    
    {\displaystyle k}
  
-point correlation function 
  
    
      
        
          R
          
            n
            ,
            V
          
          
            (
            k
            )
          
        
      
    
    {\displaystyle R_{n,V}^{(k)}}
  
 can be written as a determinant

  
    
      
        
          R
          
            n
            ,
            V
          
          
            (
            k
            )
          
        
        (
        
          x
          
            1
          
        
        ,
        
          x
          
            2
          
        
        ,
        …
        ,
        
          x
          
            k
          
        
        )
        =
        
          det
          
            1
            ≤
            i
            ,
            j
            ≤
            k
          
        
        
          (
          
            
              K
              
                n
                ,
                V
              
            
            (
            
              x
              
                i
              
            
            ,
            
              x
              
                j
              
            
            )
          
          )
        
        ,
      
    
    {\displaystyle R_{n,V}^{(k)}(x_{1},x_{2},\dots ,x_{k})=\det _{1\leq i,j\leq k}\left(K_{n,V}(x_{i},x_{j})\right),}
  

where 
  
    
      
        
          K
          
            n
            ,
            V
          
        
        (
        x
        ,
        y
        )
      
    
    {\displaystyle K_{n,V}(x,y)}
  
 is the 
  
    
      
        n
      
    
    {\displaystyle n}
  
th Christoffel-Darboux kernel

  
    
      
        
          K
          
            n
            ,
            V
          
        
        (
        x
        ,
        y
        )
        :=
        
          ∑
          
            k
            =
            0
          
          
            n
            −
            1
          
        
        
          ψ
          
            k
          
        
        (
        x
        )
        
          ψ
          
            k
          
        
        (
        y
        )
        ,
      
    
    {\displaystyle K_{n,V}(x,y):=\sum _{k=0}^{n-1}\psi _{k}(x)\psi _{k}(y),}
  

associated to 
  
    
      
        V
      
    
    {\displaystyle V}
  
, written in terms of the quasipolynomials 

  
    
      
        
          ψ
          
            k
          
        
        (
        x
        )
        =
        
          
            1
            
              
                
                  h
                  
                    k
                  
                
              
            
          
        
        
        
          p
          
            k
          
        
        (
        z
        )
        
        
          e
          
            −
            V
            (
            z
            )
            
              /
            
            2
          
        
        ,
      
    
    {\displaystyle \psi _{k}(x)={1 \over {\sqrt {h_{k}}}}\,p_{k}(z)\,e^{-V(z)/2},}
  
 
where 
  
    
      
        {
        
          p
          
            k
          
        
        (
        x
        )
        
          }
          
            k
            ∈
            
              N
            
          
        
      
    
    {\displaystyle \{p_{k}(x)\}_{k\in \mathbf {N} }}
  
 is a complete sequence of monic polynomials, of the degrees indicated, satisfying the orthogonality conditions

  
    
      
        
          ∫
          
            
              R
            
          
        
        
          ψ
          
            j
          
        
        (
        x
        )
        
          ψ
          
            k
          
        
        (
        x
        )
        d
        x
        =
        
          δ
          
            j
            k
          
        
        .
      
    
    {\displaystyle \int _{\mathbf {R} }\psi _{j}(x)\psi _{k}(x)dx=\delta _{jk}.}
  


== Generalizations ==
Wigner matrices are random Hermitian matrices 
  
    
      
        
          H
          
            n
          
        
        =
        (
        
          H
          
            n
          
        
        (
        i
        ,
        j
        )
        
          )
          
            i
            ,
            j
            =
            1
          
          
            n
          
        
      
    
    {\textstyle H_{n}=(H_{n}(i,j))_{i,j=1}^{n}}
  
 such that the entries

  
    
      
        
          {
          
            
              H
              
                n
              
            
            (
            i
            ,
            j
            )
             
            ,
            
            1
            ≤
            i
            ≤
            j
            ≤
            n
          
          }
        
      
    
    {\displaystyle \left\{H_{n}(i,j)~,\,1\leq i\leq j\leq n\right\}}
  

above the main diagonal are independent random variables with zero mean and have identical second moments.
The Gaussian ensembles can be extended for 
  
    
      
        β
        ≠
        1
        ,
        2
        ,
        4
      
    
    {\displaystyle \beta \neq 1,2,4}
  
 using the Dumitriu-Edelman tridiagonal trick. These are called the beta ensembles.
Invariant matrix ensembles are random Hermitian matrices with density on the space of real symmetric/Hermitian/quaternionic Hermitian matrices, which is of the form 
  
    
      
        
          
            1
            
              Z
              
                n
              
            
          
        
        
          e
          
            −
            n
            V
            (
            
              t
              r
            
            (
            H
            )
            )
          
        
         
        ,
      
    
    {\textstyle {\frac {1}{Z_{n}}}e^{-nV(\mathrm {tr} (H))}~,}
  
 where the function V is called the potential.
The Gaussian ensembles are the only common special cases of these two classes of random matrices. This is a consequence of a theorem by Porter and Rosenzweig.
Heavy tailed distributions generalize to random matrices as heavy tailed matrix ensembles.


== Selected bibliography ==


=== Books ===
Mehta, M.L. (2004). Random Matrices. Amsterdam: Elsevier/Academic Press. ISBN 0-12-088409-7.
Deift, Percy; Gioev, Dimitri (2009). Random matrix theory: invariant ensembles and universality. Courant lecture notes in mathematics. New York : Providence, R.I: Courant Institute of Mathematical Sciences ; American Mathematical Society. ISBN 978-0-8218-4737-4.
Forrester, Peter (2010). Log-gases and random matrices. London Mathematical Society monographs. Princeton: Princeton University Press. ISBN 978-0-691-12829-0.
Anderson, G.W.; Guionnet, A.; Zeitouni, O. (2010). An introduction to random matrices. Cambridge: Cambridge University Press. ISBN 978-0-521-19452-5.
Bai, Zhidong; Silverstein, Jack W. (2010). Spectral analysis of large dimensional random matrices. Springer series in statistics (2 ed.). New York ; London: Springer. doi:10.1007/978-1-4419-0661-8. ISBN 978-1-4419-0660-1. ISSN 0172-7397.
Akemann, G.; Baik, J.; Di Francesco, P. (2011). The Oxford Handbook of Random Matrix Theory. Oxford: Oxford University Press. ISBN 978-0-19-957400-1.
Tao, Terence (2012). Topics in random matrix theory. Graduate studies in mathematics. Providence, R.I: American Mathematical Society. ISBN 978-0-8218-7430-1.
Potters, Marc; Bouchaud, Jean-Philippe (2020-11-30). A First Course in Random Matrix Theory: for Physicists, Engineers and Data Scientists. Cambridge University Press. doi:10.1017/9781108768900. ISBN 978-1-108-76890-0.


=== Survey articles ===
Edelman, A.; Rao, N.R (2005). "Random matrix theory". Acta Numerica. 14: 233–297. Bibcode:2005AcNum..14..233E. doi:10.1017/S0962492904000236. S2CID 16038147.
Pastur, L.A. (1973). "Spectra of random self-adjoint operators". Russ. Math. Surv. 28 (1): 1–67. Bibcode:1973RuMaS..28....1P. doi:10.1070/RM1973v028n01ABEH001396. S2CID 250796916.
Diaconis, Persi (2003). "Patterns in eigenvalues: the 70th Josiah Willard Gibbs lecture". Bulletin of the American Mathematical Society. New Series. 40 (2): 155–178. doi:10.1090/S0273-0979-03-00975-3. MR 1962294.
Diaconis, Persi (2005). "What is ... a random matrix?". Notices of the American Mathematical Society. 52 (11): 1348–1349. ISSN 0002-9920. MR 2183871.
Eynard, Bertrand; Kimura, Taro; Ribault, Sylvain (2015-10-15). "Random matrices". arXiv:1510.04430v2 [math-ph].
Beenakker, Carlo (1997). "Random-matrix theory of quantum transport". Reviews of Modern Physics. 69 (3): 731–808. arXiv:cond-mat/9612179. Bibcode:1997RvMP...69..731B. doi:10.1103/RevModPhys.69.731.


=== Historic works ===
Wigner, E. (1955). "Characteristic vectors of bordered matrices with infinite dimensions". Annals of Mathematics. 62 (3): 548–564. doi:10.2307/1970079. JSTOR 1970079.
Wishart, J. (1928). "Generalized product moment distribution in samples". Biometrika. 20A (1–2): 32–52. doi:10.1093/biomet/20a.1-2.32.
von Neumann, J.; Goldstine, H.H. (1947). "Numerical inverting of matrices of high order". Bull. Amer. Math. Soc. 53 (11): 1021–1099. doi:10.1090/S0002-9904-1947-08909-6.


== References ==


== External links ==
Fyodorov, Y. (2011). "Random matrix theory". Scholarpedia. 6 (3): 9886. Bibcode:2011SchpJ...6.9886F. doi:10.4249/scholarpedia.9886.
Weisstein, E. W. "Random Matrix". Wolfram MathWorld.