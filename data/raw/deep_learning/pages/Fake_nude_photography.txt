Fake nude photography is the creation of nude photographs designed to appear as genuine nudes of an individual. The motivations for the creation of these modified photographs include curiosity, sexual gratification, the stigmatization or embarrassment of the subject, and commercial gain, such as through the sale of the photographs via pornographic websites. Fakes can be created using image editing software or through machine learning. Fake images created using the latter method are called deepfakes.


== History ==
Magazines such as Celebrity Skin published non-fake paparazzi shots and illicitly obtained nude photos, showing there was a market for such images.  Subsequently, some websites hosted fake nude or pornographic photos of celebrities, which are sometimes referred to as celebrity fakes.  In the 1990s and 2000s, fake nude images of celebrities proliferated on Usenet and on websites, leading to campaigns to take legal action against the creators of the images and websites dedicated to determining the veracity of nude photos. "Deepfakes", which use artificial neural networks to superimpose one person's face into an image or video of someone else, were popularized in the late 2010s, leading to concerns about the technology's use in fake news and revenge porn.
Fake nude photography is sometimes confused with Deepfake pornography, but the two are distinct.  Fake nude photography typically starts with human-made non-sexual images, and merely makes it appear that the people in them are nude (but not having sex).  Deepfake pornography typically starts with human-made sexual (pornographic) images or videos, and alters the actors' facial features to make the participants in the sexual act look like someone else. 


=== DeepNude ===
In June 2019, a downloadable Windows and Linux application called DeepNude was released which used a Generative Adversarial Network to remove clothing from images of women. The images it produced were typically not pornographic, merely nude.  Because there were more images of nude women than men available to its creator, the images it produced were all female, even when the original was male.  The app had both a paid and unpaid version. A few days later, on June 27, the creators removed the application and refunded consumers, although various copies of the app, both free and for charge, continue to exist. On GitHub, the open-source version of this program called "open-deepnude" was deleted. The open-source version had the advantage of allowing it to be trained on a larger dataset of nude images to increase the resulting nude image's accuracy level.  A successor free software application, Dreamtime, was later released, and some copies of it remain available, though some have been suppressed.


=== Deepfake Telegram Bot ===
In July 2019 a deepfake bot service was launched on messaging app Telegram that used AI technology to create nude images of women. The service was free and enabled users to submit photos and receive manipulated nude images within minutes. The service was connected to seven Telegram channels, including the main channel that hosts the bot, technical support, and image sharing channels. While the total number of users was unknown, the main channel had over 45,000 members. As of July 2020, it is estimated that approximately 24,000 manipulated images had been shared across the image sharing channels.


=== Nudify websites ===
By late 2024, most ways to produce nude images from photographs of clothed people were accessible at websites rather than in apps, and required payment.


== Purposes ==
The reasons for the creation of nude photos may range from a need to discredit the target publicly, personal hatred for the target, or the promise of pecuniary gains for such work on the part of the creator of such photos. Fake nude photos often target prominent figures such as businesspeople or politicians.


== Notable cases ==

In 2010, 97 people were arrested in Korea after spreading fake nude pictures of the group Girls' Generation on the internet. In 2011, a 53-year-old Incheon man was arrested after spreading more fake pictures of the same group.
In 2012, South Korean police identified 157 Korean artists of whom fake nudes were circulating.
In 2012, when Liu Yifei's fake nude photography released on the network, Liu Yifei Red Star Land Company declared a legal search to find out who created and released the photos.
In the same year, Chinese actor Huang Xiaoming released nude photos that sparked public controversy, but they were ultimately proven to be real pictures.
In 2014, supermodel Kate Upton threatened to sue a website for posting her fake nude photos. Previously, in 2011, this page was threatened by Taylor Swift.
In November 2014, singer Bi Rain was angry because of a fake nude photo that spread throughout the internet. Information reveals that: "Rain's nude photo was released from Kim Tae-hee's lost phone." Rain's label, Cube Entertainment, stated that the person in the nude photo is not Rain and the company has since stated that it will take strict legal action against those who post photos together with false comments.
In July 2018, Seoul police launched an investigation after a fake nude photo of President Moon Jae-in was posted on the website of the Korean radical feminist group WOMAD.
In early 2019, Alexandria Ocasio-Cortez, a Democratic politician, was berated by other political parties over a fake nude photo of her in the bathroom. The picture created a huge wave of media controversy in the United States.


== Methods ==
Fake nude images can be created using image editing software or neural network applications.
There are two basic  methods:

Combine and superimpose existing images onto source images, adding the face of the subject onto a nude model.
Remove clothes from the source image to make it look like a nude photo.


== Impact ==
Images of this type may have a negative psychological impact on the victims and may be used for extortion purposes.


== See also ==
Nude photography
Glamour photography
Deepfake pornography
Voyeurism


== References ==


== Further reading ==
Forbes, chapter 169, no 1–6, p. 84, Bertie Charles, Forbes Incorporated, 2002, California university.
American Journalism Review: AJR., chapter 18, no 1–5, p. 29, College of Journalism of the University of Maryland at College Park, 1996
Hana S. Noor Al-Deen, John Allen Hendricks, Social Media: Usage and Impact, p. 248, Lexington Books, 2012.
Janet Staiger, Media Reception Studies, p. 124, NYU Press, 1 July 2005
Kola Boof, Diary of a Lost Girl: The Autobiography of Kola Boof, p. 305, Door of Kush, 2006.
Laurence O'Toole, Pornocopia: porn, sex, technology and desire, p. 279, Serpent's Tail, 1999