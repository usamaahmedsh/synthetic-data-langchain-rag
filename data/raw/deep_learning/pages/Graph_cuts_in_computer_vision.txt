As applied in the field of computer vision, graph cut optimization can be employed to efficiently solve a wide variety of low-level computer vision problems (early vision), such as image smoothing, the stereo correspondence problem, image segmentation, object co-segmentation, and many other computer vision problems that can be formulated in terms of energy minimization. Graph cut techniques are now increasingly being used in conjunction with more general Artificial intelligence techniques (eg to fine tune tumour boundaries in medical imaging applications, are being embedded into Large language models to enforce more output structure etc). 
Many of these energy minimization problems can be approximated by solving a maximum flow problem in a graph (and thus, by the max-flow min-cut theorem, define a minimal cut of the graph). 
Under most formulations of such problems in computer vision, the minimum energy solution corresponds to the maximum a posteriori estimate of a solution. 
Although many computer vision algorithms involve cutting a graph (e.g., normalized cuts), the term "graph cuts" is applied specifically to those models which employ a max-flow/min-cut optimization (other graph cutting algorithms may be considered as graph partitioning algorithms).
"Binary" problems (such as denoising a binary image) can be solved exactly using this approach; problems where pixels can be labeled with more than two different labels (such as stereo correspondence, or denoising of a grayscale image) cannot be solved exactly, but solutions produced are usually near the global optimum.


== History ==
The foundational theory of graph cuts was first applied in computer vision in a legendary paper by Greig, Porteous and Seheult of Durham University. Allan Seheult and Bruce Porteous were members of Durham's lauded statistics group of the time, led by Julian Besag and Peter Green, with the optimisation expert Margaret Greig also notable as the first ever female member of staff of the Durham Mathematical Sciences Department.
In the Bayesian statistical context of smoothing noisy (or corrupted) images, using a Markov random field as the prior distribution, they showed how the maximum a posteriori estimate of a binary image can be obtained exactly by maximizing the flow through an associated image network, involving the introduction of a source and sink. The problem was therefore shown to be efficiently solvable. Prior to this result, approximate techniques such as simulated annealing (as proposed by the Geman brothers), or iterated conditional modes (a type of greedy algorithm suggested by Julian Besag) were used to solve such image smoothing problems. This seminal paper took ideas from Mathematical statistics, Physics, Optimisation and Computer science and was effectively 10 years ahead of its time.
Although the general 
  
    
      
        k
      
    
    {\displaystyle k}
  
-colour problem is NP hard for 
  
    
      
        k
        >
        2
        ,
      
    
    {\displaystyle k>2,}
  
 the approach of Greig, Porteous and Seheult has turned out to have wide applicability in general computer vision problems. For general problems, Greig, Porteous and Seheult's approach is often applied iteratively to sequences of related binary problems, usually yielding near optimal solutions.
In 2011, C. Couprie et al. proposed a general image segmentation framework, called the "Power Watershed", that minimized a real-valued indicator function from [0,1] over a graph, constrained by user seeds (or unary terms) set to 0 or 1, in which the minimization of the indicator function over the graph is optimized with respect to an exponent 
  
    
      
        p
      
    
    {\displaystyle p}
  
.  When 
  
    
      
        p
        =
        1
      
    
    {\displaystyle p=1}
  
, the Power Watershed is optimized by graph cuts, when 
  
    
      
        p
        =
        0
      
    
    {\displaystyle p=0}
  
 the Power Watershed is optimized by shortest paths, 
  
    
      
        p
        =
        2
      
    
    {\displaystyle p=2}
  
 is optimized by the random walker algorithm and 
  
    
      
        p
        =
        ∞
      
    
    {\displaystyle p=\infty }
  
 is optimized by the watershed algorithm. In this way, the Power Watershed may be viewed as a generalization of graph cuts that provides a straightforward connection with other energy optimization segmentation/clustering algorithms.


== Binary segmentation of images ==


=== Notation ===
Image: 
  
    
      
        x
        ∈
        {
        R
        ,
        G
        ,
        B
        
          }
          
            N
          
        
      
    
    {\displaystyle x\in \{R,G,B\}^{N}}
  

Output: Segmentation (also called opacity) 
  
    
      
        S
        ∈
        
          R
          
            N
          
        
      
    
    {\displaystyle S\in R^{N}}
  
 (soft segmentation). For hard segmentation 
  
    
      
        S
        ∈
        {
        0
        
           for background
        
        ,
        1
        
           for foreground/object to be detected
        
        
          }
          
            N
          
        
      
    
    {\displaystyle S\in \{0{\text{ for background}},1{\text{ for foreground/object to be detected}}\}^{N}}
  

Energy function: 
  
    
      
        E
        (
        x
        ,
        S
        ,
        C
        ,
        λ
        )
      
    
    {\displaystyle E(x,S,C,\lambda )}
  
 where C is the color parameter and λ is the coherence parameter.

  
    
      
        E
        (
        x
        ,
        S
        ,
        C
        ,
        λ
        )
        =
        
          E
          
            
              c
              o
              l
              o
              r
            
          
        
        +
        
          E
          
            
              c
              o
              h
              e
              r
              e
              n
              c
              e
            
          
        
      
    
    {\displaystyle E(x,S,C,\lambda )=E_{\rm {color}}+E_{\rm {coherence}}}
  

Optimization: The segmentation can be estimated as a global minimum over S: 
  
    
      
        
          
            arg
            ⁡
            min
          
          
            S
          
        
        E
        (
        x
        ,
        S
        ,
        C
        ,
        λ
        )
      
    
    {\displaystyle {\arg \min }_{S}E(x,S,C,\lambda )}
  


=== Existing methods ===
Standard Graph cuts: optimize energy function over the segmentation (unknown S value).
Iterated Graph cuts:
First step optimizes over the color parameters using K-means.
Second step performs the usual graph cuts algorithm.
These 2 steps are repeated recursively until convergence.
Dynamic graph cuts:Allows to re-run the algorithm much faster after modifying the problem (e.g. after new seeds have been added by a user).


=== Energy function ===

  
    
      
        Pr
        (
        x
        ∣
        S
        )
        =
        
          K
          
            −
            E
          
        
      
    
    {\displaystyle \Pr(x\mid S)=K^{-E}}
  

where the energy 
  
    
      
        E
      
    
    {\displaystyle E}
  
 is composed of two different models (
  
    
      
        
          E
          
            
              c
              o
              l
              o
              r
            
          
        
      
    
    {\displaystyle E_{\rm {color}}}
  
 and 
  
    
      
        
          E
          
            
              c
              o
              h
              e
              r
              e
              n
              c
              e
            
          
        
      
    
    {\displaystyle E_{\rm {coherence}}}
  
):


==== Likelihood / Color model / Regional term ====

  
    
      
        
          E
          
            
              c
              o
              l
              o
              r
            
          
        
      
    
    {\displaystyle E_{\rm {color}}}
  
 — unary term describing the likelihood of each color.

This term can be modeled using different local (e.g. texons) or global (e.g. histograms, GMMs, Adaboost likelihood) approaches that are described below.


===== Histogram =====
We use intensities of pixels marked as seeds to get histograms for object (foreground) and background intensity distributions: P(I|O) and P(I|B).
Then, we use these histograms to set the regional penalties as negative log-likelihoods.


===== GMM (Gaussian mixture model) =====
We usually use two distributions: one for background modelling and another for foreground pixels.
Use a Gaussian mixture model (with 5–8 components) to model those 2 distributions.
Goal: Try to pull apart those two distributions.


===== Texon =====
A texon (or texton) is a set of pixels that has certain characteristics and is repeated in an image.
Steps:
Determine a good natural scale for the texture elements.
Compute non-parametric statistics of the model-interior texons, either on intensity or on Gabor filter responses.
Examples:
Deformable-model based Textured Object Segmentation
Contour and Texture Analysis for Image Segmentation


==== Prior / Coherence model / Boundary term ====

  
    
      
        
          E
          
            
              c
              o
              h
              e
              r
              e
              n
              c
              e
            
          
        
      
    
    {\displaystyle E_{\rm {coherence}}}
  
 — binary term describing the coherence between neighborhood pixels.

In practice, pixels are defined as neighbors if they are adjacent either horizontally, vertically or diagonally (4 way connectivity or 8 way connectivity for 2D images).
Costs can be based on local intensity gradient, Laplacian zero-crossing, gradient direction, color mixture model,...
Different energy functions have been defined:
Standard Markov random field: Associate a penalty to disagreeing pixels by evaluating the difference between their segmentation label (crude measure of the length of the boundaries). See Boykov and Kolmogorov ICCV 2003
Conditional random field: If the color is very different, it might be a good place to put a boundary. See Lafferty et al. 2001; Kumar and Hebert 2003


== Criticism ==
Graph cuts methods have become popular alternatives to the level set-based approaches for optimizing the location of a contour (see for an extensive comparison).  However, graph cut approaches have been criticized in the literature for several issues:

Metrication artifacts: When an image is represented by a 4-connected lattice, graph cuts methods can exhibit unwanted "blockiness" artifacts.  Various methods have been proposed for addressing this issue, such as using additional edges or by formulating the max-flow problem in continuous space.
Shrinking bias: Since graph cuts finds a minimum cut, the algorithm can be biased toward producing a small contour.  For example, the algorithm is not well-suited for segmentation of thin objects like blood vessels (see for a proposed fix).
Multiple labels: Graph cuts is only able to find a global optimum for binary labeling (i.e., two labels) problems, such as foreground/background image segmentation.  Extensions have been proposed that can find approximate solutions for multilabel graph cuts problems.
Memory: the memory usage of graph cuts increases quickly as the image size increases. As an illustration, the Boykov-Kolmogorov max-flow algorithm v2.2 allocates 
  
    
      
        24
        n
        +
        14
        m
      
    
    {\displaystyle 24n+14m}
  
 bytes (
  
    
      
        n
      
    
    {\displaystyle n}
  
 and 
  
    
      
        m
      
    
    {\displaystyle m}
  
 are respectively the number of nodes and edges in the graph). Nevertheless, some amount of work has been recently done in this direction for reducing the graphs before the maximum-flow computation.


== Algorithm ==

Minimization is done using a standard minimum cut algorithm.
Due to the max-flow min-cut theorem we can solve energy minimization by maximizing the flow over the network. The max-flow problem consists of a directed graph with edges labeled with capacities, and there are two distinct nodes: the source and the sink. Intuitively, it is easy to see that the maximum flow is determined by the bottleneck.


=== Implementation (exact) ===

The Boykov-Kolmogorov algorithm is an efficient way to compute the max-flow for computer vision-related graphs.


=== Implementation (approximation) ===
The Sim Cut algorithm approximates the minimum graph cut. The algorithm implements a solution by simulation of an electrical network. This is the approach suggested by Cederbaum's maximum flow theorem.  Acceleration of the algorithm is possible through parallel computing.


== Software ==
http://pub.ist.ac.at/~vnk/software.html — An implementation of the maxflow algorithm described in "An Experimental Comparison of Min-Cut/Max-Flow Algorithms for Energy Minimization in Computer Vision" by Vladimir Kolmogorov
http://vision.csd.uwo.ca/code/ — some graph cut libraries and MATLAB wrappers
http://gridcut.com/ — fast multi-core max-flow/min-cut solver optimized for grid-like graphs


== References ==