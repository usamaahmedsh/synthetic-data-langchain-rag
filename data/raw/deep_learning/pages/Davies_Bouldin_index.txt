The Davies–Bouldin index (DBI), introduced by David L. Davies and Donald W. Bouldin in 1979, is a metric for evaluating clustering algorithms. This is an internal evaluation scheme, where the validation of how well the clustering has been done is made using quantities and features inherent to the dataset. This has a drawback that a good value reported by this method does not imply the best information retrieval.
According to a scientific article published in PeerJ Computer Science in 2025, the Davies-Bouldin index (along with the Silhouette coefficient) can be more informative than other common clustering internal metrics such as Dunn index, Calinski-Harabasz index, Shannon entropy, and Gap statistic when employed to assess convex-shaped clusters.


== Preliminaries ==
Given a finite collection of points in 
  
    
      
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle \mathbb {R} ^{n}}
  
, let 
  
    
      
        
          A
          
            i
          
        
      
    
    {\displaystyle A_{i}}
  
 be the centroid of cluster i. Define

  
    
      
        
          S
          
            i
          
        
        =
        
          
            (
            
              
                
                  1
                  
                    T
                    
                      i
                    
                  
                
              
              
                ∑
                
                  j
                  =
                  1
                
                
                  
                    T
                    
                      i
                    
                  
                
              
              
                
                  
                    |
                    
                      
                        X
                        
                          j
                        
                      
                      −
                      
                        A
                        
                          i
                        
                      
                    
                    |
                  
                  
                    q
                  
                
              
            
            )
          
          
            1
            
              /
            
            q
          
        
      
    
    {\displaystyle S_{i}=\left({\frac {1}{T_{i}}}\sum _{j=1}^{T_{i}}{\left|X_{j}-A_{i}\right|^{q}}\right)^{1/q}}
  
,
where 
  
    
      
        
          A
          
            i
          
        
      
    
    {\displaystyle A_{i}}
  
 is the centroid of cluster 
  
    
      
        i
      
    
    {\displaystyle i}
  
 and 
  
    
      
        
          T
          
            i
          
        
      
    
    {\displaystyle T_{i}}
  
 is the number of vectors in cluster 
  
    
      
        i
      
    
    {\displaystyle i}
  
. Statistically speaking, 
  
    
      
        
          S
          
            i
          
        
      
    
    {\displaystyle S_{i}}
  
 is the qth root of the qth moment of the points in cluster i about the mean. 
The Minkowski metric of the centroids, which characterizes clusters i and j, is defined as

  
    
      
        
          M
          
            i
            j
          
        
        =
        
          
            (
            
              
                ∑
                
                  k
                  =
                  1
                
                
                  N
                
              
              
                
                  |
                  
                    
                      A
                      
                        k
                        i
                      
                    
                    −
                    
                      A
                      
                        k
                        j
                      
                    
                  
                  |
                
                
                  p
                
              
            
            )
          
          
            1
            
              /
            
            p
          
        
      
    
    {\displaystyle M_{ij}=\left(\sum _{k=1}^{N}\left|A_{ki}-A_{kj}\right|^{p}\right)^{1/p}}
  
,
where 
  
    
      
        
          a
          
            k
            i
          
        
      
    
    {\displaystyle a_{ki}}
  
 is the kth component of n-dimensional vector 
  
    
      
        
          A
          
            i
          
        
      
    
    {\displaystyle A_{i}}
  
, which is the centroid of cluster i.
Note that q is used for S and p is used for M. 
When 
  
    
      
        p
        =
        1
      
    
    {\displaystyle p=1}
  
, 
  
    
      
        
          M
          
            i
            j
          
        
      
    
    {\displaystyle M_{ij}}
  
 is the Mahattan distance. 
When p=2, 
  
    
      
        
          M
          
            i
            j
          
        
      
    
    {\displaystyle M_{ij}}
  
 is the Euclidean distance between centroids. Many other distance metrics can be used, in the case of manifolds and high-dimensional data, where the Euclidean distance may not be the best measure for determining the clusters. It is important to note that this distance metric has to match with the metric used in the clustering scheme itself for meaningful results.
If 
  
    
      
        q
        =
        1
      
    
    {\displaystyle q=1}
  
,  
  
    
      
        
          S
          
            i
          
        
      
    
    {\displaystyle S_{i}}
  
 becomes the average Euclidean distance between the feature vectors in cluster i and the centroid of the cluster i. 
If 
  
    
      
        q
        =
        2
      
    
    {\displaystyle q=2}
  
, 
  
    
      
        
          S
          
            i
          
        
      
    
    {\displaystyle S_{i}}
  
 is the standard deviation of the distance of samples in a cluster to the respective cluster center. 
Define 
  
    
      
        
          R
          
            i
            j
          
        
      
    
    {\displaystyle R_{ij}}
  
 as the cluster similarity measure between cluster i and cluster j as

  
    
      
        
          R
          
            i
            j
          
        
        =
        
          
            
              
                S
                
                  i
                
              
              +
              
                S
                
                  j
                
              
            
            
              M
              
                i
                j
              
            
          
        
      
    
    {\displaystyle R_{ij}={\frac {S_{i}+S_{j}}{M_{ij}}}}
  
,
where 
  
    
      
        
          S
          
            i
          
        
        ,
        
          S
          
            j
          
        
        ,
        
          M
          
            i
            j
          
        
      
    
    {\displaystyle S_{i},S_{j},M_{ij}}
  
 are defined as above, and

  
    
      
        
          
            
              
                
                  
                    R
                    ¯
                  
                
              
              
                =
              
              
                
                  
                    1
                    N
                  
                
                
                  ∑
                  
                    i
                    =
                    1
                  
                  
                    N
                  
                
                
                  max
                  
                    j
                    :
                    j
                    ≠
                    i
                  
                
                
                  R
                  
                    i
                    j
                  
                
              
              
                =
              
              
                
                  
                    1
                    N
                  
                
                
                  ∑
                  
                    i
                    =
                    1
                  
                  
                    N
                  
                
                
                  R
                  
                    i
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{array}{lll}{\overline {R}}&=&{\frac {1}{N}}\sum _{i=1}^{N}\max _{j:j\neq i}R_{ij}&=&{\frac {1}{N}}\sum _{i=1}^{N}R_{i}\end{array}}}
  

If 
  
    
      
        p
        =
        q
        =
        2
      
    
    {\displaystyle p=q=2}
  
, 
  
    
      
        
          R
          
            i
            j
          
        
      
    
    {\displaystyle R_{ij}}
  
 is the reciprocal of the classic Fisher similarity measure calculated for clusters i and cluster j.


== Definition ==
Let Ri,j be a measure of how good the clustering scheme is. This measure, by definition has to account for Mi,j the separation between the ith and the jth cluster, which ideally has to be as large as possible, and Si, the within cluster scatter for cluster i, which has to be as low as possible. Hence the Davies–Bouldin index is defined as the ratio of Si and Mi,j such that these properties are conserved:

  
    
      
        
          R
          
            i
            ,
            j
          
        
        ⩾
        0
      
    
    {\displaystyle R_{i,j}\geqslant 0}
  
.

  
    
      
        
          R
          
            i
            ,
            j
          
        
        =
        
          R
          
            j
            ,
            i
          
        
      
    
    {\displaystyle R_{i,j}=R_{j,i}}
  
.
When 
  
    
      
        
          S
          
            j
          
        
        ⩾
        
          S
          
            k
          
        
      
    
    {\displaystyle S_{j}\geqslant S_{k}}
  
 and 
  
    
      
        
          M
          
            i
            ,
            j
          
        
        =
        
          M
          
            i
            ,
            k
          
        
      
    
    {\displaystyle M_{i,j}=M_{i,k}}
  
 then 
  
    
      
        
          R
          
            i
            ,
            j
          
        
        >
        
          R
          
            i
            ,
            k
          
        
      
    
    {\displaystyle R_{i,j}>R_{i,k}}
  
.
When 
  
    
      
        
          S
          
            j
          
        
        =
        
          S
          
            k
          
        
      
    
    {\displaystyle S_{j}=S_{k}}
  
 and 
  
    
      
        
          M
          
            i
            ,
            j
          
        
        ⩽
        
          M
          
            i
            ,
            k
          
        
      
    
    {\displaystyle M_{i,j}\leqslant M_{i,k}}
  
 then 
  
    
      
        
          R
          
            i
            ,
            j
          
        
        >
        
          R
          
            i
            ,
            k
          
        
      
    
    {\displaystyle R_{i,j}>R_{i,k}}
  
.
With this formulation, the lower the value, the better the separation of the clusters and the 'tightness' inside the clusters.
A solution that satisfies these properties is:

  
    
      
        
          R
          
            i
            ,
            j
          
        
        =
        
          
            
              
                S
                
                  i
                
              
              +
              
                S
                
                  j
                
              
            
            
              M
              
                i
                ,
                j
              
            
          
        
      
    
    {\displaystyle R_{i,j}={\frac {S_{i}+S_{j}}{M_{i,j}}}}
  

This is used to define Di:

  
    
      
        
          R
          
            i
          
        
        ≡
        
          max
          
            j
            ≠
            i
          
        
        
          R
          
            i
            ,
            j
          
        
      
    
    {\displaystyle R_{i}\equiv \max _{j\neq i}R_{i,j}}
  

If N is the number of clusters:

  
    
      
        
          
            R
            ¯
          
        
        ≡
        
          
            1
            N
          
        
        
          
            ∑
            
              i
              =
              1
            
            
              N
            
          
          
            R
            
              i
            
          
        
      
    
    {\displaystyle {\overline {R}}\equiv {\frac {1}{N}}\displaystyle \sum _{i=1}^{N}R_{i}}
  

  
    
      
        
          
            R
            ¯
          
        
      
    
    {\displaystyle {\overline {R}}}
  
 is called the Davies–Bouldin index. This is dependent both on the data as well as the algorithm. 
  
    
      
        
          R
          
            i
          
        
      
    
    {\displaystyle R_{i}}
  
 chooses the worst-case scenario, and this value is equal to Ri,j for the most similar cluster to cluster i. There could be many variations to this formulation, like choosing the average of the cluster similarity, weighted average and so on.


== Explanation ==
Lower index values indicate a better clustering result. The index is improved (lowered) by increased separation between clusters and decreased variation within clusters.
These conditions constrain the index so defined to be symmetric and non-negative. Due to the way it is defined, as a function of the ratio of the within cluster scatter, to the between cluster separation, a lower value will mean that the clustering is better. It happens to be the average similarity between each cluster and its most similar one, averaged over all the clusters, where the similarity is defined as Si above. This affirms the idea that no cluster has to be similar to another, and hence the best clustering scheme essentially minimizes the Davies–Bouldin index. This index thus defined is an average over all the i clusters, and hence a good measure of deciding how many clusters actually exists in the data is to plot it against the number of clusters it is calculated over. The number i for which this value is the lowest is a good measure of the number of clusters the data could be ideally classified into. This has applications in deciding the value of k in the kmeans algorithm, where the value of k is not known apriori.


== Soft version of Davies-Bouldin index ==
Recently, the Davies–Bouldin index has been extended to the domain of soft clustering categories. This extension is based on the category clustering approach according to the framework of fuzzy logic. The starting point for this new version of the validation index is the result of a given soft clustering algorithm (e.g. fuzzy c-means), shaped with the computed clustering partitions and membership values associating the elements with the clusters. In the soft domain, each element of the system belongs to every classes, given the membership values. Therefore, this soft version of the Davies–Bouldin index is able to take into account, in addition to standard validation measures such as compactness and separation of clusters, the degree to which elements belong to classes.


== Implementations ==
The scikit-learn Python open source library provides an implementation of this metric in the sklearn.metrics module.
R provides a similar implementation in its clusterSim package.
A Java implementation is found in ELKI, and can be compared to many other clustering quality indexes.


== See also ==
Silhouette score
Dunn index
Cluster analysis
Calinski-Harabasz index
Determining the number of clusters in a data set
DBCV index


== Further reading ==
Davies, David L.; Bouldin, Donald W. (1979). "A Cluster Separation Measure". IEEE Transactions on Pattern Analysis and Machine Intelligence. PAMI-1 (2): 224–227. doi:10.1109/TPAMI.1979.4766909. S2CID 13254783.
Chicco, Davide; Campagner, Andrea; Spagnolo, Andrea; Ciucci, Davide; Jurman, Giuseppe (2025). "The Silhouette coefficient and the Davies-Bouldin index are more informative than Dunn index, Calinski-Harabasz index, Shannon entropy, and Gap statistic for unsupervised clustering internal evaluation of two convex clusters". PeerJ Computer Science. 11 (e3309): 1–49. doi:10.7717/peerj-cs.3309.


== Notes and references ==