Timothy P. Lillicrap is a Canadian neuroscientist and AI researcher, adjunct professor at University College London, and staff research scientist at Google DeepMind, where he has been involved in the AlphaGo and AlphaZero projects mastering the games of Go, Chess and Shogi. His research focuses on machine learning and statistics for optimal control and decision making, as well as using these mathematical frameworks to understand how the brain learns. He has developed algorithms and approaches for exploiting deep neural networks in the context of reinforcement learning, and new recurrent memory architectures for one-shot learning.
His numerous contributions to the field have earned him a number of honors, including the Governor General's Academic Medal, an NSERC Fellowship, the Centre for Neuroscience Studies Award for Excellence, and numerous European Research Council grants. He has also won a number of Social Learning tournaments.


== Biography ==
Lillicrap attained a B.Sc. in cognitive science and artificial intelligence from University of Toronto in 2005, and a Ph.D. in systems neuroscience from Queen's University in 2012 under Stephen H. Scott. He then went on to become a postdoctoral research fellow at Oxford University, and joined Google DeepMind as a research scientist in 2014. Following a series of promotions, he eventually became a DeepMind staff research scientist in 2016, a position he still holds as of 2021.
In 2016, Lillicrap accepted an adjunct professorship at University College London.


== Select publications ==
Timothy Lillicrap has an extensive publication record. A selection of works is listed below:

Timothy Lillicrap (2014). Modelling Motor Cortex using Neural Network Controls Laws. Ph.D. Systems Neuroscience Thesis, Centre for Neuroscience Studies, Queen's University, advisor: Stephen H. Scott
Timothy Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra (2015). Continuous Control with Deep Reinforcement Learning. arXiv:1509.02971
Nicolas Heess, Jonathan J. Hunt, Timothy Lillicrap, David Silver (2015). Memory-based control with recurrent neural networks. arXiv:1512.04455
David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, Demis Hassabis (2016). Mastering the game of Go with deep neural networks and tree search. Nature, Vol. 529 » AlphaGo
Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu (2016). Asynchronous Methods for Deep Reinforcement Learning.  arXiv:1602.01783v2
Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, Sergey Levine (2016). Continuous Deep Q-Learning with Model-based Acceleration. arXiv:1603.00748
Shixiang Gu, Ethan Holly, Timothy Lillicrap, Sergey Levine (2016). Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Off-Policy Updates. arXiv:1610.00633
Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard E. Turner, Sergey Levine (2016). Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic. arXiv:1611.02247
Yutian Chen, Matthew W. Hoffman, Sergio Gomez Colmenarejo, Misha Denil, Timothy Lillicrap, Matthew Botvinick, Nando de Freitas (2017). Learning to Learn without Gradient Descent by Gradient Descent. arXiv:1611.03824v6, ICML 2017
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, Demis Hassabis (2017). Mastering the game of Go without human knowledge. Nature, Vol. 550
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, Demis Hassabis (2017). Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm. arXiv:1712.01815 » AlphaZero
Jack W. Rae, Chris Dyer, Peter Dayan, Timothy Lillicrap (2018). Fast Parametric Learning with Activation Memorization. arXiv:1803.10049
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, Demis Hassabis (2018). A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. Science, Vol. 362, No. 6419
Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy Lillicrap, David Silver (2019). Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model. arXiv:1911.08265


== Notable awards ==
NSERC Fellowship
Queen's University Graduate Award
Governor General's Academic Medal
Social Learning Strategies Tournament Winner
2nd Social Learning Strategies Tournament Winner
European Research Council Proof of Concept Grant
Centre for Neuroscience Studies Award for Excellence
University College Howard Ferguson Entrance Scholarship
HPCVL / Sun Microsystems of Canada, Inc. Scholarship in Computational Sciences and Engineering


== References ==
 Content in this article was copied from Timothy Lillicrap at the Chess Programming wiki, which is licensed under the Creative Commons Attribution-Share Alike 3.0 (Unported) (CC-BY-SA 3.0) license.


== External links ==

Homepage of Timothy P. Lillicrap
Timothy P. Lillicrap - Google Scholar Citations