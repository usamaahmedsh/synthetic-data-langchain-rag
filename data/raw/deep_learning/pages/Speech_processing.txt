Speech processing is the study of speech signals and the processing methods of  signals. The signals are usually processed in a digital representation, so speech processing can be regarded as a special case of digital signal processing, applied to speech signals. Aspects of speech processing includes the acquisition, manipulation, storage, transfer and output of speech signals. Different speech processing tasks include speech recognition, speech synthesis, speaker diarization, speech enhancement, speaker recognition, etc.


== History ==
Early attempts at speech processing and recognition were primarily focused on understanding a handful of simple phonetic elements such as vowels. In 1952, three researchers at Bell Labs, Stephen. Balashek, R. Biddulph, and K. H. Davis, developed a system that could recognize digits spoken by a single speaker. Pioneering works in field of speech recognition using analysis of its spectrum were reported in the 1940s.
Linear predictive coding (LPC), a speech processing algorithm, was first proposed by Fumitada Itakura of Nagoya University and Shuzo Saito of Nippon Telegraph and Telephone (NTT) in 1966. Further developments in LPC technology were made by Bishnu S. Atal and Manfred R. Schroeder at Bell Labs during the 1970s. LPC was the basis for voice-over-IP (VoIP) technology, as well as speech synthesizer chips, such as the Texas Instruments LPC Speech Chips used in the Speak & Spell toys from 1978.
One of the first commercially available speech recognition products was Dragon Dictate, released in 1990. In 1992, technology developed by Lawrence Rabiner and others at Bell Labs was used by AT&T in their Voice Recognition Call Processing service to route calls without a human operator. By this point, the vocabulary of these systems was larger than the average human vocabulary.
By the early 2000s, the dominant speech processing strategy started to shift away from Hidden Markov Models towards more modern neural networks and deep learning.
In 2012, Geoffrey Hinton and his team at the University of Toronto demonstrated that deep neural networks could significantly outperform traditional HMM-based systems on large vocabulary continuous speech recognition tasks. This breakthrough led to widespread adoption of deep learning techniques in the industry.
By the mid-2010s, companies like Google, Microsoft, Amazon, and Apple had integrated advanced speech recognition systems into their virtual assistants such as Google Assistant, Cortana, Alexa, and Siri. These systems utilized deep learning models to provide more natural and accurate voice interactions.
The development of Transformer-based models, like Google's BERT (Bidirectional Encoder Representations from Transformers) and OpenAI's GPT (Generative Pre-trained Transformer), further pushed the boundaries of natural language processing and speech recognition. These models enabled more context-aware and semantically rich understanding of speech. In recent years, end-to-end speech recognition models have gained popularity. These models simplify the speech recognition pipeline by directly converting audio input into text output, bypassing intermediate steps like feature extraction and acoustic modeling. This approach has streamlined the development process and improved performance.


== Techniques ==


=== Dynamic time warping ===
Dynamic time warping (DTW) is an algorithm for measuring similarity between two temporal sequences, which may vary in speed. In general, DTW is a method that calculates an optimal match between two given sequences (e.g. time series) with certain restriction and rules. The optimal match is denoted by the match that satisfies all the restrictions and the rules and that has the minimal cost, where the cost is computed as the sum of absolute differences, for each matched pair of indices, between their values.


=== Hidden Markov models ===
A hidden Markov model can be represented as the simplest dynamic Bayesian network. The goal of the algorithm is to estimate a hidden variable x(t) given a list of observations y(t). By applying the Markov property, the conditional probability distribution of the hidden variable x(t) at time t, given the values of the hidden variable x at all times, depends only on the value of the hidden variable x(t − 1). Similarly, the value of the observed variable y(t) only depends on the value of the hidden variable x(t) (both at time t).


=== Artificial neural networks ===
An artificial neural network (ANN) is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs.


=== Phase-aware processing ===
Phase is often assumed to be random, but contains useful information. Wrapping of phase: can be introduced due to periodical jumps on 
  
    
      
        2
        π
      
    
    {\displaystyle 2\pi }
  
. Phase unwrapping (see, Chapter 2.3; Instantaneous phase and frequency), it can be expressed as:

  
    
      
        ϕ
        (
        h
        ,
        l
        )
        =
        
          ϕ
          
            l
            i
            n
          
        
        (
        h
        ,
        l
        )
        +
        Ψ
        (
        h
        ,
        l
        )
      
    
    {\displaystyle \phi (h,l)=\phi _{lin}(h,l)+\Psi (h,l)}
  
, where 
  
    
      
        
          ϕ
          
            l
            i
            n
          
        
        (
        h
        ,
        l
        )
        =
        
          ω
          
            0
          
        
        (
        
          l
          ′
        
        )
        
          

          
          
            Δ
          
        
        t
      
    
    {\displaystyle \phi _{lin}(h,l)=\omega _{0}(l'){}_{\Delta }t}
  
 is linear phase (
  
    
      
        
          

          
          
            Δ
          
        
        t
      
    
    {\displaystyle {}_{\Delta }t}
  
 is temporal shift at each frame of analysis), 
  
    
      
        Ψ
        (
        h
        ,
        l
        )
      
    
    {\displaystyle \Psi (h,l)}
  
 is phase contribution of the vocal tract and phase source.
Obtained phase estimations can be used for noise reduction: temporal smoothing of instantaneous phase  and its derivatives by time (instantaneous frequency) and frequency (group delay), smoothing of phase across frequency. Joined amplitude and phase estimators can recover speech more accurately basing on assumption of von Mises distribution of phase.


== Applications ==
Interactive voice response
Virtual Assistants
Voice Identification
Emotion Recognition
Call Center Automation
Robotics


== See also ==
Computational audiology
Neurocomputational speech processing
Speech coding
Speech technology
Natural Language Processing


== References ==