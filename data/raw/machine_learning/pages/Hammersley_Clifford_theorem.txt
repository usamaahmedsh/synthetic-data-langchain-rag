The Hammersley–Clifford theorem is a result in probability theory, mathematical statistics and statistical mechanics that gives necessary and sufficient conditions under which a strictly positive probability distribution can be represented as events generated by a Markov network (also known as a Markov random field). It is the fundamental theorem of random fields. It states that a probability distribution that has a strictly positive mass or density satisfies one of the Markov properties with respect to an undirected graph G if and only if it is a Gibbs random field, that is, its density can be factorized over the cliques (or complete subgraphs) of the graph.
The relationship between Markov and Gibbs random fields was initiated by Roland Dobrushin and Frank Spitzer in the context of statistical mechanics. The theorem is named after John Hammersley and Peter Clifford, who proved the equivalence in an unpublished paper in 1971. Simpler proofs using the inclusion–exclusion principle were given independently by Geoffrey Grimmett, Preston and Sherman in 1973, with a further proof by Julian Besag in 1974.


== Proof outline ==

It is a trivial matter to show that a Gibbs random field satisfies every Markov property. As an example of this fact, see the following:
In the image to the right, a Gibbs random field over the provided graph has the form 
  
    
      
        Pr
        (
        A
        ,
        B
        ,
        C
        ,
        D
        ,
        E
        ,
        F
        )
        ∝
        
          f
          
            1
          
        
        (
        A
        ,
        B
        ,
        D
        )
        
          f
          
            2
          
        
        (
        A
        ,
        C
        ,
        D
        )
        
          f
          
            3
          
        
        (
        C
        ,
        D
        ,
        F
        )
        
          f
          
            4
          
        
        (
        C
        ,
        E
        ,
        F
        )
      
    
    {\displaystyle \Pr(A,B,C,D,E,F)\propto f_{1}(A,B,D)f_{2}(A,C,D)f_{3}(C,D,F)f_{4}(C,E,F)}
  
. If variables 
  
    
      
        C
      
    
    {\displaystyle C}
  
 and 
  
    
      
        D
      
    
    {\displaystyle D}
  
 are fixed, then the global Markov property requires that: 
  
    
      
        A
        ,
        B
        ⊥
        E
        ,
        F
        
          |
        
        C
        ,
        D
      
    
    {\displaystyle A,B\perp E,F|C,D}
  
 (see conditional independence), since 
  
    
      
        C
        ,
        D
      
    
    {\displaystyle C,D}
  
 forms a barrier between 
  
    
      
        A
        ,
        B
      
    
    {\displaystyle A,B}
  
 and 
  
    
      
        E
        ,
        F
      
    
    {\displaystyle E,F}
  
.
With 
  
    
      
        C
      
    
    {\displaystyle C}
  
 and 
  
    
      
        D
      
    
    {\displaystyle D}
  
 constant, 
  
    
      
        Pr
        (
        A
        ,
        B
        ,
        E
        ,
        F
        
          |
        
        C
        =
        c
        ,
        D
        =
        d
        )
        ∝
        [
        
          f
          
            1
          
        
        (
        A
        ,
        B
        ,
        d
        )
        
          f
          
            2
          
        
        (
        A
        ,
        c
        ,
        d
        )
        ]
        ⋅
        [
        
          f
          
            3
          
        
        (
        c
        ,
        d
        ,
        F
        )
        
          f
          
            4
          
        
        (
        c
        ,
        E
        ,
        F
        )
        ]
        =
        
          g
          
            1
          
        
        (
        A
        ,
        B
        )
        
          g
          
            2
          
        
        (
        E
        ,
        F
        )
      
    
    {\displaystyle \Pr(A,B,E,F|C=c,D=d)\propto [f_{1}(A,B,d)f_{2}(A,c,d)]\cdot [f_{3}(c,d,F)f_{4}(c,E,F)]=g_{1}(A,B)g_{2}(E,F)}
  
 where 
  
    
      
        
          g
          
            1
          
        
        (
        A
        ,
        B
        )
        =
        
          f
          
            1
          
        
        (
        A
        ,
        B
        ,
        d
        )
        
          f
          
            2
          
        
        (
        A
        ,
        c
        ,
        d
        )
      
    
    {\displaystyle g_{1}(A,B)=f_{1}(A,B,d)f_{2}(A,c,d)}
  
 and 
  
    
      
        
          g
          
            2
          
        
        (
        E
        ,
        F
        )
        =
        
          f
          
            3
          
        
        (
        c
        ,
        d
        ,
        F
        )
        
          f
          
            4
          
        
        (
        c
        ,
        E
        ,
        F
        )
      
    
    {\displaystyle g_{2}(E,F)=f_{3}(c,d,F)f_{4}(c,E,F)}
  
. This implies that 
  
    
      
        A
        ,
        B
        ⊥
        E
        ,
        F
        
          |
        
        C
        ,
        D
      
    
    {\displaystyle A,B\perp E,F|C,D}
  
.
To establish that every positive probability distribution that satisfies the local Markov property is also a Gibbs random field, the following lemma, which provides a means for combining different factorizations, needs to be proved:

Lemma 1
Let 
  
    
      
        U
      
    
    {\displaystyle U}
  
 denote the set of all random variables under consideration, and let 
  
    
      
        Θ
        ,
        
          Φ
          
            1
          
        
        ,
        
          Φ
          
            2
          
        
        ,
        …
        ,
        
          Φ
          
            n
          
        
        ⊆
        U
      
    
    {\displaystyle \Theta ,\Phi _{1},\Phi _{2},\dots ,\Phi _{n}\subseteq U}
  
 and 
  
    
      
        
          Ψ
          
            1
          
        
        ,
        
          Ψ
          
            2
          
        
        ,
        …
        ,
        
          Ψ
          
            m
          
        
        ⊆
        U
      
    
    {\displaystyle \Psi _{1},\Psi _{2},\dots ,\Psi _{m}\subseteq U}
  
 denote arbitrary sets of variables. (Here, given an arbitrary set of variables 
  
    
      
        X
      
    
    {\displaystyle X}
  
, 
  
    
      
        X
      
    
    {\displaystyle X}
  
 will also denote an arbitrary assignment to the variables from 
  
    
      
        X
      
    
    {\displaystyle X}
  
.)
If

  
    
      
        Pr
        (
        U
        )
        =
        f
        (
        Θ
        )
        
          ∏
          
            i
            =
            1
          
          
            n
          
        
        
          g
          
            i
          
        
        (
        
          Φ
          
            i
          
        
        )
        =
        
          ∏
          
            j
            =
            1
          
          
            m
          
        
        
          h
          
            j
          
        
        (
        
          Ψ
          
            j
          
        
        )
      
    
    {\displaystyle \Pr(U)=f(\Theta )\prod _{i=1}^{n}g_{i}(\Phi _{i})=\prod _{j=1}^{m}h_{j}(\Psi _{j})}
  

for functions 
  
    
      
        f
        ,
        
          g
          
            1
          
        
        ,
        
          g
          
            2
          
        
        ,
        …
        
          g
          
            n
          
        
      
    
    {\displaystyle f,g_{1},g_{2},\dots g_{n}}
  
 and 
  
    
      
        
          h
          
            1
          
        
        ,
        
          h
          
            2
          
        
        ,
        …
        ,
        
          h
          
            m
          
        
      
    
    {\displaystyle h_{1},h_{2},\dots ,h_{m}}
  
, then there exist functions 
  
    
      
        
          h
          
            1
          
          ′
        
        ,
        
          h
          
            2
          
          ′
        
        ,
        …
        ,
        
          h
          
            m
          
          ′
        
      
    
    {\displaystyle h'_{1},h'_{2},\dots ,h'_{m}}
  
 and 
  
    
      
        
          g
          
            1
          
          ′
        
        ,
        
          g
          
            2
          
          ′
        
        ,
        …
        ,
        
          g
          
            n
          
          ′
        
      
    
    {\displaystyle g'_{1},g'_{2},\dots ,g'_{n}}
  
 such that

  
    
      
        Pr
        (
        U
        )
        =
        
          
            (
          
        
        
          ∏
          
            j
            =
            1
          
          
            m
          
        
        
          h
          
            j
          
          ′
        
        (
        Θ
        ∩
        
          Ψ
          
            j
          
        
        )
        
          
            )
          
        
        
          
            (
          
        
        
          ∏
          
            i
            =
            1
          
          
            n
          
        
        
          g
          
            i
          
          ′
        
        (
        
          Φ
          
            i
          
        
        )
        
          
            )
          
        
      
    
    {\displaystyle \Pr(U)={\bigg (}\prod _{j=1}^{m}h'_{j}(\Theta \cap \Psi _{j}){\bigg )}{\bigg (}\prod _{i=1}^{n}g'_{i}(\Phi _{i}){\bigg )}}
  

In other words, 
  
    
      
        
          ∏
          
            j
            =
            1
          
          
            m
          
        
        
          h
          
            j
          
        
        (
        
          Ψ
          
            j
          
        
        )
      
    
    {\displaystyle \prod _{j=1}^{m}h_{j}(\Psi _{j})}
  
 provides a template for further factorization of 
  
    
      
        f
        (
        Θ
        )
      
    
    {\displaystyle f(\Theta )}
  
.

Lemma 1 provides a means of combining two different factorizations of 
  
    
      
        Pr
        (
        U
        )
      
    
    {\displaystyle \Pr(U)}
  
. The local Markov property implies that for any random variable 
  
    
      
        x
        ∈
        U
      
    
    {\displaystyle x\in U}
  
, that there exists factors 
  
    
      
        
          f
          
            x
          
        
      
    
    {\displaystyle f_{x}}
  
 and 
  
    
      
        
          f
          
            −
            x
          
        
      
    
    {\displaystyle f_{-x}}
  
 such that:

  
    
      
        Pr
        (
        U
        )
        =
        
          f
          
            x
          
        
        (
        x
        ,
        ∂
        x
        )
        
          f
          
            −
            x
          
        
        (
        U
        ∖
        {
        x
        }
        )
      
    
    {\displaystyle \Pr(U)=f_{x}(x,\partial x)f_{-x}(U\setminus \{x\})}
  

where 
  
    
      
        ∂
        x
      
    
    {\displaystyle \partial x}
  
 are the neighbors of node 
  
    
      
        x
      
    
    {\displaystyle x}
  
. Applying Lemma 1 repeatedly eventually factors 
  
    
      
        Pr
        (
        U
        )
      
    
    {\displaystyle \Pr(U)}
  
 into a product of clique potentials (see the image on the right).
End of Proof


== See also ==
Markov random field
Conditional random field


== Notes ==


== Further reading ==
Grimmett, Geoffrey (2018), "7.", Probability on Graphs (2nd ed.), Cambridge University Press, ISBN 9781108438179
Langseth, Helge, The Hammersley–Clifford Theorem and its Impact on Modern Statistics (PDF), Department of Mathematical Sciences, Norwegian University of Science and Technology