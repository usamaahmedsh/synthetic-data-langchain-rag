In the mathematical study of stochastic processes, a Harris chain is a Markov chain where the chain returns to a particular part of the state space an unbounded number of times. Harris chains are regenerative processes and are named after Theodore Harris. The theory of Harris chains and Harris recurrence is useful for treating Markov chains on general (possibly uncountably infinite) state spaces.


== Definition ==
Let 
  
    
      
        {
        
          X
          
            n
          
        
        }
      
    
    {\displaystyle \{X_{n}\}}
  
 be a Markov chain on a general state space 
  
    
      
        Ω
      
    
    {\displaystyle \Omega }
  
 with stochastic kernel 
  
    
      
        K
      
    
    {\displaystyle K}
  
. The kernel represents a generalized one-step transition probability law, so that 
  
    
      
        P
        (
        
          X
          
            n
            +
            1
          
        
        ∈
        C
        ∣
        
          X
          
            n
          
        
        =
        x
        )
        =
        K
        (
        x
        ,
        C
        )
      
    
    {\displaystyle P(X_{n+1}\in C\mid X_{n}=x)=K(x,C)}
  
 for all states 
  
    
      
        x
      
    
    {\displaystyle x}
  
 in 
  
    
      
        Ω
      
    
    {\displaystyle \Omega }
  
 and all measurable sets 
  
    
      
        C
        ⊆
        Ω
      
    
    {\displaystyle C\subseteq \Omega }
  
.  The chain 
  
    
      
        {
        
          X
          
            n
          
        
        }
      
    
    {\displaystyle \{X_{n}\}}
  
 is a Harris chain if there exists 
  
    
      
        A
        ⊆
        Ω
        ,
        ε
        >
        0
      
    
    {\displaystyle A\subseteq \Omega ,\varepsilon >0}
  
, and probability measure 
  
    
      
        ρ
      
    
    {\displaystyle \rho }
  
 with 
  
    
      
        ρ
        (
        Ω
        )
        =
        1
      
    
    {\displaystyle \rho (\Omega )=1}
  
 such that 

If 
  
    
      
        
          τ
          
            A
          
        
        :=
        inf
        {
        n
        ≥
        0
        :
        
          X
          
            n
          
        
        ∈
        A
        }
      
    
    {\displaystyle \tau _{A}:=\inf\{n\geq 0:X_{n}\in A\}}
  
, then 
  
    
      
        P
        (
        
          τ
          
            A
          
        
        <
        ∞
        ∣
        
          X
          
            0
          
        
        =
        x
        )
        =
        1
      
    
    {\displaystyle P(\tau _{A}<\infty \mid X_{0}=x)=1}
  
 for all 
  
    
      
        x
        ∈
        Ω
      
    
    {\displaystyle x\in \Omega }
  
.
If 
  
    
      
        x
        ∈
        A
      
    
    {\displaystyle x\in A}
  
 and 
  
    
      
        C
        ⊆
        Ω
      
    
    {\displaystyle C\subseteq \Omega }
  
 (where 
  
    
      
        C
      
    
    {\displaystyle C}
  
 is measurable), then 
  
    
      
        K
        (
        x
        ,
        C
        )
        ≥
        ε
        ρ
        (
        C
        )
      
    
    {\displaystyle K(x,C)\geq \varepsilon \rho (C)}
  
.
The first part of the definition ensures that the chain returns to some state within 
  
    
      
        A
      
    
    {\displaystyle A}
  
 with probability 1, regardless of where it starts. It follows that it visits state 
  
    
      
        A
      
    
    {\displaystyle A}
  
 infinitely often (with probability 1). The second part implies that once the Markov chain is in state 
  
    
      
        A
      
    
    {\displaystyle A}
  
, its next-state can be generated with the help of an independent Bernoulli coin flip. To see this, first note that the parameter 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
  
 must be between 0 and 1 (this can be shown by applying the second part of the definition to the set 
  
    
      
        C
        =
        Ω
      
    
    {\displaystyle C=\Omega }
  
). Now let 
  
    
      
        x
      
    
    {\displaystyle x}
  
 be a point in 
  
    
      
        A
      
    
    {\displaystyle A}
  
 and suppose 
  
    
      
        
          X
          
            n
          
        
        =
        x
      
    
    {\displaystyle X_{n}=x}
  
.  To choose the next state 
  
    
      
        
          X
          
            n
            +
            1
          
        
      
    
    {\displaystyle X_{n+1}}
  
, independently flip a biased coin with success probability 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
  
. If the coin flip is successful, choose the next state 
  
    
      
        
          X
          
            n
            +
            1
          
        
        ∈
        Ω
      
    
    {\displaystyle X_{n+1}\in \Omega }
  
 according to the probability measure 
  
    
      
        ρ
      
    
    {\displaystyle \rho }
  
. Else (and if 
  
    
      
        ε
        <
        1
      
    
    {\displaystyle \varepsilon <1}
  
), choose the next state 
  
    
      
        
          X
          
            n
            +
            1
          
        
      
    
    {\displaystyle X_{n+1}}
  
 according to the measure 
  
    
      
        P
        (
        
          X
          
            n
            +
            1
          
        
        ∈
        C
        ∣
        
          X
          
            n
          
        
        =
        x
        )
        =
        (
        K
        (
        x
        ,
        C
        )
        −
        ε
        ρ
        (
        C
        )
        )
        
          /
        
        (
        1
        −
        ε
        )
      
    
    {\displaystyle P(X_{n+1}\in C\mid X_{n}=x)=(K(x,C)-\varepsilon \rho (C))/(1-\varepsilon )}
  
 (defined for all measurable subsets 
  
    
      
        C
        ⊆
        Ω
      
    
    {\displaystyle C\subseteq \Omega }
  
).
Two random processes 
  
    
      
        {
        
          X
          
            n
          
        
        }
      
    
    {\displaystyle \{X_{n}\}}
  
 and 
  
    
      
        {
        
          Y
          
            n
          
        
        }
      
    
    {\displaystyle \{Y_{n}\}}
  
 that have the same probability law and are Harris chains according to the above definition can be coupled as follows: Suppose that 
  
    
      
        
          X
          
            n
          
        
        =
        x
      
    
    {\displaystyle X_{n}=x}
  
 and 
  
    
      
        
          Y
          
            n
          
        
        =
        y
      
    
    {\displaystyle Y_{n}=y}
  
, where 
  
    
      
        x
      
    
    {\displaystyle x}
  
 and 
  
    
      
        y
      
    
    {\displaystyle y}
  
 are points in 
  
    
      
        A
      
    
    {\displaystyle A}
  
. Using the same coin flip to decide the next-state of both processes, it follows that the next states are the same with probability at least 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
  
.


== Examples ==


=== Example 1: Countable state space ===
Let Ω be a countable state space. The kernel K is defined by the one-step conditional transition probabilities P[Xn+1 = y | Xn = x] for x,y ∈ Ω.  The measure ρ is a probability mass function on the states, so that ρ(x) ≥ 0 for all x ∈ Ω, and the sum of the ρ(x) probabilities is equal to one.  Suppose the above definition is satisfied for 
a given set A ⊆ Ω and a given parameter ε > 0.   Then P[Xn+1 = c | Xn = x] ≥ ερ(c) for all x ∈ A and all c ∈ Ω.


=== Example 2: Chains with continuous densities ===
Let {Xn}, Xn ∈ Rd be a Markov chain with a kernel that is absolutely continuous with respect to Lebesgue measure:

K(x, dy) = K(x, y) dy
such that K(x, y) is a continuous function.
Pick (x0, y0) such that K(x0, y0 ) > 0, and let A and Ω be open sets containing x0 and y0 respectively that are sufficiently small so that K(x, y) ≥ ε > 0 on A ×  Ω. Letting ρ(C) = |Ω ∩ C|/|Ω| where |Ω| is the Lebesgue measure of Ω, we have that (2) in the above definition holds.  If (1) holds, then {Xn} is a Harris chain.


== Reducibility and periodicity ==
In the following 
  
    
      
        R
        :=
        inf
        {
        n
        ≥
        1
        :
        
          X
          
            n
          
        
        ∈
        A
        }
      
    
    {\displaystyle R:=\inf\{n\geq 1:X_{n}\in A\}}
  
; i.e. 
  
    
      
        R
      
    
    {\displaystyle R}
  
 is the first time after time 0 that the process enters region 
  
    
      
        A
      
    
    {\displaystyle A}
  
. Let 
  
    
      
        μ
      
    
    {\displaystyle \mu }
  
 denote the initial distribution of the Markov chain, i.e. 
  
    
      
        
          X
          
            0
          
        
        ∼
        μ
      
    
    {\displaystyle X_{0}\sim \mu }
  
. 
Definition:   If for all 
  
    
      
        μ
      
    
    {\displaystyle \mu }
  
, 
  
    
      
        P
        [
        R
        <
        ∞
        
          |
        
        
          X
          
            0
          
        
        ∈
        A
        ]
        =
        1
      
    
    {\displaystyle P[R<\infty |X_{0}\in A]=1}
  
, then the Harris chain is called recurrent.
Definition:  A recurrent Harris chain 
  
    
      
        
          X
          
            n
          
        
      
    
    {\displaystyle X_{n}}
  
 is aperiodic if 
  
    
      
        ∃
        N
      
    
    {\displaystyle \exists N}
  
, such that 
  
    
      
        ∀
        n
        ≥
        N
      
    
    {\displaystyle \forall n\geq N}
  
, 
  
    
      
        ∀
        μ
        ,
        P
        [
        
          X
          
            n
          
        
        ∈
        A
        
          |
        
        
          X
          
            0
          
        
        ∈
        A
        ]
        >
        0
      
    
    {\displaystyle \forall \mu ,P[X_{n}\in A|X_{0}\in A]>0}
  

Theorem: Let 
  
    
      
        
          X
          
            n
          
        
      
    
    {\displaystyle X_{n}}
  
 be an aperiodic recurrent Harris chain with stationary distribution 
  
    
      
        π
      
    
    {\displaystyle \pi }
  
. If  
  
    
      
        P
        [
        R
        <
        ∞
        
          |
        
        
          X
          
            0
          
        
        ∈
        A
        ]
        =
        1
      
    
    {\displaystyle P[R<\infty |X_{0}\in A]=1}
  
 then as 
  
    
      
        n
        →
        ∞
      
    
    {\displaystyle n\rightarrow \infty }
  
, 
  
    
      
        
          |
        
        
          |
        
        
          
            L
          
        
        (
        
          X
          
            n
          
        
        
          |
        
        
          X
          
            0
          
        
        )
        −
        π
        
          |
        
        
          
            |
          
          
            T
            V
          
        
        →
        0
      
    
    {\displaystyle ||{\mathcal {L}}(X_{n}|X_{0})-\pi ||_{TV}\rightarrow 0}
  
 where 
  
    
      
        
          |
        
        
          |
        
        ⋅
        
          |
        
        
          
            |
          
          
            T
            V
          
        
      
    
    {\displaystyle ||\cdot ||_{TV}}
  
 denotes the total variation for signed measures defined on the same measurable space.


== References ==