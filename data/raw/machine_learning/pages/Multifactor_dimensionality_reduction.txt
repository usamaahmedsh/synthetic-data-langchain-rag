Multifactor dimensionality reduction (MDR) is a statistical approach, also used in machine learning automatic approaches, for detecting and characterizing combinations of attributes or independent variables that interact to influence a dependent or class variable. MDR was designed specifically to identify nonadditive interactions among discrete variables that influence a binary outcome and is considered a nonparametric and model-free alternative to traditional statistical methods such as logistic regression.
The basis of the MDR method is a constructive induction or feature engineering algorithm that converts two or more variables or attributes to a single attribute. This process of constructing a new attribute changes the representation space of the data. The end goal is to create or discover a representation that facilitates the detection of nonlinear or nonadditive interactions among the attributes such that prediction of the class variable is improved over that of the original representation of the data.


== Illustrative example ==
Consider the following simple example using the exclusive OR (XOR) function.  XOR is a logical operator that is commonly used in data mining and machine learning as an example of a function that is not linearly separable.  The table below represents a simple dataset where the relationship between the attributes (X1 and X2) and the class variable (Y) is defined by the XOR function such that Y = X1 XOR X2.
Table 1

A machine learning algorithm would need to discover or approximate the XOR function in order to accurately predict Y using information about X1 and X2.  An alternative strategy would be to first change the representation of the data using constructive induction to facilitate predictive modeling. The MDR algorithm would change the representation of the data (X1 and X2) in the following manner.  MDR starts by selecting two attributes.  In this simple example, X1 and X2 are selected.  Each combination of values for X1 and X2 are examined and the number of times Y=1 and/or Y=0 is counted.  In this simple example, Y=1 occurs zero times and Y=0 occurs once for the combination of X1=0 and X2=0.  With MDR, the ratio of these counts is computed and compared to a fixed threshold.  Here, the ratio of counts is 0/1 which is less than our fixed threshold of 1.  Since 0/1 < 1 we encode a new attribute (Z) as a 0.  When the ratio is greater than one we encode Z as a 1.  This process is repeated for all unique combinations of values for X1 and X2.  Table 2 illustrates our new transformation of the data.
Table 2

The machine learning algorithm now has much less work to do to find a good predictive function.  In fact, in this very simple example, the function Y = Z has a classification accuracy of 1.  A nice feature of constructive induction methods such as MDR is the ability to use any data mining or machine learning method to analyze the new representation of the data.  Decision trees, neural networks, or a naive Bayes classifier could be used in combination with measures of model quality such as balanced accuracy and mutual information.


== Machine learning with MDR ==
As illustrated above, the basic constructive induction algorithm in MDR is very simple.  However, its implementation for mining patterns from real data can be computationally complex.  As with any machine learning algorithm there is always concern about overfitting.  That is, machine learning algorithms are good at finding patterns in completely random data.  It is often difficult to determine whether a reported pattern is an important signal or just chance.  One approach is to estimate the generalizability of a model to independent datasets using methods such as cross-validation. Models that describe random data typically don't generalize.  Another approach is to generate many random permutations of the data to see what the data mining algorithm finds when given the chance to overfit.  Permutation testing makes it possible to generate an empirical p-value for the result. Replication in independent data may also provide evidence for an MDR model but can be sensitive to difference in the data sets. These approaches have all been shown to be useful for choosing and evaluating MDR models. An important step in a machine learning exercise is interpretation. Several approaches have been used with MDR including entropy analysis and pathway analysis. Tips and approaches for using MDR to model gene-gene interactions have been reviewed.


== Extensions to MDR ==
Numerous extensions to MDR have been introduced. These include family-based methods, fuzzy methods, covariate adjustment, odds ratios, risk scores, survival methods, robust methods, methods for quantitative traits, and many others.


== Applications of MDR ==
MDR has mostly been applied to detecting gene-gene interactions or epistasis in genetic studies of common human diseases such as atrial fibrillation, autism, bladder cancer, breast cancer, cardiovascular disease, hypertension, obesity, pancreatic cancer, prostate cancer and tuberculosis. It has also been applied to other biomedical problems such as the genetic analysis of pharmacology outcomes. A central challenge is the scaling of MDR to big data such as that from genome-wide association studies (GWAS). Several approaches have been used. One approach is to filter the features prior to MDR analysis. This can be done using biological knowledge through tools such as BioFilter. It can also be done using computational tools such as ReliefF. Another approach is to use stochastic search algorithms such as genetic programming to explore the search space of feature combinations. Yet another approach is a brute-force search using high-performance computing.


== Implementations ==
www.epistasis.org provides an open-source and freely-available MDR software package.
An R package for MDR.
An sklearn-compatible Python implementation.
An R package for Model-Based MDR.
MDR in Weka.
Generalized MDR.


== See also ==
Data mining
Dimensionality reduction
Epistasis
Feature Engineering
Machine learning
Multilinear subspace learning


== References ==


== Further reading ==
Michalski, R. S., "Pattern Recognition as Knowledge-Guided Computer Induction," Department of Computer Science Reports, No. 927, University of Illinois, Urbana, June 1978.