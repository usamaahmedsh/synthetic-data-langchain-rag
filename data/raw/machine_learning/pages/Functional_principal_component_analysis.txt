Functional principal component analysis (FPCA) is a statistical method for investigating the dominant modes of variation of functional data. Using this method,  a random function is represented in the eigenbasis, which is an orthonormal basis of the Hilbert space L2 that consists of the  eigenfunctions of the autocovariance operator. FPCA represents functional data in the most parsimonious way, in the sense that when using a fixed number of basis functions, the eigenfunction basis explains more variation than any other basis expansion. FPCA can be applied for representing random functions, or in functional regression and classification.


== Formulation ==
For a square-integrable stochastic process X(t), t ‚àà  ùíØ, let 

  
    
      
        Œº
        (
        t
        )
        =
        
          E
        
        (
        X
        (
        t
        )
        )
      
    
    {\displaystyle \mu (t)={\text{E}}(X(t))}
  

and 

  
    
      
        G
        (
        s
        ,
        t
        )
        =
        
          Cov
        
        (
        X
        (
        s
        )
        ,
        X
        (
        t
        )
        )
        =
        
          ‚àë
          
            k
            =
            1
          
          
            ‚àû
          
        
        
          Œª
          
            k
          
        
        
          œÜ
          
            k
          
        
        (
        s
        )
        
          œÜ
          
            k
          
        
        (
        t
        )
        ,
      
    
    {\displaystyle G(s,t)={\text{Cov}}(X(s),X(t))=\sum _{k=1}^{\infty }\lambda _{k}\varphi _{k}(s)\varphi _{k}(t),}
  

where 
  
    
      
        
          Œª
          
            1
          
        
        ‚â•
        
          Œª
          
            2
          
        
        ‚â•
        .
        .
        .
        ‚â•
        0
      
    
    {\displaystyle \lambda _{1}\geq \lambda _{2}\geq ...\geq 0}
  
 are the eigenvalues and 
  
    
      
        
          œÜ
          
            1
          
        
      
    
    {\displaystyle \varphi _{1}}
  
, 
  
    
      
        
          œÜ
          
            2
          
        
      
    
    {\displaystyle \varphi _{2}}
  
, ... are the orthonormal eigenfunctions of the linear Hilbert‚ÄìSchmidt operator

  
    
      
        G
        :
        
          L
          
            2
          
        
        (
        
          
            T
          
        
        )
        ‚Üí
        
          L
          
            2
          
        
        (
        
          
            T
          
        
        )
        ,
        
        G
        (
        f
        )
        =
        
          ‚à´
          
            
              T
            
          
        
        G
        (
        s
        ,
        t
        )
        f
        (
        s
        )
        d
        s
        .
      
    
    {\displaystyle G:L^{2}({\mathcal {T}})\rightarrow L^{2}({\mathcal {T}}),\,G(f)=\int _{\mathcal {T}}G(s,t)f(s)ds.}
  

By the Karhunen‚ÄìLo√®ve theorem, one can express the centered process in the eigenbasis, 

  
    
      
        X
        (
        t
        )
        ‚àí
        Œº
        (
        t
        )
        =
        
          ‚àë
          
            k
            =
            1
          
          
            ‚àû
          
        
        
          Œæ
          
            k
          
        
        
          œÜ
          
            k
          
        
        (
        t
        )
        ,
      
    
    {\displaystyle X(t)-\mu (t)=\sum _{k=1}^{\infty }\xi _{k}\varphi _{k}(t),}
  

where 

  
    
      
        
          Œæ
          
            k
          
        
        =
        
          ‚à´
          
            
              T
            
          
        
        (
        X
        (
        t
        )
        ‚àí
        Œº
        (
        t
        )
        )
        
          œÜ
          
            k
          
        
        (
        t
        )
        d
        t
      
    
    {\displaystyle \xi _{k}=\int _{\mathcal {T}}(X(t)-\mu (t))\varphi _{k}(t)dt}
  

is the principal component associated with the k-th eigenfunction 
  
    
      
        
          œÜ
          
            k
          
        
      
    
    {\displaystyle \varphi _{k}}
  
, with the properties

  
    
      
        
          E
        
        (
        
          Œæ
          
            k
          
        
        )
        =
        0
        ,
        
          Var
        
        (
        
          Œæ
          
            k
          
        
        )
        =
        
          Œª
          
            k
          
        
        
           and 
        
        
          E
        
        (
        
          Œæ
          
            k
          
        
        
          Œæ
          
            l
          
        
        )
        =
        0
        
           for 
        
        k
        ‚â†
        l
        .
      
    
    {\displaystyle {\text{E}}(\xi _{k})=0,{\text{Var}}(\xi _{k})=\lambda _{k}{\text{ and }}{\text{E}}(\xi _{k}\xi _{l})=0{\text{ for }}k\neq l.}
  

The centered process is then equivalent to Œæ1, Œæ2, .... A common assumption is that X can be represented by only the first few eigenfunctions (after subtracting the mean function), i.e.

  
    
      
        X
        (
        t
        )
        ‚âà
        
          X
          
            m
          
        
        (
        t
        )
        =
        Œº
        (
        t
        )
        +
        
          ‚àë
          
            k
            =
            1
          
          
            m
          
        
        
          Œæ
          
            k
          
        
        
          œÜ
          
            k
          
        
        (
        t
        )
        ,
      
    
    {\displaystyle X(t)\approx X_{m}(t)=\mu (t)+\sum _{k=1}^{m}\xi _{k}\varphi _{k}(t),}
  

where

  
    
      
        
          E
        
        
          (
          
            
              ‚à´
              
                
                  T
                
              
            
            
              
                (
                
                  X
                  (
                  t
                  )
                  ‚àí
                  
                    X
                    
                      m
                    
                  
                  (
                  t
                  )
                
                )
              
              
                2
              
            
            d
            t
          
          )
        
        =
        
          ‚àë
          
            j
            >
            m
          
        
        
          Œª
          
            j
          
        
        ‚Üí
        0
        
           as 
        
        m
        ‚Üí
        ‚àû
        .
      
    
    {\displaystyle \mathrm {E} \left(\int _{\mathcal {T}}\left(X(t)-X_{m}(t)\right)^{2}dt\right)=\sum _{j>m}\lambda _{j}\rightarrow 0{\text{ as }}m\rightarrow \infty .}
  


== Interpretation of eigenfunctions ==
The first eigenfunction 
  
    
      
        
          œÜ
          
            1
          
        
      
    
    {\displaystyle \varphi _{1}}
  
 depicts the dominant mode of variation of X. 

  
    
      
        
          œÜ
          
            1
          
        
        =
        
          
            
              a
              r
              g
              
              m
              a
              x
            
            
              ‚Äñ
              
                œÜ
              
              ‚Äñ
              =
              1
            
          
        
        
          {
          
            Var
            ‚Å°
            
              (
              
                
                  ‚à´
                  
                    
                      T
                    
                  
                
                (
                X
                (
                t
                )
                ‚àí
                Œº
                (
                t
                )
                )
                œÜ
                (
                t
                )
                d
                t
              
              )
            
          
          }
        
        ,
      
    
    {\displaystyle \varphi _{1}={\underset {\Vert \mathbf {\varphi } \Vert =1}{\operatorname {arg\,max} }}\left\{\operatorname {Var} \left(\int _{\mathcal {T}}(X(t)-\mu (t))\varphi (t)dt\right)\right\},}
  

where

  
    
      
        ‚Äñ
        
          œÜ
        
        ‚Äñ
        =
        
          
            (
            
              
                ‚à´
                
                  
                    T
                  
                
              
              œÜ
              (
              t
              
                )
                
                  2
                
              
              d
              t
            
            )
          
          
            
              1
              2
            
          
        
        .
      
    
    {\displaystyle \Vert \mathbf {\varphi } \Vert =\left(\int _{\mathcal {T}}\varphi (t)^{2}dt\right)^{\frac {1}{2}}.}
  

The k-th eigenfunction 
  
    
      
        
          œÜ
          
            k
          
        
      
    
    {\displaystyle \varphi _{k}}
  
 is the dominant mode of variation orthogonal to 
  
    
      
        
          œÜ
          
            1
          
        
      
    
    {\displaystyle \varphi _{1}}
  
, 
  
    
      
        
          œÜ
          
            2
          
        
      
    
    {\displaystyle \varphi _{2}}
  
, ... , 
  
    
      
        
          œÜ
          
            k
            ‚àí
            1
          
        
      
    
    {\displaystyle \varphi _{k-1}}
  
,

  
    
      
        
          œÜ
          
            k
          
        
        =
        
          
            
              a
              r
              g
              
              m
              a
              x
            
            
              ‚Äñ
              
                œÜ
              
              ‚Äñ
              =
              1
              ,
              ‚ü®
              œÜ
              ,
              
                œÜ
                
                  j
                
              
              ‚ü©
              =
              0
              
                 for 
              
              j
              =
              1
              ,
              ‚Ä¶
              ,
              k
              ‚àí
              1
            
          
        
        
          {
          
            Var
            ‚Å°
            
              (
              
                
                  ‚à´
                  
                    
                      T
                    
                  
                
                (
                X
                (
                t
                )
                ‚àí
                Œº
                (
                t
                )
                )
                œÜ
                (
                t
                )
                d
                t
              
              )
            
          
          }
        
        ,
      
    
    {\displaystyle \varphi _{k}={\underset {\Vert \mathbf {\varphi } \Vert =1,\langle \varphi ,\varphi _{j}\rangle =0{\text{ for }}j=1,\dots ,k-1}{\operatorname {arg\,max} }}\left\{\operatorname {Var} \left(\int _{\mathcal {T}}(X(t)-\mu (t))\varphi (t)dt\right)\right\},}
  

where

  
    
      
        ‚ü®
        œÜ
        ,
        
          œÜ
          
            j
          
        
        ‚ü©
        =
        
          ‚à´
          
            
              T
            
          
        
        œÜ
        (
        t
        )
        
          œÜ
          
            j
          
        
        (
        t
        )
        d
        t
        ,
        
           for 
        
        j
        =
        1
        ,
        ‚Ä¶
        ,
        k
        ‚àí
        1.
      
    
    {\displaystyle \langle \varphi ,\varphi _{j}\rangle =\int _{\mathcal {T}}\varphi (t)\varphi _{j}(t)dt,{\text{ for }}j=1,\dots ,k-1.}
  


== Estimation ==
Let Yij = Xi(tij) + Œµij be the observations made at locations (usually time points) tij, where Xi is the i-th realization of the smooth stochastic process that generates the data, and Œµij are identically and independently distributed normal random variable with mean 0 and variance œÉ2, j = 1, 2, ..., mi. To obtain an estimate of the mean function Œº(tij), if a dense sample on a regular grid is available, one may take the average at each location tij:

  
    
      
        
          
            
              Œº
              ^
            
          
        
        (
        
          t
          
            i
            j
          
        
        )
        =
        
          
            1
            n
          
        
        
          ‚àë
          
            i
            =
            1
          
          
            n
          
        
        
          Y
          
            i
            j
          
        
        .
      
    
    {\displaystyle {\hat {\mu }}(t_{ij})={\frac {1}{n}}\sum _{i=1}^{n}Y_{ij}.}
  

If the observations are sparse, one needs to smooth the data pooled from all observations to obtain the mean estimate, using smoothing methods like local linear smoothing or spline smoothing.
Then the estimate of the covariance function 
  
    
      
        
          
            
              G
              ^
            
          
        
        (
        s
        ,
        t
        )
      
    
    {\displaystyle {\hat {G}}(s,t)}
  
 is obtained by averaging (in the dense case) or smoothing (in the sparse case) the raw covariances

  
    
      
        
          G
          
            i
          
        
        (
        
          t
          
            i
            j
          
        
        ,
        
          t
          
            i
            l
          
        
        )
        =
        (
        
          Y
          
            i
            j
          
        
        ‚àí
        
          
            
              Œº
              ^
            
          
        
        (
        
          t
          
            i
            j
          
        
        )
        )
        (
        
          Y
          
            i
            l
          
        
        ‚àí
        
          
            
              Œº
              ^
            
          
        
        (
        
          t
          
            i
            l
          
        
        )
        )
        ,
        j
        ‚â†
        l
        ,
        i
        =
        1
        ,
        ‚Ä¶
        ,
        n
        .
      
    
    {\displaystyle G_{i}(t_{ij},t_{il})=(Y_{ij}-{\hat {\mu }}(t_{ij}))(Y_{il}-{\hat {\mu }}(t_{il})),j\neq l,i=1,\dots ,n.}
  

Note that the diagonal elements of Gi should be removed because they contain measurement error.
In practice, 
  
    
      
        
          
            
              G
              ^
            
          
        
        (
        s
        ,
        t
        )
      
    
    {\displaystyle {\hat {G}}(s,t)}
  
 is discretized to an equal-spaced dense grid, and the estimation of eigenvalues Œªk and eigenvectors vk is carried out by numerical linear algebra. The eigenfunction estimates 
  
    
      
        
          
            
              
                œÜ
                ^
              
            
          
          
            k
          
        
      
    
    {\displaystyle {\hat {\varphi }}_{k}}
  
 can then be obtained by interpolating the eigenvectors 
  
    
      
        
          
            
              
                v
                
                  k
                
              
              ^
            
          
        
        .
      
    
    {\displaystyle {\hat {v_{k}}}.}
  

The fitted covariance should be positive definite and symmetric and is then obtained as

  
    
      
        
          
            
              G
              ~
            
          
        
        (
        s
        ,
        t
        )
        =
        
          ‚àë
          
            
              Œª
              
                k
              
            
            >
            0
          
        
        
          
            
              
                Œª
                ^
              
            
          
          
            k
          
        
        
          
            
              
                œÜ
                ^
              
            
          
          
            k
          
        
        (
        s
        )
        
          
            
              
                œÜ
                ^
              
            
          
          
            k
          
        
        (
        t
        )
        .
      
    
    {\displaystyle {\tilde {G}}(s,t)=\sum _{\lambda _{k}>0}{\hat {\lambda }}_{k}{\hat {\varphi }}_{k}(s){\hat {\varphi }}_{k}(t).}
  

Let 
  
    
      
        
          
            
              V
              ^
            
          
        
        (
        t
        )
      
    
    {\displaystyle {\hat {V}}(t)}
  
 be a smoothed version of the diagonal elements Gi(tij, tij) of the raw covariance matrices. Then 
  
    
      
        
          
            
              V
              ^
            
          
        
        (
        t
        )
      
    
    {\displaystyle {\hat {V}}(t)}
  
 is an estimate of (G(t, t) + œÉ2). An estimate of œÉ2 is obtained by

  
    
      
        
          
            
              
                œÉ
                ^
              
            
          
          
            2
          
        
        =
        
          
            2
            
              
                |
              
              
                
                  T
                
              
              
                |
              
            
          
        
        
          ‚à´
          
            
              T
            
          
        
        (
        
          
            
              V
              ^
            
          
        
        (
        t
        )
        ‚àí
        
          
            
              G
              ~
            
          
        
        (
        t
        ,
        t
        )
        )
        d
        t
        ,
      
    
    {\displaystyle {\hat {\sigma }}^{2}={\frac {2}{|{\mathcal {T}}|}}\int _{\mathcal {T}}({\hat {V}}(t)-{\tilde {G}}(t,t))dt,}
  
 if 
  
    
      
        
          
            
              
                œÉ
                ^
              
            
          
          
            2
          
        
        >
        0
        ;
      
    
    {\displaystyle {\hat {\sigma }}^{2}>0;}
  
 otherwise 
  
    
      
        
          
            
              
                œÉ
                ^
              
            
          
          
            2
          
        
        =
        0.
      
    
    {\displaystyle {\hat {\sigma }}^{2}=0.}
  

If the observations Xij, j=1, 2, ..., mi are dense in ùíØ, then the k-th FPC Œæk can be estimated by numerical integration, implementing

  
    
      
        
          
            
              
                Œæ
                ^
              
            
          
          
            k
          
        
        =
        ‚ü®
        X
        ‚àí
        
          
            
              Œº
              ^
            
          
        
        ,
        
          
            
              
                œÜ
                ^
              
            
          
          
            k
          
        
        ‚ü©
        .
      
    
    {\displaystyle {\hat {\xi }}_{k}=\langle X-{\hat {\mu }},{\hat {\varphi }}_{k}\rangle .}
  

However, if the observations are sparse, this method will not work. Instead, one can use best linear unbiased predictors, yielding 

  
    
      
        
          
            
              
                Œæ
                ^
              
            
          
          
            k
          
        
        =
        
          
            
              
                Œª
                ^
              
            
          
          
            k
          
        
        
          
            
              
                œÜ
                ^
              
            
          
          
            k
          
          
            T
          
        
        
          
            
              
                Œ£
                ^
              
            
          
          
            
              Y
              
                i
              
            
          
          
            ‚àí
            1
          
        
        (
        
          Y
          
            i
          
        
        ‚àí
        
          
            
              Œº
              ^
            
          
        
        )
        ,
      
    
    {\displaystyle {\hat {\xi }}_{k}={\hat {\lambda }}_{k}{\hat {\varphi }}_{k}^{T}{\hat {\Sigma }}_{Y_{i}}^{-1}(Y_{i}-{\hat {\mu }}),}
  

where

  
    
      
        
          
            
              
                Œ£
                ^
              
            
          
          
            
              Y
              
                i
              
            
          
        
        =
        
          
            
              G
              ~
            
          
        
        +
        
          
            
              
                œÉ
                ^
              
            
          
          
            2
          
        
        
          
            I
          
          
            
              m
              
                i
              
            
          
        
      
    
    {\displaystyle {\hat {\Sigma }}_{Y_{i}}={\tilde {G}}+{\hat {\sigma }}^{2}\mathbf {I} _{m_{i}}}
  
,
and 
  
    
      
        
          
            
              G
              ~
            
          
        
      
    
    {\displaystyle {\tilde {G}}}
  
 is evaluated at the grid points generated by tij, j = 1, 2, ..., mi. The algorithm, PACE, has an available Matlab package and R package
Asymptotic convergence properties of these estimates have been investigated.


== Applications ==
FPCA can be applied for displaying the modes of functional variation, in scatterplots of FPCs against each other or of responses against FPCs, for modeling sparse longitudinal data, or for functional regression and classification (e.g., functional linear regression). Scree plots and other methods can be used to determine the number of components included. Functional Principal component analysis has varied applications in time series analysis. At present, this method is being adapted from traditional multivariate techniques to analyze financial data sets such as stock market indices and generate implied volatility graphs. A good example of advantages of the functional approach is the Smoothed FPCA (SPCA), developed by Silverman [1996] and studied by Pezzulli and Silverman [1993], that enables direct combination of FPCA along with a general smoothing approach that makes using the information stored in some linear differential operators possible. An important application of the FPCA already known from multivariate PCA is motivated by the Karhunen-Lo√®ve decomposition of a random function to the set of functional parameters ‚Äì factor functions and corresponding factor loadings (scalar random variables). This application is much more important than in the standard multivariate PCA since the distribution of the random function is in general too complex to be directly analyzed and the Karhunen-Lo√®ve decomposition reduces the analysis to the interpretation of the factor functions and the distribution of scalar random variables. Due to dimensionality reduction as well as its accuracy to represent data, there is a wide scope for further developments of functional principal component techniques in the financial field.
Applications of PCA in automotive engineering.


== Connection with principal component analysis ==
The following table shows a comparison of various elements of principal component analysis (PCA) and FPCA. The two methods are both used for dimensionality reduction. In implementations, FPCA uses a PCA step.
However, PCA and FPCA differ in some critical aspects. First, the order of multivariate data in PCA can be permuted, which has no effect on the analysis, but the order of functional data carries time or space information and cannot be reordered. Second, the spacing of observations in FPCA matters, while there is no spacing issue in PCA. Third, regular PCA does not work for high-dimensional data without regularization, while FPCA has a built-in regularization due to the smoothness of the functional data and the truncation to a finite number of included components.


== See also ==
Principal component analysis


== Notes ==


== References ==
James O. Ramsay; B. W. Silverman (8 June 2005). Functional Data Analysis. Springer. ISBN 978-0-387-40080-8.