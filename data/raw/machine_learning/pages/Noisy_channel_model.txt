The noisy channel model is a framework used in spell checkers, question answering, speech recognition, and machine translation. In this model, the goal is to find the intended word given a word where the letters have been scrambled in some manner.


== In spell-checking ==
See Chapter B of.
Given an alphabet 
  
    
      
        Σ
      
    
    {\displaystyle \Sigma }
  
, let 
  
    
      
        
          Σ
          
            ∗
          
        
      
    
    {\displaystyle \Sigma ^{*}}
  
 be the set of all finite strings over 
  
    
      
        Σ
      
    
    {\displaystyle \Sigma }
  
. Let the dictionary 
  
    
      
        D
      
    
    {\displaystyle D}
  
 of valid words be some subset of 
  
    
      
        
          Σ
          
            ∗
          
        
      
    
    {\displaystyle \Sigma ^{*}}
  
, i.e.,

  
    
      
        D
        ⊆
        
          Σ
          
            ∗
          
        
      
    
    {\displaystyle D\subseteq \Sigma ^{*}}
  
.
The noisy channel is the matrix

  
    
      
        
          Γ
          
            w
            s
          
        
        =
        Pr
        (
        s
        
          |
        
        w
        )
      
    
    {\displaystyle \Gamma _{ws}=\Pr(s|w)}
  
,
where 
  
    
      
        w
        ∈
        D
      
    
    {\displaystyle w\in D}
  
 is the intended word and 
  
    
      
        s
        ∈
        
          Σ
          
            ∗
          
        
      
    
    {\displaystyle s\in \Sigma ^{*}}
  
 is the scrambled word that was actually received.
The goal of the noisy channel model is to find the intended word given the scrambled word that was received. The decision function 
  
    
      
        σ
        :
        
          Σ
          
            ∗
          
        
        →
        D
      
    
    {\displaystyle \sigma :\Sigma ^{*}\to D}
  
 is a function that, given a scrambled word, returns
the intended word.
Methods of constructing a decision function include the maximum likelihood rule, the maximum a posteriori rule, and the minimum distance rule.
In some cases, it may be better to accept the scrambled word as the intended
word rather than attempt to find an intended word in the dictionary. For
example, the word schönfinkeling may not be in the dictionary, but might
in fact be the intended word.


=== Example ===
Consider the English alphabet

  
    
      
        Σ
        =
        {
        a
        ,
        b
        ,
        c
        ,
        .
        .
        .
        ,
        y
        ,
        z
        ,
        A
        ,
        B
        ,
        .
        .
        .
        ,
        Z
        ,
        .
        .
        .
        }
      
    
    {\displaystyle \Sigma =\{a,b,c,...,y,z,A,B,...,Z,...\}}
  
. Some subset

  
    
      
        D
        ⊆
        
          Σ
          
            ∗
          
        
      
    
    {\displaystyle D\subseteq \Sigma ^{*}}
  
 makes up the dictionary of valid English
words.
There are several mistakes that may occur while typing, including:

Missing letters, e.g., leter instead of letter
Accidental letter additions, e.g., misstake instead of mistake
Swapping letters, e.g., recieved instead of received
Replacing letters, e.g., fimite instead of finite
To construct the noisy channel matrix 
  
    
      
        Γ
      
    
    {\displaystyle \Gamma }
  
, we must consider
the probability of each mistake, given the intended word (
  
    
      
        Pr
        (
        s
        
          |
        
        w
        )
      
    
    {\displaystyle \Pr(s|w)}
  
 for all 
  
    
      
        w
        ∈
        D
      
    
    {\displaystyle w\in D}
  
 and 
  
    
      
        s
        ∈
        
          Σ
          
            ∗
          
        
      
    
    {\displaystyle s\in \Sigma ^{*}}
  
). These probabilities may be gathered, for example, by considering the Damerau–Levenshtein distance between 
  
    
      
        s
      
    
    {\displaystyle s}
  
 and 
  
    
      
        w
      
    
    {\displaystyle w}
  
 or by comparing the draft of an essay with one that has been manually edited for spelling.


== In machine translation ==
One naturally wonders if the problem of translation could conceivably be treated as a problem in cryptography. When I look at an article in Russian, I say: 'This is really written in English, but it has been coded in some strange symbols. I will now proceed to decode.
See chapter 1, and chapter 25 of.
Suppose we want to translate a foreign language to English, we could model 
  
    
      
        P
        (
        E
        
          |
        
        F
        )
      
    
    {\displaystyle P(E|F)}
  
 directly: the probability that we have English sentence E given foreign sentence F, then we pick the most likely one 
  
    
      
        
          
            
              E
              ^
            
          
        
        =
        arg
        ⁡
        
          max
          
            E
          
        
        P
        (
        E
        
          |
        
        F
        )
      
    
    {\displaystyle {\hat {E}}=\arg \max _{E}P(E|F)}
  
. However, by Bayes law, we have the equivalent equation:
  
    
      
        
          
            
              E
              ^
            
          
        
        =
        
          
            argmax
            
              E
              ∈
              
                 English 
              
            
          
        
        
          
            
              
                P
                (
                F
                ∣
                E
                )
              
              ⏞
            
          
          
            translation model 
          
        
        
          
            
              
                P
                (
                E
                )
              
              ⏞
            
          
          
            language model
          
        
      
    
    {\displaystyle {\hat {E}}={\underset {E\in {\text{ English }}}{\operatorname {argmax} }}\overbrace {P(F\mid E)} ^{\text{translation model  }}\overbrace {P(E)} ^{\text{language model}}}
  
The benefit of the noisy-channel model is in terms of data: If collecting a parallel corpus is costly, then we would have only a small parallel corpus, so we can only train a moderately good English-to-foreign translation model, and a moderately good foreign-to-English translation model. However, we can collect a large corpus in the foreign language only, and a large corpus in the English language only, to train two good language models. Combining these four models, we immediately get a good English-to-foreign translator and a good foreign-to-English translator.
The cost of noisy-channel model is that using Bayesian inference is more costly than using a translation model directly. Instead of reading out the most likely translation by 
  
    
      
        arg
        ⁡
        
          max
          
            E
          
        
        P
        (
        E
        
          |
        
        F
        )
      
    
    {\displaystyle \arg \max _{E}P(E|F)}
  
, it would have to read out predictions by both the translation model and the language model, multiply them, and search for the highest number.


== In speech recognition ==
Speech recognition can be thought of as translating from a sound-language to a text-language. Consequently, we have
  
    
      
        
          
            
              T
              ^
            
          
        
        =
        
          
            argmax
            
              T
              ∈
              
                 Text 
              
            
          
        
        
          
            
              
                P
                (
                S
                ∣
                T
                )
              
              ⏞
            
          
          
            speech model 
          
        
        
          
            
              
                P
                (
                T
                )
              
              ⏞
            
          
          
            language model
          
        
      
    
    {\displaystyle {\hat {T}}={\underset {T\in {\text{ Text }}}{\operatorname {argmax} }}\overbrace {P(S\mid T)} ^{\text{speech model  }}\overbrace {P(T)} ^{\text{language model}}}
  
where 
  
    
      
        P
        (
        S
        
          |
        
        T
        )
      
    
    {\displaystyle P(S|T)}
  
 is the probability that a speech sound S is produced if the speaker is intending to say text T. Intuitively, this equation states that the most likely text is a text that's both a likely text in the language, and produces the speech sound with high probability.
The utility of the noisy-channel model is not in capacity. Theoretically, any noisy-channel model can be replicated by a direct 
  
    
      
        P
        (
        T
        
          |
        
        S
        )
      
    
    {\displaystyle P(T|S)}
  
 model. However, the noisy-channel model factors the model into two parts which are appropriate for the situation, and consequently it is generally more well-behaved.
When a human speaks, it does not produce the sound directly, but first produces the text it wants to speak in the language centers of the brain, then the text is translated into sound by the motor cortex, vocal cords, and other parts of the body. The noisy-channel model matches this model of the human, and so it is appropriate. This is justified in the practical success of noisy-channel model in speech recognition.


=== Example ===
Consider the sound-language sentence (written in IPA for English) S = aɪ wʊd laɪk wʌn tuː. There are three possible texts 
  
    
      
        
          T
          
            1
          
        
        ,
        
          T
          
            2
          
        
        ,
        
          T
          
            3
          
        
      
    
    {\displaystyle T_{1},T_{2},T_{3}}
  
:

  
    
      
        
          T
          
            1
          
        
        =
      
    
    {\displaystyle T_{1}=}
  
 I would like one to.

  
    
      
        
          T
          
            2
          
        
        =
      
    
    {\displaystyle T_{2}=}
  
 I would like one too.

  
    
      
        
          T
          
            3
          
        
        =
      
    
    {\displaystyle T_{3}=}
  
 I would like one two.
that are equally likely, in the sense that 
  
    
      
        P
        (
        S
        
          |
        
        
          T
          
            1
          
        
        )
        =
        P
        (
        S
        
          |
        
        
          T
          
            2
          
        
        )
        =
        P
        (
        S
        
          |
        
        
          T
          
            3
          
        
        )
      
    
    {\displaystyle P(S|T_{1})=P(S|T_{2})=P(S|T_{3})}
  
. With a good English language model, we would have 
  
    
      
        P
        (
        
          T
          
            2
          
        
        )
        >
        P
        (
        
          T
          
            1
          
        
        )
        >
        P
        (
        
          T
          
            3
          
        
        )
      
    
    {\displaystyle P(T_{2})>P(T_{1})>P(T_{3})}
  
, since the second sentence is grammatical, the first is not quite, but close to a grammatical one (such as "I would like one to [go]."), while the third one is far from grammatical.
Consequently, the noisy-channel model would output 
  
    
      
        
          T
          
            2
          
        
      
    
    {\displaystyle T_{2}}
  
 as the best transcription.


== See also ==
Coding theory


== References ==