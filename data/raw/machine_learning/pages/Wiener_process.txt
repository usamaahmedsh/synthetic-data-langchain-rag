In mathematics, the Wiener process (or Brownian motion, due to its historical connection with the physical process of the same name) is a real-valued continuous-time stochastic process named after Norbert Wiener. It is one of the best known Lévy processes (càdlàg stochastic processes with stationary independent increments). It occurs frequently in pure and applied mathematics, economics, quantitative finance, evolutionary biology, and physics.
The Wiener process plays an important role in both pure and applied mathematics. In pure mathematics, the Wiener process gave rise to the study of continuous time martingales. It is a key process in terms of which more complicated stochastic processes can be described. As such, it plays a vital role in stochastic calculus, diffusion processes and even potential theory. It is the driving process of Schramm–Loewner evolution. In applied mathematics, the Wiener process is used to represent the integral of a white noise Gaussian process, and so is useful as a model of noise in electronics engineering (see Brownian noise), instrument errors in filtering theory and disturbances in control theory.
The Wiener process has applications throughout the mathematical sciences. In physics it is used to study Brownian motion and other types of diffusion via the Fokker–Planck and Langevin equations. It also forms the basis for the rigorous path integral formulation of quantum mechanics (by the Feynman–Kac formula, a solution to the Schrödinger equation can be represented in terms of the Wiener process) and the study of eternal inflation in physical cosmology. It is also prominent in the mathematical theory of finance, in particular the Black–Scholes option pricing model.


== Characterisations of the Wiener process ==
The Wiener process 
  
    
      
        
          W
          
            t
          
        
      
    
    {\displaystyle W_{t}}
  
 is characterised by the following properties:

  
    
      
        
          W
          
            0
          
        
        =
        0
      
    
    {\displaystyle W_{0}=0}
  
 almost surely.

  
    
      
        W
      
    
    {\displaystyle W}
  
 has independent increments: for every 
  
    
      
        t
        >
        0
      
    
    {\displaystyle t>0}
  
, the future increments 
  
    
      
        
          W
          
            t
            +
            u
          
        
        −
        
          W
          
            t
          
        
        ,
        
        u
        ≥
        0
        ,
      
    
    {\displaystyle W_{t+u}-W_{t},\,u\geq 0,}
  
 are independent of the past values 
  
    
      
        
          W
          
            s
          
        
      
    
    {\displaystyle W_{s}}
  
, 
  
    
      
        s
        <
        t
        .
      
    
    {\displaystyle s<t.}
  

  
    
      
        W
      
    
    {\displaystyle W}
  
 has Gaussian increments: for all 
  
    
      
        t
        ≥
        0
        ,
        u
        ≥
        0
      
    
    {\displaystyle t\geq 0,u\geq 0}
  
, 
  
    
      
        
          W
          
            t
            +
            u
          
        
        −
        
          W
          
            t
          
        
        ∼
        
          
            N
          
        
        (
        0
        ,
        u
        )
        .
      
    
    {\displaystyle W_{t+u}-W_{t}\sim {\mathcal {N}}(0,u).}
  

  
    
      
        W
      
    
    {\displaystyle W}
  
 has almost surely continuous paths: 
  
    
      
        
          W
          
            t
          
        
      
    
    {\displaystyle W_{t}}
  
 is almost surely continuous in 
  
    
      
        t
        .
      
    
    {\displaystyle t.}
  

That the process has independent increments means that if 0 ≤ s1 < t1 ≤ s2 < t2 then Wt1 − Ws1 and Wt2 − Ws2 are independent random variables, and the similar condition holds for n increments. 
Condition 2 can equivalently be formulated: For every 
  
    
      
        t
        >
        0
      
    
    {\displaystyle t>0}
  
 and 
  
    
      
        u
        ≥
        0
      
    
    {\displaystyle u\geq 0}
  
, the increment 
  
    
      
        
          W
          
            t
            +
            u
          
        
        −
        
          W
          
            t
          
        
      
    
    {\displaystyle W_{t+u}-W_{t}}
  
 is independent of the sigma-algebra 
  
    
      
        
          
            
              F
            
          
          
            t
          
          
            B
          
        
        =
        σ
        (
        
          W
          
            s
          
        
        :
        0
        ≤
        s
        ≤
        t
        )
        .
      
    
    {\displaystyle {\mathcal {F}}_{t}^{B}=\sigma (W_{s}:0\leq s\leq t).}
  
.

An alternative characterisation of the Wiener process is the so-called Lévy characterisation that says that the Wiener process is an almost surely continuous martingale with W0 = 0 and quadratic variation [Wt, Wt] = t (which means that Wt2 − t is also a martingale).
A third characterisation is that the Wiener process has a spectral representation as a sine series whose coefficients are independent N(0, 1) random variables.  This representation can be obtained using the Karhunen–Loève theorem.
Another characterisation of a Wiener process is the definite integral (from time zero to time t) of a zero mean, unit variance, delta correlated ("white") Gaussian process.
The Wiener process can be constructed as the scaling limit of a random walk, or other discrete-time stochastic processes with stationary independent increments. This is known as Donsker's theorem. Like the random walk, the Wiener process is recurrent in one or two dimensions (meaning that it returns almost surely to any fixed neighborhood of the origin infinitely often) whereas it is not recurrent in dimensions three and higher (where a multidimensional Wiener process is a process such that its coordinates are independent Wiener processes). Unlike the random walk, it is scale invariant, meaning that

  
    
      
        
          α
          
            −
            1
          
        
        
          W
          
            
              α
              
                2
              
            
            t
          
        
      
    
    {\displaystyle \alpha ^{-1}W_{\alpha ^{2}t}}
  

is a Wiener process for any nonzero constant α. The Wiener measure is the probability law on the space of continuous functions g, with g(0) = 0, induced by the Wiener process. An integral based on Wiener measure may be called a Wiener integral.


== Wiener process as a limit of random walk ==
Let 
  
    
      
        
          ξ
          
            1
          
        
        ,
        
          ξ
          
            2
          
        
        ,
        …
      
    
    {\displaystyle \xi _{1},\xi _{2},\ldots }
  
 be i.i.d. random variables with mean 0 and variance 1. For each n, define a continuous time stochastic process

  
    
      
        
          W
          
            n
          
        
        (
        t
        )
        =
        
          
            1
            
              n
            
          
        
        
          ∑
          
            1
            ≤
            k
            ≤
            ⌊
            n
            t
            ⌋
          
        
        
          ξ
          
            k
          
        
        ,
        
        t
        ∈
        [
        0
        ,
        1
        ]
        .
      
    
    {\displaystyle W_{n}(t)={\frac {1}{\sqrt {n}}}\sum \limits _{1\leq k\leq \lfloor nt\rfloor }\xi _{k},\qquad t\in [0,1].}
  

This is a random step function. Increments of 
  
    
      
        
          W
          
            n
          
        
      
    
    {\displaystyle W_{n}}
  
 are independent because the 
  
    
      
        
          ξ
          
            k
          
        
      
    
    {\displaystyle \xi _{k}}
  
 are independent. For large n, 
  
    
      
        
          W
          
            n
          
        
        (
        t
        )
        −
        
          W
          
            n
          
        
        (
        s
        )
      
    
    {\displaystyle W_{n}(t)-W_{n}(s)}
  
 is close to 
  
    
      
        N
        (
        0
        ,
        t
        −
        s
        )
      
    
    {\displaystyle N(0,t-s)}
  
 by the central limit theorem. Donsker's theorem asserts that as 
  
    
      
        n
        →
        ∞
      
    
    {\displaystyle n\to \infty }
  
, 
  
    
      
        
          W
          
            n
          
        
      
    
    {\displaystyle W_{n}}
  
 approaches a Wiener process, which explains the ubiquity of Brownian motion.


== Properties of a one-dimensional Wiener process ==


=== Basic properties ===
The unconditional probability density function follows a normal distribution with mean = 0 and variance = t, at a fixed time t:

  
    
      
        
          f
          
            
              W
              
                t
              
            
          
        
        (
        x
        )
        =
        
          
            1
            
              2
              π
              t
            
          
        
        
          e
          
            −
            
              x
              
                2
              
            
            
              /
            
            (
            2
            t
            )
          
        
        .
      
    
    {\displaystyle f_{W_{t}}(x)={\frac {1}{\sqrt {2\pi t}}}e^{-x^{2}/(2t)}.}
  

The expectation is zero:

  
    
      
        E
        ⁡
        [
        
          W
          
            t
          
        
        ]
        =
        0.
      
    
    {\displaystyle \operatorname {E} [W_{t}]=0.}
  

The variance, using the computational formula, is t:

  
    
      
        Var
        ⁡
        (
        
          W
          
            t
          
        
        )
        =
        t
        .
      
    
    {\displaystyle \operatorname {Var} (W_{t})=t.}
  

These results follow immediately from the definition that increments have a normal distribution, centered at zero. Thus

  
    
      
        
          W
          
            t
          
        
        =
        
          W
          
            t
          
        
        −
        
          W
          
            0
          
        
        ∼
        N
        (
        0
        ,
        t
        )
        .
      
    
    {\displaystyle W_{t}=W_{t}-W_{0}\sim N(0,t).}
  

A useful decomposition for proving martingale properties also called Brownian increment decomposition is

  
    
      
        
          W
          
            t
          
        
        =
        
          W
          
            s
          
        
        +
        (
        
          W
          
            t
          
        
        −
        
          W
          
            s
          
        
        )
        ,
        
        s
        ≤
        t
      
    
    {\displaystyle W_{t}=W_{s}+(W_{t}-W_{s}),\;s\leq t}
  


=== Covariance and correlation ===
The covariance and correlation (where 
  
    
      
        s
        ≤
        t
      
    
    {\displaystyle s\leq t}
  
):

  
    
      
        
          
            
              
                cov
                ⁡
                (
                
                  W
                  
                    s
                  
                
                ,
                
                  W
                  
                    t
                  
                
                )
              
              
                
                =
                s
                ,
              
            
            
              
                corr
                ⁡
                (
                
                  W
                  
                    s
                  
                
                ,
                
                  W
                  
                    t
                  
                
                )
              
              
                
                =
                
                  
                    
                      cov
                      ⁡
                      (
                      
                        W
                        
                          s
                        
                      
                      ,
                      
                        W
                        
                          t
                        
                      
                      )
                    
                    
                      
                        σ
                        
                          
                            W
                            
                              s
                            
                          
                        
                      
                      
                        σ
                        
                          
                            W
                            
                              t
                            
                          
                        
                      
                    
                  
                
                =
                
                  
                    s
                    
                      s
                      t
                    
                  
                
                =
                
                  
                    
                      s
                      t
                    
                  
                
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\operatorname {cov} (W_{s},W_{t})&=s,\\\operatorname {corr} (W_{s},W_{t})&={\frac {\operatorname {cov} (W_{s},W_{t})}{\sigma _{W_{s}}\sigma _{W_{t}}}}={\frac {s}{\sqrt {st}}}={\sqrt {\frac {s}{t}}}.\end{aligned}}}
  

These results follow from the definition that non-overlapping increments are independent, of which only the property that they are uncorrelated is used. Suppose that 
  
    
      
        
          t
          
            1
          
        
        ≤
        
          t
          
            2
          
        
      
    
    {\displaystyle t_{1}\leq t_{2}}
  
.

  
    
      
        cov
        ⁡
        (
        
          W
          
            
              t
              
                1
              
            
          
        
        ,
        
          W
          
            
              t
              
                2
              
            
          
        
        )
        =
        E
        ⁡
        
          [
          
            (
            
              W
              
                
                  t
                  
                    1
                  
                
              
            
            −
            E
            ⁡
            [
            
              W
              
                
                  t
                  
                    1
                  
                
              
            
            ]
            )
            ⋅
            (
            
              W
              
                
                  t
                  
                    2
                  
                
              
            
            −
            E
            ⁡
            [
            
              W
              
                
                  t
                  
                    2
                  
                
              
            
            ]
            )
          
          ]
        
        =
        E
        ⁡
        
          [
          
            
              W
              
                
                  t
                  
                    1
                  
                
              
            
            ⋅
            
              W
              
                
                  t
                  
                    2
                  
                
              
            
          
          ]
        
        .
      
    
    {\displaystyle \operatorname {cov} (W_{t_{1}},W_{t_{2}})=\operatorname {E} \left[(W_{t_{1}}-\operatorname {E} [W_{t_{1}}])\cdot (W_{t_{2}}-\operatorname {E} [W_{t_{2}}])\right]=\operatorname {E} \left[W_{t_{1}}\cdot W_{t_{2}}\right].}
  

Substituting

  
    
      
        
          W
          
            
              t
              
                2
              
            
          
        
        =
        (
        
          W
          
            
              t
              
                2
              
            
          
        
        −
        
          W
          
            
              t
              
                1
              
            
          
        
        )
        +
        
          W
          
            
              t
              
                1
              
            
          
        
      
    
    {\displaystyle W_{t_{2}}=(W_{t_{2}}-W_{t_{1}})+W_{t_{1}}}
  

we arrive at:

  
    
      
        
          
            
              
                E
                ⁡
                [
                
                  W
                  
                    
                      t
                      
                        1
                      
                    
                  
                
                ⋅
                
                  W
                  
                    
                      t
                      
                        2
                      
                    
                  
                
                ]
              
              
                
                =
                E
                ⁡
                
                  [
                  
                    
                      W
                      
                        
                          t
                          
                            1
                          
                        
                      
                    
                    ⋅
                    (
                    (
                    
                      W
                      
                        
                          t
                          
                            2
                          
                        
                      
                    
                    −
                    
                      W
                      
                        
                          t
                          
                            1
                          
                        
                      
                    
                    )
                    +
                    
                      W
                      
                        
                          t
                          
                            1
                          
                        
                      
                    
                    )
                  
                  ]
                
              
            
            
              
              
                
                =
                E
                ⁡
                
                  [
                  
                    
                      W
                      
                        
                          t
                          
                            1
                          
                        
                      
                    
                    ⋅
                    (
                    
                      W
                      
                        
                          t
                          
                            2
                          
                        
                      
                    
                    −
                    
                      W
                      
                        
                          t
                          
                            1
                          
                        
                      
                    
                    )
                  
                  ]
                
                +
                E
                ⁡
                
                  [
                  
                    W
                    
                      
                        t
                        
                          1
                        
                      
                    
                    
                      2
                    
                  
                  ]
                
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\operatorname {E} [W_{t_{1}}\cdot W_{t_{2}}]&=\operatorname {E} \left[W_{t_{1}}\cdot ((W_{t_{2}}-W_{t_{1}})+W_{t_{1}})\right]\\&=\operatorname {E} \left[W_{t_{1}}\cdot (W_{t_{2}}-W_{t_{1}})\right]+\operatorname {E} \left[W_{t_{1}}^{2}\right].\end{aligned}}}
  

Since 
  
    
      
        
          W
          
            
              t
              
                1
              
            
          
        
        =
        
          W
          
            
              t
              
                1
              
            
          
        
        −
        
          W
          
            
              t
              
                0
              
            
          
        
      
    
    {\displaystyle W_{t_{1}}=W_{t_{1}}-W_{t_{0}}}
  
 and 
  
    
      
        
          W
          
            
              t
              
                2
              
            
          
        
        −
        
          W
          
            
              t
              
                1
              
            
          
        
      
    
    {\displaystyle W_{t_{2}}-W_{t_{1}}}
  
 are independent,

  
    
      
        E
        ⁡
        
          [
          
            
              W
              
                
                  t
                  
                    1
                  
                
              
            
            ⋅
            (
            
              W
              
                
                  t
                  
                    2
                  
                
              
            
            −
            
              W
              
                
                  t
                  
                    1
                  
                
              
            
            )
          
          ]
        
        =
        E
        ⁡
        [
        
          W
          
            
              t
              
                1
              
            
          
        
        ]
        ⋅
        E
        ⁡
        [
        
          W
          
            
              t
              
                2
              
            
          
        
        −
        
          W
          
            
              t
              
                1
              
            
          
        
        ]
        =
        0.
      
    
    {\displaystyle \operatorname {E} \left[W_{t_{1}}\cdot (W_{t_{2}}-W_{t_{1}})\right]=\operatorname {E} [W_{t_{1}}]\cdot \operatorname {E} [W_{t_{2}}-W_{t_{1}}]=0.}
  

Thus

  
    
      
        cov
        ⁡
        (
        
          W
          
            
              t
              
                1
              
            
          
        
        ,
        
          W
          
            
              t
              
                2
              
            
          
        
        )
        =
        E
        ⁡
        
          [
          
            W
            
              
                t
                
                  1
                
              
            
            
              2
            
          
          ]
        
        =
        
          t
          
            1
          
        
        .
      
    
    {\displaystyle \operatorname {cov} (W_{t_{1}},W_{t_{2}})=\operatorname {E} \left[W_{t_{1}}^{2}\right]=t_{1}.}
  

A corollary useful for simulation is that we can write, for t1 < t2:

  
    
      
        
          W
          
            
              t
              
                2
              
            
          
        
        =
        
          W
          
            
              t
              
                1
              
            
          
        
        +
        
          
            
              t
              
                2
              
            
            −
            
              t
              
                1
              
            
          
        
        ⋅
        Z
      
    
    {\displaystyle W_{t_{2}}=W_{t_{1}}+{\sqrt {t_{2}-t_{1}}}\cdot Z}
  

where Z is an independent standard normal variable.


=== Wiener representation ===
Wiener (1923) also gave a representation of a Brownian path in terms of a random Fourier series. If 
  
    
      
        
          ξ
          
            n
          
        
      
    
    {\displaystyle \xi _{n}}
  
 are independent Gaussian variables with mean zero and variance one, then

  
    
      
        
          W
          
            t
          
        
        =
        
          ξ
          
            0
          
        
        t
        +
        
          
            2
          
        
        
          ∑
          
            n
            =
            1
          
          
            ∞
          
        
        
          ξ
          
            n
          
        
        
          
            
              sin
              ⁡
              π
              n
              t
            
            
              π
              n
            
          
        
      
    
    {\displaystyle W_{t}=\xi _{0}t+{\sqrt {2}}\sum _{n=1}^{\infty }\xi _{n}{\frac {\sin \pi nt}{\pi n}}}
  

and

  
    
      
        
          W
          
            t
          
        
        =
        
          
            2
          
        
        
          ∑
          
            n
            =
            1
          
          
            ∞
          
        
        
          ξ
          
            n
          
        
        
          
            
              sin
              ⁡
              
                (
                
                  
                    (
                    
                      n
                      −
                      
                        
                          1
                          2
                        
                      
                    
                    )
                  
                  π
                  t
                
                )
              
            
            
              
                (
                
                  n
                  −
                  
                    
                      1
                      2
                    
                  
                
                )
              
              π
            
          
        
      
    
    {\displaystyle W_{t}={\sqrt {2}}\sum _{n=1}^{\infty }\xi _{n}{\frac {\sin \left(\left(n-{\frac {1}{2}}\right)\pi t\right)}{\left(n-{\frac {1}{2}}\right)\pi }}}
  

represent a Brownian motion on 
  
    
      
        [
        0
        ,
        1
        ]
      
    
    {\displaystyle [0,1]}
  
. The scaled process

  
    
      
        
          
            c
          
        
        
        W
        
          (
          
            
              t
              c
            
          
          )
        
      
    
    {\displaystyle {\sqrt {c}}\,W\left({\frac {t}{c}}\right)}
  

is a Brownian motion on 
  
    
      
        [
        0
        ,
        c
        ]
      
    
    {\displaystyle [0,c]}
  
 (cf. Karhunen–Loève theorem).


=== Running maximum ===
The joint distribution of the running maximum

  
    
      
        
          M
          
            t
          
        
        =
        
          max
          
            0
            ≤
            s
            ≤
            t
          
        
        
          W
          
            s
          
        
      
    
    {\displaystyle M_{t}=\max _{0\leq s\leq t}W_{s}}
  

and Wt is

  
    
      
        
          f
          
            
              M
              
                t
              
            
            ,
            
              W
              
                t
              
            
          
        
        (
        m
        ,
        w
        )
        =
        
          
            
              2
              (
              2
              m
              −
              w
              )
            
            
              t
              
                
                  2
                  π
                  t
                
              
            
          
        
        
          e
          
            −
            
              
                
                  (
                  2
                  m
                  −
                  w
                  
                    )
                    
                      2
                    
                  
                
                
                  2
                  t
                
              
            
          
        
        ,
        
        m
        ≥
        0
        ,
        w
        ≤
        m
        .
      
    
    {\displaystyle f_{M_{t},W_{t}}(m,w)={\frac {2(2m-w)}{t{\sqrt {2\pi t}}}}e^{-{\frac {(2m-w)^{2}}{2t}}},\qquad m\geq 0,w\leq m.}
  

To get the unconditional distribution of 
  
    
      
        
          f
          
            
              M
              
                t
              
            
          
        
      
    
    {\displaystyle f_{M_{t}}}
  
, integrate over −∞ < w ≤ m:

  
    
      
        
          
            
              
                
                  f
                  
                    
                      M
                      
                        t
                      
                    
                  
                
                (
                m
                )
              
              
                
                =
                
                  ∫
                  
                    −
                    ∞
                  
                  
                    m
                  
                
                
                  f
                  
                    
                      M
                      
                        t
                      
                    
                    ,
                    
                      W
                      
                        t
                      
                    
                  
                
                (
                m
                ,
                w
                )
                
                d
                w
                =
                
                  ∫
                  
                    −
                    ∞
                  
                  
                    m
                  
                
                
                  
                    
                      2
                      (
                      2
                      m
                      −
                      w
                      )
                    
                    
                      t
                      
                        
                          2
                          π
                          t
                        
                      
                    
                  
                
                
                  e
                  
                    −
                    
                      
                        
                          (
                          2
                          m
                          −
                          w
                          
                            )
                            
                              2
                            
                          
                        
                        
                          2
                          t
                        
                      
                    
                  
                
                
                d
                w
              
            
            
              
              
                
                =
                
                  
                    
                      2
                      
                        π
                        t
                      
                    
                  
                
                
                  e
                  
                    −
                    
                      
                        
                          m
                          
                            2
                          
                        
                        
                          2
                          t
                        
                      
                    
                  
                
                ,
                
                m
                ≥
                0
                ,
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}f_{M_{t}}(m)&=\int _{-\infty }^{m}f_{M_{t},W_{t}}(m,w)\,dw=\int _{-\infty }^{m}{\frac {2(2m-w)}{t{\sqrt {2\pi t}}}}e^{-{\frac {(2m-w)^{2}}{2t}}}\,dw\\[5pt]&={\sqrt {\frac {2}{\pi t}}}e^{-{\frac {m^{2}}{2t}}},\qquad m\geq 0,\end{aligned}}}
  

the probability density function of a Half-normal distribution. The expectation is

  
    
      
        E
        ⁡
        [
        
          M
          
            t
          
        
        ]
        =
        
          ∫
          
            0
          
          
            ∞
          
        
        m
        
          f
          
            
              M
              
                t
              
            
          
        
        (
        m
        )
        
        d
        m
        =
        
          ∫
          
            0
          
          
            ∞
          
        
        m
        
          
            
              2
              
                π
                t
              
            
          
        
        
          e
          
            −
            
              
                
                  m
                  
                    2
                  
                
                
                  2
                  t
                
              
            
          
        
        
        d
        m
        =
        
          
            
              
                2
                t
              
              π
            
          
        
      
    
    {\displaystyle \operatorname {E} [M_{t}]=\int _{0}^{\infty }mf_{M_{t}}(m)\,dm=\int _{0}^{\infty }m{\sqrt {\frac {2}{\pi t}}}e^{-{\frac {m^{2}}{2t}}}\,dm={\sqrt {\frac {2t}{\pi }}}}
  

If at time 
  
    
      
        t
      
    
    {\displaystyle t}
  
 the Wiener process has a known value 
  
    
      
        
          W
          
            t
          
        
      
    
    {\displaystyle W_{t}}
  
, it is possible to calculate the conditional probability distribution of the maximum in interval 
  
    
      
        [
        0
        ,
        t
        ]
      
    
    {\displaystyle [0,t]}
  
 (cf. Probability distribution of extreme points of a Wiener stochastic process). The  cumulative probability distribution function of the maximum value, conditioned by the known value 
  
    
      
        
          W
          
            t
          
        
      
    
    {\displaystyle W_{t}}
  
, is:

  
    
      
        
        
          F
          
            
              M
              
                
                  W
                  
                    t
                  
                
              
            
          
        
        (
        m
        )
        =
        Pr
        
          (
          
            
              M
              
                
                  W
                  
                    t
                  
                
              
            
            =
            
              max
              
                0
                ≤
                s
                ≤
                t
              
            
            W
            (
            s
            )
            ≤
            m
            ∣
            W
            (
            t
            )
            =
            
              W
              
                t
              
            
          
          )
        
        =
         
        1
        −
         
        
          e
          
            −
            2
            
              
                
                  m
                  (
                  m
                  −
                  
                    W
                    
                      t
                    
                  
                  )
                
                t
              
            
          
        
         
        
        ,
        
         
         
        m
        >
        max
        (
        0
        ,
        
          W
          
            t
          
        
        )
      
    
    {\displaystyle \,F_{M_{W_{t}}}(m)=\Pr \left(M_{W_{t}}=\max _{0\leq s\leq t}W(s)\leq m\mid W(t)=W_{t}\right)=\ 1-\ e^{-2{\frac {m(m-W_{t})}{t}}}\ \,,\,\ \ m>\max(0,W_{t})}
  


=== Self-similarity ===


==== Brownian scaling ====
For every c > 0 the process 
  
    
      
        
          V
          
            t
          
        
        =
        (
        1
        
          /
        
        
          
            c
          
        
        )
        
          W
          
            c
            t
          
        
      
    
    {\displaystyle V_{t}=(1/{\sqrt {c}})W_{ct}}
  
 is another Wiener process.


==== Time reversal ====
The process 
  
    
      
        
          V
          
            t
          
        
        =
        
          W
          
            1
            −
            t
          
        
        −
        
          W
          
            1
          
        
      
    
    {\displaystyle V_{t}=W_{1-t}-W_{1}}
  
 for 0 ≤ t ≤ 1 is distributed like Wt for 0 ≤ t ≤ 1.


==== Time inversion ====
The process 
  
    
      
        
          V
          
            t
          
        
        =
        t
        
          W
          
            1
            
              /
            
            t
          
        
      
    
    {\displaystyle V_{t}=tW_{1/t}}
  
 is another Wiener process.


==== Projective invariance ====
Consider a Wiener process 
  
    
      
        W
        (
        t
        )
      
    
    {\displaystyle W(t)}
  
, 
  
    
      
        t
        ∈
        
          R
        
      
    
    {\displaystyle t\in \mathbb {R} }
  
, conditioned so that 
  
    
      
        
          lim
          
            t
            →
            ±
            ∞
          
        
        t
        W
        (
        t
        )
        =
        0
      
    
    {\displaystyle \lim _{t\to \pm \infty }tW(t)=0}
  
 (which holds almost surely) and as usual 
  
    
      
        W
        (
        0
        )
        =
        0
      
    
    {\displaystyle W(0)=0}
  
.  Then the following are all Wiener processes (Takenaka 1988):

  
    
      
        
          
            
              
                
                  W
                  
                    1
                    ,
                    s
                  
                
                (
                t
                )
              
              
                =
              
              
                W
                (
                t
                +
                s
                )
                −
                W
                (
                s
                )
                ,
                
                s
                ∈
                
                  R
                
              
            
            
              
                
                  W
                  
                    2
                    ,
                    σ
                  
                
                (
                t
                )
              
              
                =
              
              
                
                  σ
                  
                    −
                    1
                    
                      /
                    
                    2
                  
                
                W
                (
                σ
                t
                )
                ,
                
                σ
                >
                0
              
            
            
              
                
                  W
                  
                    3
                  
                
                (
                t
                )
              
              
                =
              
              
                t
                W
                (
                −
                1
                
                  /
                
                t
                )
                .
              
            
          
        
      
    
    {\displaystyle {\begin{array}{rcl}W_{1,s}(t)&=&W(t+s)-W(s),\quad s\in \mathbb {R} \\W_{2,\sigma }(t)&=&\sigma ^{-1/2}W(\sigma t),\quad \sigma >0\\W_{3}(t)&=&tW(-1/t).\end{array}}}
  

Thus the Wiener process is invariant under the projective group PSL(2,R), being invariant under the generators of the group.  The action of an element 
  
    
      
        g
        =
        
          
            [
            
              
                
                  a
                
                
                  b
                
              
              
                
                  c
                
                
                  d
                
              
            
            ]
          
        
      
    
    {\displaystyle g={\begin{bmatrix}a&b\\c&d\end{bmatrix}}}
  
 is

  
    
      
        
          W
          
            g
          
        
        (
        t
        )
        =
        (
        c
        t
        +
        d
        )
        W
        
          (
          
            
              
                a
                t
                +
                b
              
              
                c
                t
                +
                d
              
            
          
          )
        
        −
        c
        t
        W
        
          (
          
            
              a
              c
            
          
          )
        
        −
        d
        W
        
          (
          
            
              b
              d
            
          
          )
        
        ,
      
    
    {\displaystyle W_{g}(t)=(ct+d)W\left({\frac {at+b}{ct+d}}\right)-ctW\left({\frac {a}{c}}\right)-dW\left({\frac {b}{d}}\right),}
  

which defines a group action, in the sense that 
  
    
      
        (
        
          W
          
            g
          
        
        
          )
          
            h
          
        
        =
        
          W
          
            g
            h
          
        
        .
      
    
    {\displaystyle (W_{g})_{h}=W_{gh}.}
  


==== Conformal invariance in two dimensions ====
Let 
  
    
      
        W
        (
        t
        )
      
    
    {\displaystyle W(t)}
  
 be a two-dimensional Wiener process, regarded as a complex-valued process with 
  
    
      
        W
        (
        0
        )
        =
        0
        ∈
        
          C
        
      
    
    {\displaystyle W(0)=0\in \mathbb {C} }
  
.  Let 
  
    
      
        D
        ⊂
        
          C
        
      
    
    {\displaystyle D\subset \mathbb {C} }
  
 be an open set containing 0, and 
  
    
      
        
          τ
          
            D
          
        
      
    
    {\displaystyle \tau _{D}}
  
 be associated Markov time:

  
    
      
        
          τ
          
            D
          
        
        =
        inf
        {
        t
        ≥
        0
        
          |
        
        W
        (
        t
        )
        ∉
        D
        }
        .
      
    
    {\displaystyle \tau _{D}=\inf\{t\geq 0|W(t)\not \in D\}.}
  

If 
  
    
      
        f
        :
        D
        →
        
          C
        
      
    
    {\displaystyle f:D\to \mathbb {C} }
  
 is a holomorphic function which is not constant, such that 
  
    
      
        f
        (
        0
        )
        =
        0
      
    
    {\displaystyle f(0)=0}
  
, then 
  
    
      
        f
        (
        
          W
          
            t
          
        
        )
      
    
    {\displaystyle f(W_{t})}
  
 is a time-changed Wiener process in 
  
    
      
        f
        (
        D
        )
      
    
    {\displaystyle f(D)}
  
 (Lawler 2005).  More precisely, the process 
  
    
      
        Y
        (
        t
        )
      
    
    {\displaystyle Y(t)}
  
 is Wiener in 
  
    
      
        D
      
    
    {\displaystyle D}
  
 with the Markov time 
  
    
      
        S
        (
        t
        )
      
    
    {\displaystyle S(t)}
  
 where

  
    
      
        Y
        (
        t
        )
        =
        f
        (
        W
        (
        σ
        (
        t
        )
        )
        )
      
    
    {\displaystyle Y(t)=f(W(\sigma (t)))}
  

  
    
      
        S
        (
        t
        )
        =
        
          ∫
          
            0
          
          
            t
          
        
        
          |
        
        
          f
          ′
        
        (
        W
        (
        s
        )
        )
        
          
            |
          
          
            2
          
        
        
        d
        s
      
    
    {\displaystyle S(t)=\int _{0}^{t}|f'(W(s))|^{2}\,ds}
  

  
    
      
        σ
        (
        t
        )
        =
        
          S
          
            −
            1
          
        
        (
        t
        )
        :
        
        t
        =
        
          ∫
          
            0
          
          
            σ
            (
            t
            )
          
        
        
          |
        
        
          f
          ′
        
        (
        W
        (
        s
        )
        )
        
          
            |
          
          
            2
          
        
        
        d
        s
        .
      
    
    {\displaystyle \sigma (t)=S^{-1}(t):\quad t=\int _{0}^{\sigma (t)}|f'(W(s))|^{2}\,ds.}
  


=== A class of Brownian martingales ===
If a polynomial p(x, t) satisfies the partial differential equation

  
    
      
        
          (
          
            
              
                ∂
                
                  ∂
                  t
                
              
            
            +
            
              
                1
                2
              
            
            
              
                
                  ∂
                  
                    2
                  
                
                
                  ∂
                  
                    x
                    
                      2
                    
                  
                
              
            
          
          )
        
        p
        (
        x
        ,
        t
        )
        =
        0
      
    
    {\displaystyle \left({\frac {\partial }{\partial t}}+{\frac {1}{2}}{\frac {\partial ^{2}}{\partial x^{2}}}\right)p(x,t)=0}
  

then the stochastic process

  
    
      
        
          M
          
            t
          
        
        =
        p
        (
        
          W
          
            t
          
        
        ,
        t
        )
      
    
    {\displaystyle M_{t}=p(W_{t},t)}
  

is a martingale.
Example: 
  
    
      
        
          W
          
            t
          
          
            2
          
        
        −
        t
      
    
    {\displaystyle W_{t}^{2}-t}
  
 is a martingale, which shows that the quadratic variation of W on [0, t] is equal to t. It follows that the expected time of first exit of W from (−c, c) is equal to c2.
More generally, for every polynomial p(x, t) the following stochastic process is a martingale:

  
    
      
        
          M
          
            t
          
        
        =
        p
        (
        
          W
          
            t
          
        
        ,
        t
        )
        −
        
          ∫
          
            0
          
          
            t
          
        
        a
        (
        
          W
          
            s
          
        
        ,
        s
        )
        
        
          d
        
        s
        ,
      
    
    {\displaystyle M_{t}=p(W_{t},t)-\int _{0}^{t}a(W_{s},s)\,\mathrm {d} s,}
  

where a is the polynomial

  
    
      
        a
        (
        x
        ,
        t
        )
        =
        
          (
          
            
              
                ∂
                
                  ∂
                  t
                
              
            
            +
            
              
                1
                2
              
            
            
              
                
                  ∂
                  
                    2
                  
                
                
                  ∂
                  
                    x
                    
                      2
                    
                  
                
              
            
          
          )
        
        p
        (
        x
        ,
        t
        )
        .
      
    
    {\displaystyle a(x,t)=\left({\frac {\partial }{\partial t}}+{\frac {1}{2}}{\frac {\partial ^{2}}{\partial x^{2}}}\right)p(x,t).}
  

Example: 
  
    
      
        p
        (
        x
        ,
        t
        )
        =
        
          
            (
            
              
                x
                
                  2
                
              
              −
              t
            
            )
          
          
            2
          
        
        ,
      
    
    {\displaystyle p(x,t)=\left(x^{2}-t\right)^{2},}
  
 
  
    
      
        a
        (
        x
        ,
        t
        )
        =
        4
        
          x
          
            2
          
        
        ;
      
    
    {\displaystyle a(x,t)=4x^{2};}
  
 the process

  
    
      
        
          
            (
            
              
                W
                
                  t
                
                
                  2
                
              
              −
              t
            
            )
          
          
            2
          
        
        −
        4
        
          ∫
          
            0
          
          
            t
          
        
        
          W
          
            s
          
          
            2
          
        
        
        
          d
        
        s
      
    
    {\displaystyle \left(W_{t}^{2}-t\right)^{2}-4\int _{0}^{t}W_{s}^{2}\,\mathrm {d} s}
  

is a martingale, which shows that the quadratic variation of the martingale 
  
    
      
        
          W
          
            t
          
          
            2
          
        
        −
        t
      
    
    {\displaystyle W_{t}^{2}-t}
  
 on [0, t] is equal to

  
    
      
        4
        
          ∫
          
            0
          
          
            t
          
        
        
          W
          
            s
          
          
            2
          
        
        
        
          d
        
        s
        .
      
    
    {\displaystyle 4\int _{0}^{t}W_{s}^{2}\,\mathrm {d} s.}
  

About functions p(xa, t) more general than polynomials, see local martingales.


=== Some properties of sample paths ===
The set of all functions w with these properties is of full Wiener measure. That is, a path (sample function) of the Wiener process has all these properties almost surely:


==== Qualitative properties ====
For every ε > 0, the function w takes both (strictly) positive and (strictly) negative values on (0, ε).
The function w is continuous everywhere but differentiable nowhere (like the Weierstrass function).
For any 
  
    
      
        ϵ
        >
        0
      
    
    {\displaystyle \epsilon >0}
  
, 
  
    
      
        w
        (
        t
        )
      
    
    {\displaystyle w(t)}
  
 is almost surely not 
  
    
      
        (
        
          
            
              1
              2
            
          
        
        +
        ϵ
        )
      
    
    {\displaystyle ({\tfrac {1}{2}}+\epsilon )}
  
-Hölder continuous, and almost surely 
  
    
      
        (
        
          
            
              1
              2
            
          
        
        −
        ϵ
        )
      
    
    {\displaystyle ({\tfrac {1}{2}}-\epsilon )}
  
-Hölder continuous.
Points of local maximum of the function w are a dense countable set; the maximum values are pairwise different; each local maximum is sharp in the following sense: if w has a local maximum at t then 
  
    
      
        
          lim
          
            s
            →
            t
          
        
        
          
            
              
                |
              
              w
              (
              s
              )
              −
              w
              (
              t
              )
              
                |
              
            
            
              
                |
              
              s
              −
              t
              
                |
              
            
          
        
        →
        ∞
        .
      
    
    {\displaystyle \lim _{s\to t}{\frac {|w(s)-w(t)|}{|s-t|}}\to \infty .}
  
 The same holds for local minima.
The function w has no points of local increase, that is, no t > 0 satisfies the following for some ε in (0, t): first, w(s) ≤ w(t) for all s in (t − ε, t), and second, w(s) ≥ w(t) for all s in (t, t + ε). (Local increase is a weaker condition than that w is increasing on (t − ε, t + ε).) The same holds for local decrease.
The function w is of unbounded variation on every interval.
The quadratic variation of w over [0,t] is t.
Zeros of the function w are a nowhere dense perfect set of Lebesgue measure 0 and Hausdorff dimension 1/2 (therefore, uncountable).


==== Quantitative properties ====


===== Law of the iterated logarithm =====

  
    
      
        
          lim sup
          
            t
            →
            +
            ∞
          
        
        
          
            
              
                |
              
              w
              (
              t
              )
              
                |
              
            
            
              2
              t
              log
              ⁡
              log
              ⁡
              t
            
          
        
        =
        1
        ,
        
        
          almost surely
        
        .
      
    
    {\displaystyle \limsup _{t\to +\infty }{\frac {|w(t)|}{\sqrt {2t\log \log t}}}=1,\quad {\text{almost surely}}.}
  


===== Modulus of continuity =====
Local modulus of continuity:

  
    
      
        
          lim sup
          
            ε
            →
            0
            +
          
        
        
          
            
              
                |
              
              w
              (
              ε
              )
              
                |
              
            
            
              2
              ε
              log
              ⁡
              log
              ⁡
              (
              1
              
                /
              
              ε
              )
            
          
        
        =
        1
        ,
        
        
          almost surely
        
        .
      
    
    {\displaystyle \limsup _{\varepsilon \to 0+}{\frac {|w(\varepsilon )|}{\sqrt {2\varepsilon \log \log(1/\varepsilon )}}}=1,\qquad {\text{almost surely}}.}
  

Global modulus of continuity (Lévy):

  
    
      
        
          lim sup
          
            ε
            →
            0
            +
          
        
        
          sup
          
            0
            ≤
            s
            <
            t
            ≤
            1
            ,
            t
            −
            s
            ≤
            ε
          
        
        
          
            
              
                |
              
              w
              (
              s
              )
              −
              w
              (
              t
              )
              
                |
              
            
            
              2
              ε
              log
              ⁡
              (
              1
              
                /
              
              ε
              )
            
          
        
        =
        1
        ,
        
        
          almost surely
        
        .
      
    
    {\displaystyle \limsup _{\varepsilon \to 0+}\sup _{0\leq s<t\leq 1,t-s\leq \varepsilon }{\frac {|w(s)-w(t)|}{\sqrt {2\varepsilon \log(1/\varepsilon )}}}=1,\qquad {\text{almost surely}}.}
  


===== Dimension doubling theorem =====
The dimension doubling theorems say that the Hausdorff dimension of a set under a Brownian motion doubles almost surely.


==== Local time ====
The image of the Lebesgue measure on [0, t] under the map w (the pushforward measure) has a density Lt.  Thus,

  
    
      
        
          ∫
          
            0
          
          
            t
          
        
        f
        (
        w
        (
        s
        )
        )
        
        
          d
        
        s
        =
        
          ∫
          
            −
            ∞
          
          
            +
            ∞
          
        
        f
        (
        x
        )
        
          L
          
            t
          
        
        (
        x
        )
        
        
          d
        
        x
      
    
    {\displaystyle \int _{0}^{t}f(w(s))\,\mathrm {d} s=\int _{-\infty }^{+\infty }f(x)L_{t}(x)\,\mathrm {d} x}
  

for a wide class of functions f (namely: all continuous functions; all locally integrable functions; all non-negative measurable functions). The density Lt is (more exactly, can and will be chosen to be) continuous. The number Lt(x) is called the local time at x of w on [0, t]. It is strictly positive for all x of the interval (a, b) where a and b are the least and the greatest value of w on [0, t], respectively. (For x outside this interval the local time evidently vanishes.) Treated as a function of two variables x and t, the local time is still continuous. Treated as a function of t (while x is fixed), the local time is a singular function corresponding to a nonatomic measure on the set of zeros of w.
These continuity properties are fairly non-trivial. Consider that the local time can also be defined (as the density of the pushforward measure) for a smooth function. Then, however,  the density is discontinuous, unless the given function is monotone. In other words, there is a conflict between good behavior of a function and good behavior of its local time. In this sense, the continuity of the local time of the Wiener process is another manifestation of non-smoothness of the trajectory.


=== Information rate ===
The information rate of the Wiener process with respect to the squared error distance, i.e. its quadratic rate-distortion function, is given by 

  
    
      
        R
        (
        D
        )
        =
        
          
            2
            
              
                π
                
                  2
                
              
              D
              ln
              ⁡
              2
            
          
        
        ≈
        0.29
        
          D
          
            −
            1
          
        
        .
      
    
    {\displaystyle R(D)={\frac {2}{\pi ^{2}D\ln 2}}\approx 0.29D^{-1}.}
  

Therefore, it is impossible to encode 
  
    
      
        {
        
          w
          
            t
          
        
        
          }
          
            t
            ∈
            [
            0
            ,
            T
            ]
          
        
      
    
    {\displaystyle \{w_{t}\}_{t\in [0,T]}}
  
 using a binary code of less than 
  
    
      
        T
        R
        (
        D
        )
      
    
    {\displaystyle TR(D)}
  
 bits and recover it with expected mean squared error less than 
  
    
      
        D
      
    
    {\displaystyle D}
  
. On the other hand, for any 
  
    
      
        ε
        >
        0
      
    
    {\displaystyle \varepsilon >0}
  
, there exists 
  
    
      
        T
      
    
    {\displaystyle T}
  
 large enough and a binary code of no more than 
  
    
      
        
          2
          
            T
            R
            (
            D
            )
          
        
      
    
    {\displaystyle 2^{TR(D)}}
  
 distinct elements such that the expected mean squared error in recovering 
  
    
      
        {
        
          w
          
            t
          
        
        
          }
          
            t
            ∈
            [
            0
            ,
            T
            ]
          
        
      
    
    {\displaystyle \{w_{t}\}_{t\in [0,T]}}
  
 from this code is at most 
  
    
      
        D
        −
        ε
      
    
    {\displaystyle D-\varepsilon }
  
.
In many cases, it is impossible to encode the Wiener process without sampling it first. When the Wiener process is sampled at intervals 
  
    
      
        
          T
          
            s
          
        
      
    
    {\displaystyle T_{s}}
  
 before applying a binary code to represent these samples, the optimal trade-off between code rate 
  
    
      
        R
        (
        
          T
          
            s
          
        
        ,
        D
        )
      
    
    {\displaystyle R(T_{s},D)}
  
 and expected mean square error 
  
    
      
        D
      
    
    {\displaystyle D}
  
 (in estimating the continuous-time Wiener process) follows the parametric representation 

  
    
      
        R
        (
        
          T
          
            s
          
        
        ,
        
          D
          
            θ
          
        
        )
        =
        
          
            
              T
              
                s
              
            
            2
          
        
        
          ∫
          
            0
          
          
            1
          
        
        
          log
          
            2
          
          
            +
          
        
        ⁡
        
          [
          
            
              
                S
                (
                φ
                )
                −
                
                  
                    1
                    6
                  
                
              
              θ
            
          
          ]
        
        d
        φ
        ,
      
    
    {\displaystyle R(T_{s},D_{\theta })={\frac {T_{s}}{2}}\int _{0}^{1}\log _{2}^{+}\left[{\frac {S(\varphi )-{\frac {1}{6}}}{\theta }}\right]d\varphi ,}
  

  
    
      
        
          D
          
            θ
          
        
        =
        
          
            
              T
              
                s
              
            
            6
          
        
        +
        
          T
          
            s
          
        
        
          ∫
          
            0
          
          
            1
          
        
        min
        
          {
          
            S
            (
            φ
            )
            −
            
              
                1
                6
              
            
            ,
            θ
          
          }
        
        d
        φ
        ,
      
    
    {\displaystyle D_{\theta }={\frac {T_{s}}{6}}+T_{s}\int _{0}^{1}\min \left\{S(\varphi )-{\frac {1}{6}},\theta \right\}d\varphi ,}
  

where 
  
    
      
        S
        (
        φ
        )
        =
        (
        2
        sin
        ⁡
        (
        π
        φ
        
          /
        
        2
        )
        
          )
          
            −
            2
          
        
      
    
    {\displaystyle S(\varphi )=(2\sin(\pi \varphi /2))^{-2}}
  
 and 
  
    
      
        
          log
          
            +
          
        
        ⁡
        [
        x
        ]
        =
        max
        {
        0
        ,
        log
        ⁡
        (
        x
        )
        }
      
    
    {\displaystyle \log ^{+}[x]=\max\{0,\log(x)\}}
  
. In particular, 
  
    
      
        
          T
          
            s
          
        
        
          /
        
        6
      
    
    {\displaystyle T_{s}/6}
  
 is the mean squared error associated only with the sampling operation (without encoding).


== Related processes ==

The stochastic process defined by

  
    
      
        
          X
          
            t
          
        
        =
        μ
        t
        +
        σ
        
          W
          
            t
          
        
      
    
    {\displaystyle X_{t}=\mu t+\sigma W_{t}}
  

is called a Wiener process with drift μ and infinitesimal variance σ2. These processes exhaust continuous Lévy processes, which means that they are the only continuous Lévy processes,
as a consequence of the Lévy–Khintchine representation. 
Two random processes on the time interval [0, 1] appear, roughly speaking, when conditioning the Wiener process to vanish on both ends of [0,1]. With no further conditioning, the process takes both positive and negative values on [0, 1] and is called Brownian bridge. Conditioned also to stay positive on (0, 1), the process is called Brownian excursion. In both cases a rigorous treatment involves a limiting procedure, since the formula P(A|B) = P(A ∩ B)/P(B)  does not apply when P(B) = 0.
A geometric Brownian motion can be written

  
    
      
        
          e
          
            μ
            t
            −
            
              
                
                  
                    σ
                    
                      2
                    
                  
                  t
                
                2
              
            
            +
            σ
            
              W
              
                t
              
            
          
        
        .
      
    
    {\displaystyle e^{\mu t-{\frac {\sigma ^{2}t}{2}}+\sigma W_{t}}.}
  

It is a stochastic process which is used to model processes that can never take on negative values, such as the value of stocks.
The stochastic process

  
    
      
        
          X
          
            t
          
        
        =
        
          e
          
            −
            t
          
        
        
          W
          
            
              e
              
                2
                t
              
            
          
        
      
    
    {\displaystyle X_{t}=e^{-t}W_{e^{2t}}}
  

is distributed like the Ornstein–Uhlenbeck process with parameters 
  
    
      
        θ
        =
        1
      
    
    {\displaystyle \theta =1}
  
, 
  
    
      
        μ
        =
        0
      
    
    {\displaystyle \mu =0}
  
, and 
  
    
      
        
          σ
          
            2
          
        
        =
        2
      
    
    {\displaystyle \sigma ^{2}=2}
  
.
The time of hitting a single point x > 0 by the Wiener process is a random variable with the Lévy distribution. The family of these random variables (indexed by all positive numbers x) is a left-continuous modification of a Lévy process. The right-continuous modification of this process is given by times of first exit from closed intervals [0, x].
The local time L = (Lxt)x ∈ R, t ≥ 0 of a Brownian motion describes the time that the process spends at the point x. Formally

  
    
      
        
          L
          
            x
          
        
        (
        t
        )
        =
        
          ∫
          
            0
          
          
            t
          
        
        δ
        (
        x
        −
        
          B
          
            t
          
        
        )
        
        d
        s
      
    
    {\displaystyle L^{x}(t)=\int _{0}^{t}\delta (x-B_{t})\,ds}
  

where δ is the Dirac delta function. The behaviour of the local time is characterised by Ray–Knight theorems.


=== Brownian martingales ===
Let A be an event related to the Wiener process (more formally: a set, measurable with respect to the Wiener   measure, in the space of functions), and Xt the conditional probability of A given the Wiener process on the time interval [0, t] (more formally: the Wiener measure of the set of trajectories whose concatenation with the given partial trajectory on [0, t] belongs to A). Then the process Xt is a continuous martingale. Its martingale property follows immediately from the definitions, but its continuity is a very special fact – a special case of a general theorem stating that all Brownian martingales are continuous. A Brownian martingale is, by definition, a martingale adapted to the Brownian filtration; and the Brownian filtration is, by definition, the filtration generated by the Wiener process. Also 
  
    
      
        
          B
          
            t
          
          
            2
          
        
        −
        t
      
    
    {\displaystyle B_{t}^{2}-t}
  
 and 
  
    
      
        
          e
          
            θ
            
              B
              
                t
              
            
            −
            
              
                
                  
                    θ
                    
                      2
                    
                  
                  2
                
              
            
            t
          
        
      
    
    {\displaystyle e^{\theta B_{t}-{\tfrac {\theta ^{2}}{2}}t}}
  
 are martingales.


=== Integrated Brownian motion ===
The time-integral of the Wiener process

  
    
      
        
          W
          
            (
            −
            1
            )
          
        
        (
        t
        )
        :=
        
          ∫
          
            0
          
          
            t
          
        
        W
        (
        s
        )
        
        d
        s
      
    
    {\displaystyle W^{(-1)}(t):=\int _{0}^{t}W(s)\,ds}
  

is called integrated Brownian motion or integrated Wiener process.  It arises in many applications and can be shown to have the distribution N(0, t3/3), calculated using the fact that the covariance of the Wiener process is 
  
    
      
        t
        ∧
        s
        =
        min
        (
        t
        ,
        s
        )
      
    
    {\displaystyle t\wedge s=\min(t,s)}
  
.
For the general case of the process defined by 

  
    
      
        
          V
          
            f
          
        
        (
        t
        )
        =
        
          ∫
          
            0
          
          
            t
          
        
        
          f
          ′
        
        (
        s
        )
        W
        (
        s
        )
        
        d
        s
        =
        
          ∫
          
            0
          
          
            t
          
        
        (
        f
        (
        t
        )
        −
        f
        (
        s
        )
        )
        
        d
        
          W
          
            s
          
        
      
    
    {\displaystyle V_{f}(t)=\int _{0}^{t}f'(s)W(s)\,ds=\int _{0}^{t}(f(t)-f(s))\,dW_{s}}
  

Then, for 
  
    
      
        a
        >
        0
      
    
    {\displaystyle a>0}
  
,

  
    
      
        Var
        ⁡
        (
        
          V
          
            f
          
        
        (
        t
        )
        )
        =
        
          ∫
          
            0
          
          
            t
          
        
        (
        f
        (
        t
        )
        −
        f
        (
        s
        )
        
          )
          
            2
          
        
        
        d
        s
      
    
    {\displaystyle \operatorname {Var} (V_{f}(t))=\int _{0}^{t}(f(t)-f(s))^{2}\,ds}
  

  
    
      
        cov
        ⁡
        (
        
          V
          
            f
          
        
        (
        t
        +
        a
        )
        ,
        
          V
          
            f
          
        
        (
        t
        )
        )
        =
        
          ∫
          
            0
          
          
            t
          
        
        (
        f
        (
        t
        +
        a
        )
        −
        f
        (
        s
        )
        )
        (
        f
        (
        t
        )
        −
        f
        (
        s
        )
        )
        
        d
        s
      
    
    {\displaystyle \operatorname {cov} (V_{f}(t+a),V_{f}(t))=\int _{0}^{t}(f(t+a)-f(s))(f(t)-f(s))\,ds}
  

In fact, 
  
    
      
        
          V
          
            f
          
        
        (
        t
        )
      
    
    {\displaystyle V_{f}(t)}
  
 is always a zero mean normal random variable. This allows for simulation of 
  
    
      
        
          V
          
            f
          
        
        (
        t
        +
        a
        )
      
    
    {\displaystyle V_{f}(t+a)}
  
 given 
  
    
      
        
          V
          
            f
          
        
        (
        t
        )
      
    
    {\displaystyle V_{f}(t)}
  
 by taking

  
    
      
        
          V
          
            f
          
        
        (
        t
        +
        a
        )
        =
        A
        ⋅
        
          V
          
            f
          
        
        (
        t
        )
        +
        B
        ⋅
        Z
      
    
    {\displaystyle V_{f}(t+a)=A\cdot V_{f}(t)+B\cdot Z}
  

where Z is a standard normal variable and

  
    
      
        A
        =
        
          
            
              cov
              ⁡
              (
              
                V
                
                  f
                
              
              (
              t
              +
              a
              )
              ,
              
                V
                
                  f
                
              
              (
              t
              )
              )
            
            
              Var
              ⁡
              (
              
                V
                
                  f
                
              
              (
              t
              )
              )
            
          
        
      
    
    {\displaystyle A={\frac {\operatorname {cov} (V_{f}(t+a),V_{f}(t))}{\operatorname {Var} (V_{f}(t))}}}
  

  
    
      
        
          B
          
            2
          
        
        =
        Var
        ⁡
        (
        
          V
          
            f
          
        
        (
        t
        +
        a
        )
        )
        −
        
          A
          
            2
          
        
        Var
        ⁡
        (
        
          V
          
            f
          
        
        (
        t
        )
        )
      
    
    {\displaystyle B^{2}=\operatorname {Var} (V_{f}(t+a))-A^{2}\operatorname {Var} (V_{f}(t))}
  

The case of 
  
    
      
        
          V
          
            f
          
        
        (
        t
        )
        =
        
          W
          
            (
            −
            1
            )
          
        
        (
        t
        )
      
    
    {\displaystyle V_{f}(t)=W^{(-1)}(t)}
  
 corresponds to 
  
    
      
        f
        (
        t
        )
        =
        t
      
    
    {\displaystyle f(t)=t}
  
. All these results can be seen as direct consequences of Itô isometry.
The n-times-integrated Wiener process is a zero-mean normal variable with variance 
  
    
      
        
          
            t
            
              2
              n
              +
              1
            
          
        
        
          
            (
            
              
                
                  t
                  
                    n
                  
                
                
                  n
                  !
                
              
            
            )
          
          
            2
          
        
      
    
    {\displaystyle {\frac {t}{2n+1}}\left({\frac {t^{n}}{n!}}\right)^{2}}
  
. This is given by the Cauchy formula for repeated integration.


=== Time change ===
Every continuous martingale (starting at the origin) is a time changed Wiener process.
Example:  2Wt = V(4t) where V is another Wiener process (different from W but distributed like W).
Example. 
  
    
      
        
          W
          
            t
          
          
            2
          
        
        −
        t
        =
        
          V
          
            A
            (
            t
            )
          
        
      
    
    {\displaystyle W_{t}^{2}-t=V_{A(t)}}
  
 where 
  
    
      
        A
        (
        t
        )
        =
        4
        
          ∫
          
            0
          
          
            t
          
        
        
          W
          
            s
          
          
            2
          
        
        
        
          d
        
        s
      
    
    {\displaystyle A(t)=4\int _{0}^{t}W_{s}^{2}\,\mathrm {d} s}
  
 and V is another Wiener process.
In general, if M is a continuous martingale then 
  
    
      
        
          M
          
            t
          
        
        −
        
          M
          
            0
          
        
        =
        
          V
          
            A
            (
            t
            )
          
        
      
    
    {\displaystyle M_{t}-M_{0}=V_{A(t)}}
  
 where A(t) is the quadratic variation of M on [0, t], and V is a Wiener process.
Corollary. (See also Doob's martingale convergence theorems) Let Mt be a continuous martingale, and

  
    
      
        
          M
          
            ∞
          
          
            −
          
        
        =
        
          lim inf
          
            t
            →
            ∞
          
        
        
          M
          
            t
          
        
        ,
      
    
    {\displaystyle M_{\infty }^{-}=\liminf _{t\to \infty }M_{t},}
  

  
    
      
        
          M
          
            ∞
          
          
            +
          
        
        =
        
          lim sup
          
            t
            →
            ∞
          
        
        
          M
          
            t
          
        
        .
      
    
    {\displaystyle M_{\infty }^{+}=\limsup _{t\to \infty }M_{t}.}
  

Then only the following two cases are possible:

  
    
      
        −
        ∞
        <
        
          M
          
            ∞
          
          
            −
          
        
        =
        
          M
          
            ∞
          
          
            +
          
        
        <
        +
        ∞
        ,
      
    
    {\displaystyle -\infty <M_{\infty }^{-}=M_{\infty }^{+}<+\infty ,}
  

  
    
      
        −
        ∞
        =
        
          M
          
            ∞
          
          
            −
          
        
        <
        
          M
          
            ∞
          
          
            +
          
        
        =
        +
        ∞
        ;
      
    
    {\displaystyle -\infty =M_{\infty }^{-}<M_{\infty }^{+}=+\infty ;}
  

other cases (such as 
  
    
      
        
          M
          
            ∞
          
          
            −
          
        
        =
        
          M
          
            ∞
          
          
            +
          
        
        =
        +
        ∞
        ,
      
    
    {\displaystyle M_{\infty }^{-}=M_{\infty }^{+}=+\infty ,}
  
   
  
    
      
        
          M
          
            ∞
          
          
            −
          
        
        <
        
          M
          
            ∞
          
          
            +
          
        
        <
        +
        ∞
      
    
    {\displaystyle M_{\infty }^{-}<M_{\infty }^{+}<+\infty }
  
 etc.) are of probability 0.
Especially, a nonnegative continuous martingale has a finite limit (as t → ∞) almost surely.
All stated (in this subsection) for martingales holds also for local martingales.


=== Change of measure ===
A wide class of continuous semimartingales (especially, of diffusion processes) is related to the Wiener process via a combination of time change and change of measure.
Using this fact, the qualitative properties stated above for the Wiener process can be generalized to a wide class of continuous semimartingales.


=== Complex-valued Wiener process ===
The complex-valued Wiener process may be defined as a complex-valued random process of the form 
  
    
      
        
          Z
          
            t
          
        
        =
        
          X
          
            t
          
        
        +
        i
        
          Y
          
            t
          
        
      
    
    {\displaystyle Z_{t}=X_{t}+iY_{t}}
  
 where 
  
    
      
        
          X
          
            t
          
        
      
    
    {\displaystyle X_{t}}
  
 and 
  
    
      
        
          Y
          
            t
          
        
      
    
    {\displaystyle Y_{t}}
  
 are independent Wiener processes (real-valued). In other words, it is the 2-dimensional Wiener process, where we identify 
  
    
      
        
          
            R
          
          
            2
          
        
      
    
    {\displaystyle \mathbb {R} ^{2}}
  
 with 
  
    
      
        
          C
        
      
    
    {\displaystyle \mathbb {C} }
  
.


==== Self-similarity ====
Brownian scaling, time reversal, time inversion: the same as in the real-valued case.
Rotation invariance: for every complex number 
  
    
      
        c
      
    
    {\displaystyle c}
  
 such that 
  
    
      
        
          |
        
        c
        
          |
        
        =
        1
      
    
    {\displaystyle |c|=1}
  
 the process 
  
    
      
        c
        ⋅
        
          Z
          
            t
          
        
      
    
    {\displaystyle c\cdot Z_{t}}
  
 is another complex-valued Wiener process.


==== Time change ====
If 
  
    
      
        f
      
    
    {\displaystyle f}
  
 is an entire function then the process 
  
    
      
        f
        (
        
          Z
          
            t
          
        
        )
        −
        f
        (
        0
        )
      
    
    {\displaystyle f(Z_{t})-f(0)}
  
 is a time-changed complex-valued Wiener process.
Example: 
  
    
      
        
          Z
          
            t
          
          
            2
          
        
        =
        
          (
          
            
              X
              
                t
              
              
                2
              
            
            −
            
              Y
              
                t
              
              
                2
              
            
          
          )
        
        +
        2
        
          X
          
            t
          
        
        
          Y
          
            t
          
        
        i
        =
        
          U
          
            A
            (
            t
            )
          
        
      
    
    {\displaystyle Z_{t}^{2}=\left(X_{t}^{2}-Y_{t}^{2}\right)+2X_{t}Y_{t}i=U_{A(t)}}
  
 where

  
    
      
        A
        (
        t
        )
        =
        4
        
          ∫
          
            0
          
          
            t
          
        
        
          |
        
        
          Z
          
            s
          
        
        
          
            |
          
          
            2
          
        
        
        
          d
        
        s
      
    
    {\displaystyle A(t)=4\int _{0}^{t}|Z_{s}|^{2}\,\mathrm {d} s}
  

and 
  
    
      
        U
      
    
    {\displaystyle U}
  
 is another complex-valued Wiener process.
In contrast to the real-valued case, a complex-valued martingale is generally not a time-changed complex-valued Wiener process. For example, the martingale 
  
    
      
        2
        
          X
          
            t
          
        
        +
        i
        
          Y
          
            t
          
        
      
    
    {\displaystyle 2X_{t}+iY_{t}}
  
 is not (here 
  
    
      
        
          X
          
            t
          
        
      
    
    {\displaystyle X_{t}}
  
 and 
  
    
      
        
          Y
          
            t
          
        
      
    
    {\displaystyle Y_{t}}
  
 are independent Wiener processes, as before).


=== Brownian sheet ===

The Brownian sheet is a multiparamateric generalization. The definition varies from authors, some define the Brownian sheet to have specifically a two-dimensional time parameter 
  
    
      
        t
      
    
    {\displaystyle t}
  
 while others define it for general dimensions.


== See also ==


== Notes ==


== References ==
Kleinert, Hagen (2004). Path Integrals in Quantum Mechanics, Statistics, Polymer Physics, and Financial Markets (4th ed.). Singapore: World Scientific. ISBN 981-238-107-4. (also available online: PDF-files Archived 2008-06-15 at the Wayback Machine)
Lawler, Greg (2005), Conformally invariant processes in the plane, AMS.
Stark, Henry; Woods, John (2002). Probability and Random Processes with Applications to Signal Processing (3rd ed.). New Jersey: Prentice Hall. ISBN 0-13-020071-9.
Revuz, Daniel; Yor, Marc (1994). Continuous martingales and Brownian motion (Second ed.). Springer-Verlag.
Takenaka, Shigeo (1988), "On pathwise projective invariance of Brownian motion", Proc Japan Acad, 64: 41–44.


== External links ==
Brownian Motion for the School-Going Child
Brownian Motion, "Diverse and Undulating"
Discusses history, botany and physics of Brown's original observations, with videos
"Interactive Web Application: Stochastic Processes used in Quantitative Finance". Archived from the original on 2015-09-20. Retrieved 2015-07-03.