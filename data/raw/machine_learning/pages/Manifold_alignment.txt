Manifold alignment is a class of machine learning algorithms that produce projections between sets of data, given that the original data sets lie on a common manifold. The concept was first introduced as such by Ham, Lee, and Saul in 2003, adding a manifold constraint to the general problem of correlating sets of high-dimensional vectors.


== Overview ==
Manifold alignment assumes that disparate data sets produced by similar generating processes will share a similar underlying manifold representation. By learning projections from each original space to the shared manifold, correspondences are recovered and knowledge from one domain can be transferred to another. Most manifold alignment techniques consider only two data sets, but the concept extends to arbitrarily many initial data sets.
Consider the case of aligning two data sets, 
  
    
      
        X
      
    
    {\displaystyle X}
  
 and 
  
    
      
        Y
      
    
    {\displaystyle Y}
  
, with 
  
    
      
        
          X
          
            i
          
        
        ∈
        
          
            R
          
          
            m
          
        
      
    
    {\displaystyle X_{i}\in \mathbb {R} ^{m}}
  
 and 
  
    
      
        
          Y
          
            i
          
        
        ∈
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle Y_{i}\in \mathbb {R} ^{n}}
  
.
Manifold alignment algorithms attempt to project both 
  
    
      
        X
      
    
    {\displaystyle X}
  
 and 
  
    
      
        Y
      
    
    {\displaystyle Y}
  
 into a new d-dimensional space such that the projections both minimize distance between corresponding points and preserve the local manifold structure of the original data. The projection functions are denoted:

  
    
      
        
          ϕ
          
            X
          
        
        :
        
        
          
            R
          
          
            m
          
        
        →
        
          
            R
          
          
            d
          
        
      
    
    {\displaystyle \phi _{X}:\,\mathbb {R} ^{m}\rightarrow \mathbb {R} ^{d}}
  

  
    
      
        
          ϕ
          
            Y
          
        
        :
        
        
          
            R
          
          
            n
          
        
        →
        
          
            R
          
          
            d
          
        
      
    
    {\displaystyle \phi _{Y}:\,\mathbb {R} ^{n}\rightarrow \mathbb {R} ^{d}}
  

Let 
  
    
      
        W
      
    
    {\displaystyle W}
  
 represent the binary correspondence matrix between points in 
  
    
      
        X
      
    
    {\displaystyle X}
  
 and 
  
    
      
        Y
      
    
    {\displaystyle Y}
  
:

  
    
      
        
          W
          
            i
            ,
            j
          
        
        =
        
          
            {
            
              
                
                  1
                
                
                  i
                  f
                  
                  
                    X
                    
                      i
                    
                  
                  ↔
                  
                    Y
                    
                      j
                    
                  
                
              
              
                
                  0
                
                
                  o
                  t
                  h
                  e
                  r
                  w
                  i
                  s
                  e
                
              
            
            
          
        
      
    
    {\displaystyle W_{i,j}={\begin{cases}1&if\,X_{i}\leftrightarrow Y_{j}\\0&otherwise\end{cases}}}
  

Let 
  
    
      
        
          S
          
            X
          
        
      
    
    {\displaystyle S_{X}}
  
 and 
  
    
      
        
          S
          
            Y
          
        
      
    
    {\displaystyle S_{Y}}
  
 represent pointwise similarities within data sets. This is usually encoded as the heat kernel of the adjacency matrix of a k-nearest neighbor graph.
Finally, introduce a coefficient 
  
    
      
        0
        ≤
        μ
        ≤
        1
      
    
    {\displaystyle 0\leq \mu \leq 1}
  
, which can be tuned to adjust the weight of the 'preserve manifold structure' goal, versus the 'minimize corresponding point distances' goal.
With these definitions in place, the loss function for manifold alignment can be written:

  
    
      
        arg
        ⁡
        
          min
          
            
              ϕ
              
                X
              
            
            ,
            
              ϕ
              
                Y
              
            
          
        
        μ
        
          ∑
          
            i
            ,
            j
          
        
        
          
            ‖
            
              
                ϕ
                
                  X
                
              
              
                (
                
                  X
                  
                    i
                  
                
                )
              
              −
              
                ϕ
                
                  X
                
              
              
                (
                
                  X
                  
                    j
                  
                
                )
              
            
            ‖
          
          
            2
          
        
        
          S
          
            X
            ,
            i
            ,
            j
          
        
        +
        μ
        
          ∑
          
            i
            ,
            j
          
        
        
          
            ‖
            
              
                ϕ
                
                  Y
                
              
              
                (
                
                  Y
                  
                    i
                  
                
                )
              
              −
              
                ϕ
                
                  Y
                
              
              
                (
                
                  Y
                  
                    j
                  
                
                )
              
            
            ‖
          
          
            2
          
        
        
          S
          
            Y
            ,
            i
            ,
            j
          
        
        +
        
          (
          
            1
            −
            μ
          
          )
        
        
          ∑
          
            i
            ,
            j
          
        
        ‖
        
          ϕ
          
            X
          
        
        
          (
          
            X
            
              i
            
          
          )
        
        −
        
          ϕ
          
            Y
          
        
        
          (
          
            Y
            
              j
            
          
          )
        
        
          ‖
          
            2
          
        
        
          W
          
            i
            ,
            j
          
        
      
    
    {\displaystyle \arg \min _{\phi _{X},\phi _{Y}}\mu \sum _{i,j}\left\Vert \phi _{X}\left(X_{i}\right)-\phi _{X}\left(X_{j}\right)\right\Vert ^{2}S_{X,i,j}+\mu \sum _{i,j}\left\Vert \phi _{Y}\left(Y_{i}\right)-\phi _{Y}\left(Y_{j}\right)\right\Vert ^{2}S_{Y,i,j}+\left(1-\mu \right)\sum _{i,j}\Vert \phi _{X}\left(X_{i}\right)-\phi _{Y}\left(Y_{j}\right)\Vert ^{2}W_{i,j}}
  

Solving this optimization problem is equivalent to solving a generalized eigenvalue problem using the graph laplacian of the joint matrix, G:

  
    
      
        G
        =
        
          [
          
            
              
                
                  μ
                  
                    S
                    
                      X
                    
                  
                
                
                  
                    (
                    
                      1
                      −
                      μ
                    
                    )
                  
                  W
                
              
              
                
                  
                    (
                    
                      1
                      −
                      μ
                    
                    )
                  
                  
                    W
                    
                      T
                    
                  
                
                
                  μ
                  
                    S
                    
                      Y
                    
                  
                
              
            
          
          ]
        
      
    
    {\displaystyle G=\left[{\begin{array}{cc}\mu S_{X}&\left(1-\mu \right)W\\\left(1-\mu \right)W^{T}&\mu S_{Y}\end{array}}\right]}
  


== Inter-data correspondences ==
The algorithm described above requires full pairwise correspondence information between input data sets; a supervised learning paradigm. However, this information is usually difficult or impossible to obtain in real world applications. Recent work has extended the core manifold alignment algorithm to semi-supervised

, unsupervised

, and multiple-instance

settings.


== One-step vs. two-step alignment ==
The algorithm described above performs a "one-step" alignment, finding embeddings for both data sets at the same time. A similar effect can also be achieved with "two-step" alignments

, following a slightly modified procedure:

Project each input data set to a lower-dimensional space independently, using any of a variety of dimension reduction algorithms.
Perform linear manifold alignment on the embedded data, holding the first data set fixed, mapping each additional data set onto the first's manifold. This approach has the benefit of decomposing the required computation, which lowers memory overhead and allows parallel implementations.


== Instance-level vs. feature-level projections ==
Manifold alignment can be used to find linear (feature-level) projections, or nonlinear (instance-level) embeddings. While the instance-level version generally produces more accurate alignments, it sacrifices a great degree of flexibility as the learned embedding is often difficult to parameterize. Feature-level projections allow any new instances to be easily embedded in the manifold space, and projections may be combined to form direct mappings between the original data representations. These properties are especially important for knowledge-transfer applications.


== Applications ==
Manifold alignment is suited to problems with several corpora that lie on a shared manifold, even when each corpus is of a different dimensionality. Many real-world problems fit this description, but traditional techniques are not able to take advantage of all corpora at the same time. Manifold alignment also facilitates transfer learning, in which knowledge of one domain is used to jump-start learning in correlated domains.
Applications of manifold alignment include:

Cross-language information retrieval / automatic translation
By representing documents as vector of word counts, manifold alignment can recover the mapping between documents of different languages.
Cross-language document correspondence is relatively easy to obtain, especially from multi-lingual organizations like the European Union.
Transfer learning of policy and state representations for reinforcement learning
Alignment of protein NMR structures
Accelerating model learning in robotics by sharing data generated by other robots 


== See also ==
Manifold hypothesis


== References ==


== Further reading ==
Xiong, L.; F. Wang; C. Zhang (2007). "Semi-definite manifold alignment". Proceedings of the 18th European Conference on Machine Learning. CiteSeerX 10.1.1.91.7346.
Wang, Chang; Sridhar Mahadevan (2009). "A General Framework for Manifold Alignment" (PDF). AAAI Fall Symposium on Manifold Learning and Its Applications.
Wang, Chang; Sridhar Mahadevan (2010). "Multiscale Manifold Alignment" (PDF). Univ. Of Massachusetts TR UM-CS-2010-049.
Ma, Yunqian (Apr 15, 2012). Manifold Learning Theory and Applications. Taylor & Francis Group. p. 376. ISBN 978-1-4398-7109-6.
Chang Wang's Manifold alignment overview