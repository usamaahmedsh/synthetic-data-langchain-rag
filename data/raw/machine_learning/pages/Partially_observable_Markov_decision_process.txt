A partially observable Markov decision process (POMDP) is a generalization of a Markov decision process (MDP). A POMDP models an agent decision process in which it is assumed that the system dynamics are determined by an MDP, but the agent cannot directly observe the underlying state. Instead, it must maintain a sensor model (the probability distribution of different observations given the underlying state) and the underlying MDP. Unlike the policy function in MDP which maps the underlying states to the actions, POMDP's policy is a mapping from the history of observations (or belief states) to the actions.
The POMDP framework is general enough to model a variety of real-world sequential decision processes. Applications include robot navigation problems, machine maintenance, and planning under uncertainty in general. The general framework of Markov decision processes with imperfect information was described by Karl Johan Åström in 1965 in the case of a discrete state space, and it was further studied in the operations research community where the acronym POMDP was coined. It was later adapted for problems in artificial intelligence and automated planning by Leslie P. Kaelbling and Michael L. Littman.
An exact solution to a POMDP yields the optimal action for each possible belief over the world states. The optimal action maximizes the expected reward (or minimizes the cost) of the agent over a possibly infinite horizon. The sequence of optimal actions is known as the optimal policy of the agent for interacting with its environment.


== Definition ==


=== Formal definition ===
A discrete-time POMDP is a 8-tuple 
  
    
      
        (
        
          
            S
          
        
        ,
        
          
            A
          
        
        ,
        
          
            O
          
        
        ,
        
          T
        
        ,
        
          
            T
          
          
            0
          
        
        ,
        
          O
        
        ,
        R
        ,
        N
        )
      
    
    {\displaystyle ({\mathcal {S}},{\mathcal {A}},{\mathcal {O}},\mathbb {T} ,\mathbb {T} _{0},\mathbb {O} ,R,N)}
  
, where

  
    
      
        
          
            S
          
        
      
    
    {\displaystyle {\mathcal {S}}}
  
 is a set, denoting the state-space,

  
    
      
        
          
            A
          
        
      
    
    {\displaystyle {\mathcal {A}}}
  
 is a set, denoting the action-space,

  
    
      
        
          
            O
          
        
      
    
    {\displaystyle {\mathcal {O}}}
  
 is a set, denoting the observation-space,

  
    
      
        
          T
        
        (
        
          s
          
            t
            +
            1
          
        
        
          |
        
        
          s
          
            t
          
        
        ,
        
          a
          
            t
          
        
        )
        :
        
          
            S
          
        
        ×
        
          
            A
          
        
        ×
        
          
            S
          
        
        →
        [
        0
        ,
        1
        ]
      
    
    {\displaystyle \mathbb {T} (s_{t+1}|s_{t},a_{t}):{\mathcal {S}}\times {\mathcal {A}}\times {\mathcal {S}}\to [0,1]}
  
 is a probability measure, defining the (Markovian) system dynamics

  
    
      
        
          
            T
          
          
            0
          
        
        (
        s
        )
        :
        S
        →
        
          R
        
      
    
    {\displaystyle \mathbb {T} _{0}(s):S\to \mathbb {R} }
  
 is a probability measure, defining the distribution of initial states.

  
    
      
        
          O
        
        (
        o
        
          |
        
        s
        )
        :
        
          
            O
          
        
        ×
        
          
            S
          
        
      
    
    {\displaystyle \mathbb {O} (o|s):{\mathcal {O}}\times {\mathcal {S}}}
  
 is a set of observations,

  
    
      
        R
        :
        (
        
          
            O
          
        
        ×
        
          
            H
          
        
        
          )
          
            N
          
        
        ×
        
          
            O
          
        
        →
        
          R
        
      
    
    {\displaystyle R:({\mathcal {O}}\times {\mathcal {H}})^{N}\times {\mathcal {O}}\to \mathbb {R} }
  
 is the reward function for episodes 
  
    
      
        τ
        ∈
        (
        
          
            O
          
        
        ×
        
          
            H
          
        
        
          )
          
            N
          
        
        ×
        
          
            O
          
        
      
    
    {\displaystyle \tau \in ({\mathcal {O}}\times {\mathcal {H}})^{N}\times {\mathcal {O}}}
  

  
    
      
        N
        ∈
        
          N
        
        ∪
        {
        +
        ∞
        }
      
    
    {\displaystyle N\in \mathbb {N} \cup \{+\infty \}}
  
 is the time-horizon of the process.
Initially, at time 
  
    
      
        t
        =
        0
      
    
    {\displaystyle t=0}
  
, we sample an initial unobservable state 
  
    
      
        
          
            S
          
        
        ∋
        
          s
          
            0
          
        
        ∼
        
          
            T
          
          
            0
          
        
      
    
    {\displaystyle {\mathcal {S}}\ni s_{0}\sim \mathbb {T} _{0}}
  
. At time 
  
    
      
        1
        ≤
        t
        ≤
        N
      
    
    {\displaystyle 1\leq t\leq N}
  
, we have some unobservable state 
  
    
      
        
          s
          
            t
          
        
        ∈
        
          
            S
          
        
      
    
    {\displaystyle s_{t}\in {\mathcal {S}}}
  
, we choose an action 
  
    
      
        
          a
          
            t
          
        
        ∈
        
          
            A
          
        
      
    
    {\displaystyle a_{t}\in {\mathcal {A}}}
  
, and we update our unobservable world state by sampling 
  
    
      
        
          
            S
          
        
        ∋
        
          s
          
            t
            +
            1
          
        
        
          |
        
        
          s
          
            t
          
        
        ,
        
          a
          
            t
          
        
        ∼
        
          T
        
      
    
    {\displaystyle {\mathcal {S}}\ni s_{t+1}|s_{t},a_{t}\sim \mathbb {T} }
  
. Additionally, at each time 
  
    
      
        0
        ≤
        t
        ≤
        N
      
    
    {\displaystyle 0\leq t\leq N}
  
 our agent receives an observation 
  
    
      
        
          
            O
          
        
        ∋
        
          o
          
            t
          
        
        
          |
        
        
          s
          
            t
          
        
        ∼
        
          O
        
      
    
    {\displaystyle {\mathcal {O}}\ni o_{t}|s_{t}\sim \mathbb {O} }
  
. All together, we observe an episode 
  
    
      
        τ
        =
        (
        
          o
          
            0
          
        
        ,
        
          a
          
            0
          
        
        ,
        …
        ,
        
          o
          
            N
            −
            1
          
        
        ,
        
          a
          
            N
            −
            1
          
        
        ,
        
          o
          
            N
          
        
        )
        ∈
        (
        
          
            O
          
        
        ×
        
          
            H
          
        
        
          )
          
            N
          
        
        ×
        
          
            O
          
        
        =:
        
          
            T
          
        
      
    
    {\displaystyle \tau =(o_{0},a_{0},\dots ,o_{N-1},a_{N-1},o_{N})\in ({\mathcal {O}}\times {\mathcal {H}})^{N}\times {\mathcal {O}}=:{\mathcal {T}}}
  
 with reward 
  
    
      
        R
        (
        τ
        )
        ∈
        
          R
        
      
    
    {\displaystyle R(\tau )\in \mathbb {R} }
  
. The agent's goal is to choose a policy 
  
    
      
        π
        =
        {
        
          π
          
            h
          
        
        :
        
          
            
              T
            
          
          
            h
          
        
        →
        Δ
        (
        
          
            A
          
        
        )
        }
      
    
    {\displaystyle \pi =\{\pi _{h}:{\mathcal {T}}_{h}\to \Delta ({\mathcal {A}})\}}
  
, where 
  
    
      
        
          
            
              T
            
          
          
            h
          
        
        :=
        (
        
          
            O
          
        
        ×
        
          
            A
          
        
        
          )
          
            h
          
        
        ×
        
          
            O
          
        
      
    
    {\displaystyle {\mathcal {T}}_{h}:=({\mathcal {O}}\times {\mathcal {A}})^{h}\times {\mathcal {O}}}
  
 is the space of 
  
    
      
        h
      
    
    {\displaystyle h}
  
-trajectories and 
  
    
      
        Δ
        (
        
          
            A
          
        
        )
      
    
    {\displaystyle \Delta ({\mathcal {A}})}
  
 is the 
  
    
      
        
          
            A
          
        
      
    
    {\displaystyle {\mathcal {A}}}
  
-simplex, namely the space of probability measures on 
  
    
      
        
          
            A
          
        
      
    
    {\displaystyle {\mathcal {A}}}
  
, to maximise the expected episodic reward 
  
    
      
        
          
            E
          
          
            τ
            ∼
            
              
                T
              
              
                π
              
            
          
        
        R
        (
        τ
        )
      
    
    {\displaystyle \mathbb {E} _{\tau \sim \mathbb {T} ^{\pi }}R(\tau )}
  
, where 
  
    
      
        
          
            T
          
          
            π
          
        
      
    
    {\displaystyle \mathbb {T} ^{\pi }}
  
 is the episodic distribution where actions are sampled according to the policy 
  
    
      
        π
      
    
    {\displaystyle \pi }
  
.


=== Discussion ===
Because the agent does not directly observe the environment's state, the agent must make decisions under uncertainty of the true environment state. However, by interacting with the environment and receiving observations, the agent may update its belief in the true state by updating the probability distribution of the current state. A consequence of this property is that the optimal behavior may often include (information gathering) actions that are taken purely because they improve the agent's estimate of the current state, thereby allowing it to make better decisions in the future.
It is instructive to compare the above definition with the definition of a Markov decision process. An MDP does not include the observation set, because the agent always knows with certainty the environment's current state. Alternatively, an MDP can be reformulated as a POMDP by setting the observation set to be equal to the set of states and defining the observation conditional probabilities to deterministically select the observation that corresponds to the true state.


== Belief update ==
After having taken the action 
  
    
      
        a
      
    
    {\displaystyle a}
  
 and observing 
  
    
      
        o
      
    
    {\displaystyle o}
  
, an agent needs to update its belief in the state the environment may (or not) be in. Since the state is Markovian (by assumption), maintaining a belief over the states solely requires knowledge of the previous belief state, the action taken, and the current observation. The operation is denoted 
  
    
      
        
          b
          ′
        
        =
        τ
        (
        b
        ,
        a
        ,
        o
        )
      
    
    {\displaystyle b'=\tau (b,a,o)}
  
. Below we describe how this belief update is computed.
After reaching 
  
    
      
        
          s
          ′
        
      
    
    {\displaystyle s'}
  
, the agent observes 
  
    
      
        o
        ∈
        Ω
      
    
    {\displaystyle o\in \Omega }
  
 with probability 
  
    
      
        O
        (
        o
        ∣
        
          s
          ′
        
        ,
        a
        )
      
    
    {\displaystyle O(o\mid s',a)}
  
. Let 
  
    
      
        b
      
    
    {\displaystyle b}
  
 be a probability distribution over the state space 
  
    
      
        S
      
    
    {\displaystyle S}
  
. 
  
    
      
        b
        (
        s
        )
      
    
    {\displaystyle b(s)}
  
 denotes the probability that the environment is in state 
  
    
      
        s
      
    
    {\displaystyle s}
  
. Given 
  
    
      
        b
        (
        s
        )
      
    
    {\displaystyle b(s)}
  
, then after taking action 
  
    
      
        a
      
    
    {\displaystyle a}
  
 and observing 
  
    
      
        o
      
    
    {\displaystyle o}
  
,

  
    
      
        
          b
          ′
        
        (
        
          s
          ′
        
        )
        =
        η
        O
        (
        o
        ∣
        
          s
          ′
        
        ,
        a
        )
        
          ∑
          
            s
            ∈
            S
          
        
        T
        (
        
          s
          ′
        
        ∣
        s
        ,
        a
        )
        b
        (
        s
        )
      
    
    {\displaystyle b'(s')=\eta O(o\mid s',a)\sum _{s\in S}T(s'\mid s,a)b(s)}
  

where 
  
    
      
        η
        =
        1
        
          /
        
        Pr
        (
        o
        ∣
        b
        ,
        a
        )
      
    
    {\displaystyle \eta =1/\Pr(o\mid b,a)}
  
 is a normalizing constant with 
  
    
      
        Pr
        (
        o
        ∣
        b
        ,
        a
        )
        =
        
          ∑
          
            
              s
              ′
            
            ∈
            S
          
        
        O
        (
        o
        ∣
        
          s
          ′
        
        ,
        a
        )
        
          ∑
          
            s
            ∈
            S
          
        
        T
        (
        
          s
          ′
        
        ∣
        s
        ,
        a
        )
        b
        (
        s
        )
      
    
    {\displaystyle \Pr(o\mid b,a)=\sum _{s'\in S}O(o\mid s',a)\sum _{s\in S}T(s'\mid s,a)b(s)}
  
.


== Belief MDP ==
A Markovian belief state allows a POMDP to be formulated as a Markov decision process where every belief is a state. The resulting belief MDP will thus be defined on a continuous state space (even if the "originating" POMDP has a finite number of states: there are infinite belief states (in 
  
    
      
        B
      
    
    {\displaystyle B}
  
) because there are an infinite number of probability distributions over the states (of 
  
    
      
        S
      
    
    {\displaystyle S}
  
)).
Formally, the belief MDP is defined as a tuple 
  
    
      
        (
        B
        ,
        A
        ,
        τ
        ,
        r
        ,
        γ
        )
      
    
    {\displaystyle (B,A,\tau ,r,\gamma )}
  
 where

  
    
      
        B
      
    
    {\displaystyle B}
  
 is the set of belief states over the POMDP states,

  
    
      
        A
      
    
    {\displaystyle A}
  
 is the same finite set of action as for the original POMDP,

  
    
      
        τ
      
    
    {\displaystyle \tau }
  
 is the belief state transition function,

  
    
      
        r
        :
        B
        ×
        A
        →
        
          R
        
      
    
    {\displaystyle r:B\times A\to \mathbb {R} }
  
 is the reward function on belief states,

  
    
      
        γ
      
    
    {\displaystyle \gamma }
  
 is the discount factor equal to the 
  
    
      
        γ
      
    
    {\displaystyle \gamma }
  
 in the original POMDP.
Of these, 
  
    
      
        τ
      
    
    {\displaystyle \tau }
  
 and 
  
    
      
        r
      
    
    {\displaystyle r}
  
 need to be derived from the original POMDP. 
  
    
      
        τ
      
    
    {\displaystyle \tau }
  
 is

  
    
      
        τ
        (
        b
        ,
        a
        ,
        
          b
          ′
        
        )
        =
        
          ∑
          
            o
            ∈
            Ω
          
        
        Pr
        (
        
          b
          ′
        
        
          |
        
        b
        ,
        a
        ,
        o
        )
        Pr
        (
        o
        
          |
        
        a
        ,
        b
        )
        ,
      
    
    {\displaystyle \tau (b,a,b')=\sum _{o\in \Omega }\Pr(b'|b,a,o)\Pr(o|a,b),}
  

where 
  
    
      
        Pr
        (
        o
        
          |
        
        a
        ,
        b
        )
      
    
    {\displaystyle \Pr(o|a,b)}
  
 is the value derived in the previous section and

  
    
      
        P
        r
        (
        
          b
          ′
        
        
          |
        
        b
        ,
        a
        ,
        o
        )
        =
        
          
            {
            
              
                
                  1
                
                
                  
                    if the belief update with arguments 
                  
                  b
                  ,
                  a
                  ,
                  o
                  
                     returns 
                  
                  
                    b
                    ′
                  
                
              
              
                
                  0
                
                
                  
                    otherwise 
                  
                
              
            
            
          
        
        .
      
    
    {\displaystyle Pr(b'|b,a,o)={\begin{cases}1&{\text{if the belief update with arguments }}b,a,o{\text{ returns }}b'\\0&{\text{otherwise }}\end{cases}}.}
  

The belief MDP reward function (
  
    
      
        r
      
    
    {\displaystyle r}
  
) is the expected reward from the POMDP reward function over the belief state distribution:

  
    
      
        r
        (
        b
        ,
        a
        )
        =
        
          ∑
          
            s
            ∈
            S
          
        
        b
        (
        s
        )
        R
        (
        s
        ,
        a
        )
      
    
    {\displaystyle r(b,a)=\sum _{s\in S}b(s)R(s,a)}
  
.
The belief MDP is not partially observable anymore, since at any given time the agent knows its belief, and by extension the state of the belief MDP.


=== Policy and value function ===
Unlike the "originating" POMDP (where each action is available from only one state), in the corresponding Belief MDP all belief states allow all actions, since you (almost) always have some probability of believing you are in any (originating) state. As such, 
  
    
      
        π
      
    
    {\displaystyle \pi }
  
 specifies an action 
  
    
      
        a
        =
        π
        (
        b
        )
      
    
    {\displaystyle a=\pi (b)}
  
 for any belief 
  
    
      
        b
      
    
    {\displaystyle b}
  
.
Here it is assumed the objective is to maximize the expected total discounted reward over an infinite horizon. When 
  
    
      
        R
      
    
    {\displaystyle R}
  
 defines a cost, the objective becomes the minimization of the expected cost.
The expected reward for policy 
  
    
      
        π
      
    
    {\displaystyle \pi }
  
 starting from belief 
  
    
      
        
          b
          
            0
          
        
      
    
    {\displaystyle b_{0}}
  
 is defined as

  
    
      
        
          V
          
            π
          
        
        (
        
          b
          
            0
          
        
        )
        =
        
          ∑
          
            t
            =
            0
          
          
            ∞
          
        
        
          γ
          
            t
          
        
        r
        (
        
          b
          
            t
          
        
        ,
        
          a
          
            t
          
        
        )
        =
        
          ∑
          
            t
            =
            0
          
          
            ∞
          
        
        
          γ
          
            t
          
        
        E
        
          
            [
          
        
        R
        (
        
          s
          
            t
          
        
        ,
        
          a
          
            t
          
        
        )
        ∣
        
          b
          
            0
          
        
        ,
        π
        
          
            ]
          
        
      
    
    {\displaystyle V^{\pi }(b_{0})=\sum _{t=0}^{\infty }\gamma ^{t}r(b_{t},a_{t})=\sum _{t=0}^{\infty }\gamma ^{t}E{\Bigl [}R(s_{t},a_{t})\mid b_{0},\pi {\Bigr ]}}
  

where 
  
    
      
        γ
        <
        1
      
    
    {\displaystyle \gamma <1}
  
 is the discount factor. The optimal policy 
  
    
      
        
          π
          
            ∗
          
        
      
    
    {\displaystyle \pi ^{*}}
  
 is obtained by optimizing the long-term reward.

  
    
      
        
          π
          
            ∗
          
        
        =
        
          
            
              argmax
            
            π
          
        
         
        
          V
          
            π
          
        
        (
        
          b
          
            0
          
        
        )
      
    
    {\displaystyle \pi ^{*}={\underset {\pi }{\mbox{argmax}}}\ V^{\pi }(b_{0})}
  

where 
  
    
      
        
          b
          
            0
          
        
      
    
    {\displaystyle b_{0}}
  
 is the initial belief.
The optimal policy, denoted by 
  
    
      
        
          π
          
            ∗
          
        
      
    
    {\displaystyle \pi ^{*}}
  
, yields the highest expected reward value for each belief state, compactly represented by the optimal value function 
  
    
      
        
          V
          
            ∗
          
        
      
    
    {\displaystyle V^{*}}
  
. This value function is solution to the Bellman optimality equation:

  
    
      
        
          V
          
            ∗
          
        
        (
        b
        )
        =
        
          max
          
            a
            ∈
            A
          
        
        
          
            [
          
        
        r
        (
        b
        ,
        a
        )
        +
        γ
        
          ∑
          
            o
            ∈
            Ω
          
        
        Pr
        (
        o
        ∣
        b
        ,
        a
        )
        
          V
          
            ∗
          
        
        (
        τ
        (
        b
        ,
        a
        ,
        o
        )
        )
        
          
            ]
          
        
      
    
    {\displaystyle V^{*}(b)=\max _{a\in A}{\Bigl [}r(b,a)+\gamma \sum _{o\in \Omega }\Pr(o\mid b,a)V^{*}(\tau (b,a,o)){\Bigr ]}}
  

For finite-horizon POMDPs, the optimal value function is piecewise-linear and convex. It can be represented as a finite set of vectors. In the infinite-horizon formulation, a finite vector set can approximate 
  
    
      
        
          V
          
            ∗
          
        
      
    
    {\displaystyle V^{*}}
  
 arbitrarily closely, whose shape remains convex. Value iteration applies dynamic programming update to gradually improve on the value until convergence to an 
  
    
      
        ϵ
      
    
    {\displaystyle \epsilon }
  
-optimal value function, and preserves its piecewise linearity and convexity. By improving the value, the policy is implicitly improved. Another dynamic programming technique called policy iteration explicitly represents and improves the policy instead.


== Approximate POMDP solutions ==
In practice, POMDPs are often computationally intractable to solve exactly. This intractability is often due to the curse of dimensionality or the curse of history (the fact that optimal policies may depend on the entire history of actions and observations). To address these issues, computer scientists have developed various approximate POMDP solutions. These solutions typically attempt to approximate the problem or solution with a limited number of parameters, plan only over a small part of the belief space online, or summarize the action-observation history compactly.
Grid-based algorithms comprise one approximate solution technique.  In this approach, the value function is computed for a set of points in the belief space, and interpolation is used to determine the optimal action to take for other belief states that are encountered which are not in the set of grid points. More recent work makes use of sampling techniques, generalization techniques and exploitation of problem structure, and has extended POMDP solving into large domains with millions of states. For example, adaptive grids and point-based methods sample random reachable belief points to constrain the planning to relevant areas in the belief space.
Dimensionality reduction using PCA has also been explored.
Online planning algorithms approach large POMDPs by constructing a new policy for the current belief each time a new observation is received. Such a policy only needs to consider future beliefs reachable from the current belief, which is often only a very small part of the full belief space. This family includes variants of Monte Carlo tree search and heuristic search. Similar to MDPs, it is possible to construct online algorithms that find arbitrarily near-optimal policies and have no direct computational complexity dependence on the size of the state and observation spaces.
Another line of approximate solution techniques for solving POMDPs relies on using (a subset of) the history of previous observations, actions and rewards up to the current time step as a pseudo-state. Usual techniques for solving MDPs based on these pseudo-states can then be used (e.g. Q-learning). Ideally the pseudo-states should contain the most important information from the whole history (to reduce bias) while being as compressed as possible (to reduce overfitting).


== POMDP theory ==
Planning in POMDP is undecidable in general. However, some settings have been identified to be decidable (see Table 2 in, reproduced below). Different objectives have been considered. Büchi objectives are defined by Büchi automata. Reachability is an example of a Büchi condition (for instance, reaching a good state in which all robots are home). coBüchi objectives correspond to traces that do not satisfy a given Büchi condition (for instance, not reaching a bad state in which some robot died). Parity objectives are defined via parity games; they enable to define complex objectives such that reaching a good state every 10 timesteps. The objective can be satisfied:

almost-surely, that is the probability to satisfy the objective is 1;
positive, that is the probability to satisfy the objective is strictly greater than 0;
quantitative, that is the probability to satisfy the objective is greater than a given threshold.
We also consider the finite memory case in which the agent is a finite-state machine, and the general case in which the agent has an infinite memory.


== Applications ==
POMDPs can be used to model many kinds of real-world problems. Notable applications include the use of a POMDP in management of patients with ischemic heart disease, assistive technology for persons with dementia, the conservation of the critically endangered and difficult to detect Sumatran tigers and aircraft collision avoidance.
One application is a teaching case, a crying baby problem, where a parent needs to sequentially decide whether to feed the baby based on the observation of whether the baby is crying or not, which is an imperfect representation of the actual baby's state of hunger.


== References ==


== External links ==
APPL, a fast point-based POMDP solver
Finite-state Controllers using Branch-and-Bound An Exact POMDP Solver for Policies of a Bounded Size
pomdp: Infrastructure for Partially Observable Markov Decision Processes (POMDP) an R package which includes an interface to Tony Cassandra's pomdp-solve program.
POMDPs.jl, an interface for defining and solving MDPs and POMDPs in Julia and python with a variety of solvers.
pyPOMDP, a (PO)MDP toolbox (simulator, solver, learner, file reader) for Python by Oliver Stollmann and Bastian Migge
zmdp, a POMDP solver by Trey Smith