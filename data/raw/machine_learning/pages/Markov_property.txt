In probability theory and statistics, the term Markov property refers to the memoryless property of a stochastic process, which means that its future evolution is independent of its history. It is named after the Russian mathematician  Andrey Markov. The term strong Markov property is similar to the Markov property, except that the meaning of "present" is defined in terms of a random variable known as a stopping time.
The term Markov assumption is used to describe a model where the Markov property is assumed to hold, such as a hidden Markov model.
A Markov random field extends this property to two or more dimensions or to random variables defined for an interconnected network of items. An example of a model for such a field is the Ising model.
A discrete-time stochastic process satisfying the Markov property is known as a Markov chain.


== Introduction ==
A stochastic process has the Markov property if the conditional probability distribution of future states of the process (conditional on both past and present values) depends only upon the present state; that is, given the present, the future does not depend on the past. A process with this property is said to be Markov or Markovian and known as a Markov process. Two famous classes of Markov process are the Markov chain and Brownian motion.
Note that there is a subtle, often overlooked and very important point that is often missed in the plain English statement of the definition: the statespace of the process is constant through time. The conditional description involves a fixed "bandwidth". For example, without this restriction we could augment any process to one which includes the complete history from a given initial condition and it would be made to be Markovian. But the state space would be of increasing dimensionality over time and does not meet the definition.


== History ==


== Definition ==
Let 
  
    
      
        (
        Ω
        ,
        
          
            F
          
        
        ,
        P
        )
      
    
    {\displaystyle (\Omega ,{\mathcal {F}},P)}
  
 be a probability space with a filtration 
  
    
      
        (
        
          
            
              F
            
          
          
            s
          
        
        ,
         
        s
        ∈
        I
        )
      
    
    {\displaystyle ({\mathcal {F}}_{s},\ s\in I)}
  
, for some (totally ordered) index set 
  
    
      
        I
      
    
    {\displaystyle I}
  
; and let 
  
    
      
        (
        S
        ,
        Σ
        )
      
    
    {\displaystyle (S,\Sigma )}
  
 be a measurable space. An 
  
    
      
        (
        S
        ,
        Σ
        )
      
    
    {\displaystyle (S,\Sigma )}
  
-valued stochastic process 
  
    
      
        X
        =
        {
        
          X
          
            t
          
        
        :
        Ω
        →
        S
        
          }
          
            t
            ∈
            I
          
        
      
    
    {\displaystyle X=\{X_{t}:\Omega \to S\}_{t\in I}}
  
 adapted to the filtration is said to possess the Markov property if, for each 
  
    
      
        A
        ∈
        Σ
      
    
    {\displaystyle A\in \Sigma }
  
  and each 
  
    
      
        s
        ,
        t
        ∈
        I
      
    
    {\displaystyle s,t\in I}
  
 with 
  
    
      
        s
        <
        t
      
    
    {\displaystyle s<t}
  
,

  
    
      
        P
        (
        
          X
          
            t
          
        
        ∈
        A
        ∣
        
          
            
              F
            
          
          
            s
          
        
        )
        =
        P
        (
        
          X
          
            t
          
        
        ∈
        A
        ∣
        
          X
          
            s
          
        
        )
        .
      
    
    {\displaystyle P(X_{t}\in A\mid {\mathcal {F}}_{s})=P(X_{t}\in A\mid X_{s}).}
  

In the case where 
  
    
      
        S
      
    
    {\displaystyle S}
  
 is a discrete set with the discrete sigma algebra and 
  
    
      
        I
        =
        
          N
        
      
    
    {\displaystyle I=\mathbb {N} }
  
, this can be reformulated as follows:

  
    
      
        P
        (
        
          X
          
            n
            +
            1
          
        
        =
        
          x
          
            n
            +
            1
          
        
        ∣
        
          X
          
            n
          
        
        =
        
          x
          
            n
          
        
        ,
        …
        ,
        
          X
          
            1
          
        
        =
        
          x
          
            1
          
        
        )
        =
        P
        (
        
          X
          
            n
            +
            1
          
        
        =
        
          x
          
            n
            +
            1
          
        
        ∣
        
          X
          
            n
          
        
        =
        
          x
          
            n
          
        
        )
        
           for all 
        
        n
        ∈
        
          N
        
        .
      
    
    {\displaystyle P(X_{n+1}=x_{n+1}\mid X_{n}=x_{n},\dots ,X_{1}=x_{1})=P(X_{n+1}=x_{n+1}\mid X_{n}=x_{n}){\text{ for all }}n\in \mathbb {N} .}
  

In other words, the distribution of 
  
    
      
        X
      
    
    {\displaystyle X}
  
 at time 
  
    
      
        n
        +
        1
      
    
    {\displaystyle n+1}
  
 depend solely on the state of 
  
    
      
        X
      
    
    {\displaystyle X}
  
 at time 
  
    
      
        n
      
    
    {\displaystyle n}
  
 and is independent of the state of the process at any time previous to 
  
    
      
        n
      
    
    {\displaystyle n}
  
, which corresponds precisely to the intuition described in the introduction.
If 
  
    
      
        I
        =
        [
        0
        ,
        ∞
        )
      
    
    {\displaystyle I=[0,\infty )}
  
, then 
  
    
      
        X
      
    
    {\displaystyle X}
  
 is called time-homogeneous if for all 
  
    
      
        t
        ,
        s
        ≥
        0
      
    
    {\displaystyle t,s\geq 0}
  
 the weak Markov property holds:

  
    
      
        P
        (
        
          X
          
            t
            +
            s
          
        
        ∈
        A
        ∣
        
          
            
              F
            
          
          
            s
          
        
        )
        =
        P
        (
        
          X
          
            t
          
        
        ∈
        A
        ∣
        
          X
          
            0
          
        
        =
        x
        )
        
          
            |
          
          
            x
            =
            
              X
              
                s
              
            
          
        
        =:
        
          P
          
            
              X
              
                s
              
            
          
        
        (
        
          X
          
            t
          
        
        ∈
        A
        )
      
    
    {\displaystyle P(X_{t+s}\in A\mid {\mathcal {F}}_{s})=P(X_{t}\in A\mid X_{0}=x)|_{x=X_{s}}=:P^{X_{s}}(X_{t}\in A)}
  
.
The newly introduced probability measure 
  
    
      
        
          P
          
            x
          
        
        (
        
          X
          
            t
          
        
        ∈
        ⋅
        )
      
    
    {\displaystyle P^{x}(X_{t}\in \cdot )}
  
, 
  
    
      
        x
        ∈
        S
      
    
    {\displaystyle x\in S}
  
, has the following intuition: It gives the probability that the process 
  
    
      
        X
      
    
    {\displaystyle X}
  
 lies in some set at time 
  
    
      
        t
      
    
    {\displaystyle t}
  
, when it was started in 
  
    
      
        x
      
    
    {\displaystyle x}
  
 at time zero. The function 
  
    
      
        
          P
          
            t
          
        
        (
        x
        ,
        A
        )
        :=
        
          P
          
            x
          
        
        (
        
          X
          
            t
          
        
        ∈
        A
        )
      
    
    {\displaystyle P_{t}(x,A):=P^{x}(X_{t}\in A)}
  
, 
  
    
      
        (
        t
        ,
        x
        ,
        A
        )
        ∈
        
          
            R
          
          
            +
          
        
        ×
        S
        ×
        Σ
      
    
    {\displaystyle (t,x,A)\in \mathbb {R} _{+}\times S\times \Sigma }
  
, is also called the transition function of 
  
    
      
        X
      
    
    {\displaystyle X}
  
 and the collection 
  
    
      
        (
        
          P
          
            t
          
        
        
          )
          
            t
            ≥
            0
          
        
      
    
    {\displaystyle (P_{t})_{t\geq 0}}
  
 its transition semigroup.


== Alternative formulations ==
There exists multiple alternative formulations of the elementary Markov property described above. The following are all equivalent:

For all 
  
    
      
        t
        ≥
        0
      
    
    {\displaystyle t\geq 0}
  
 the 
  
    
      
        σ
      
    
    {\displaystyle \sigma }
  
-algebras 
  
    
      
        
          
            
              F
            
          
          
            t
          
        
      
    
    {\displaystyle {\mathcal {F}}_{t}}
  
 and 
  
    
      
        
          
            
              F
            
          
          
            t
          
          ′
        
        :=
        σ
        (
        
          X
          
            s
          
        
        :
        s
        ≥
        t
        )
      
    
    {\displaystyle {\mathcal {F}}_{t}':=\sigma (X_{s}:s\geq t)}
  
 are conditionally independent given 
  
    
      
        
          X
          
            t
          
        
      
    
    {\displaystyle X_{t}}
  
. In other words, for all 
  
    
      
        A
        ∈
        
          
            
              F
            
          
          
            t
          
        
      
    
    {\displaystyle A\in {\mathcal {F}}_{t}}
  
, 
  
    
      
        B
        ∈
        
          
            
              F
            
          
          
            t
          
          ′
        
      
    
    {\displaystyle B\in {\mathcal {F}}_{t}'}
  
:

  
    
      
        P
        (
        A
        ∩
        B
        ∣
        
          X
          
            t
          
        
        )
        =
        P
        (
        A
        ∣
        
          X
          
            t
          
        
        )
        P
        (
        B
        ∣
        
          X
          
            t
          
        
        )
      
    
    {\displaystyle P(A\cap B\mid X_{t})=P(A\mid X_{t})P(B\mid X_{t})}
  
.

For all 
  
    
      
        t
        ≥
        0
      
    
    {\displaystyle t\geq 0}
  
, 
  
    
      
        B
        ∈
        
          
            
              F
            
          
          
            t
          
          ′
        
      
    
    {\displaystyle B\in {\mathcal {F}}_{t}'}
  
:

  
    
      
        P
        (
        B
        ∣
        
          
            
              F
            
          
          
            t
          
        
        )
        =
        P
        (
        B
        ∣
        
          X
          
            t
          
        
        )
      
    
    {\displaystyle P(B\mid {\mathcal {F}}_{t})=P(B\mid X_{t})}
  
.

For all 
  
    
      
        t
        ≥
        0
      
    
    {\displaystyle t\geq 0}
  
, 
  
    
      
        A
        ∈
        
          
            
              F
            
          
          
            t
          
        
      
    
    {\displaystyle A\in {\mathcal {F}}_{t}}
  
:

  
    
      
        P
        (
        A
        ∣
        
          
            
              F
            
          
          
            t
          
          
            
              
              ′
            
          
        
        )
        =
        P
        (
        A
        ∣
        
          X
          
            t
          
        
        )
      
    
    {\displaystyle P(A\mid {\mathcal {F}}_{t}^{'})=P(A\mid X_{t})}
  
.

For all 
  
    
      
        t
        ≥
        0
      
    
    {\displaystyle t\geq 0}
  
 and 
  
    
      
        Y
        :
        Ω
        →
        
          R
        
      
    
    {\displaystyle Y:\Omega \rightarrow \mathbb {R} }
  
 bounded and 
  
    
      
        
          
            
              F
            
          
          
            t
          
          ′
        
      
    
    {\displaystyle {\mathcal {F}}_{t}'}
  
-measurable

  
    
      
        E
        ⁡
        [
        Y
        ∣
        
          
            
              F
            
          
          
            t
          
        
        ]
        =
        E
        ⁡
        [
        Y
        ∣
        
          X
          
            t
          
        
        ]
      
    
    {\displaystyle \operatorname {E} [Y\mid {\mathcal {F}}_{t}]=\operatorname {E} [Y\mid X_{t}]}
  
.

For all 
  
    
      
        t
        ≥
        s
        ≥
        0
      
    
    {\displaystyle t\geq s\geq 0}
  
 and 
  
    
      
        f
        :
        S
        →
        
          R
        
      
    
    {\displaystyle f:S\rightarrow \mathbb {R} }
  
 bounded and measurable

  
    
      
        E
        ⁡
        [
        f
        (
        
          X
          
            t
          
        
        )
        ∣
        
          
            
              F
            
          
          
            s
          
        
        ]
        =
        E
        ⁡
        [
        f
        (
        
          X
          
            t
          
        
        )
        ∣
        
          X
          
            s
          
        
        ]
      
    
    {\displaystyle \operatorname {E} [f(X_{t})\mid {\mathcal {F}}_{s}]=\operatorname {E} [f(X_{t})\mid X_{s}]}
  
.

For all 
  
    
      
        t
        ≥
        s
        ≥
        0
      
    
    {\displaystyle t\geq s\geq 0}
  
 and 
  
    
      
        f
        :
        S
        →
        
          R
        
      
    
    {\displaystyle f:S\rightarrow \mathbb {R} }
  
 continuous with compact support

  
    
      
        E
        ⁡
        [
        f
        (
        
          X
          
            t
          
        
        )
        ∣
        
          
            
              F
            
          
          
            s
          
        
        ]
        =
        E
        ⁡
        [
        f
        (
        
          X
          
            t
          
        
        )
        ∣
        
          X
          
            s
          
        
        ]
      
    
    {\displaystyle \operatorname {E} [f(X_{t})\mid {\mathcal {F}}_{s}]=\operatorname {E} [f(X_{t})\mid X_{s}]}
  
.

For all 
  
    
      
        0
        ≤
        
          s
          
            1
          
        
        <
        .
        .
        .
        <
        
          s
          
            n
          
        
        <
        s
        <
        t
      
    
    {\displaystyle 0\leq s_{1}<...<s_{n}<s<t}
  
 and 
  
    
      
        f
        :
        S
        →
        
          R
        
      
    
    {\displaystyle f:S\rightarrow \mathbb {R} }
  
 continuous with compact support

  
    
      
        E
        ⁡
        [
        f
        (
        
          X
          
            t
          
        
        )
        ∣
        
          X
          
            s
          
        
        ,
        
          X
          
            
              s
              
                n
              
            
          
        
        ,
        .
        .
        .
        ,
        
          X
          
            
              s
              
                1
              
            
          
        
        ]
        =
        E
        ⁡
        [
        f
        (
        
          X
          
            t
          
        
        )
        ∣
        
          X
          
            s
          
        
        ]
      
    
    {\displaystyle \operatorname {E} [f(X_{t})\mid X_{s},X_{s_{n}},...,X_{s_{1}}]=\operatorname {E} [f(X_{t})\mid X_{s}]}
  
.
If there exists a so-called shift-semigroup 
  
    
      
        (
        
          θ
          
            t
          
        
        
          )
          
            t
            ≥
            0
          
        
      
    
    {\displaystyle (\theta _{t})_{t\geq 0}}
  
, i.e., functions 
  
    
      
        
          θ
          
            t
          
        
        :
        Ω
        →
        Ω
      
    
    {\displaystyle \theta _{t}:\Omega \to \Omega }
  
 such that

  
    
      
        
          θ
          
            0
          
        
        =
        
          
            i
            d
          
          
            Ω
          
        
      
    
    {\displaystyle \theta _{0}=\mathrm {id} _{\Omega }}
  
,

  
    
      
        
          θ
          
            t
          
        
        ∘
        
          θ
          
            s
          
        
        =
        
          θ
          
            t
            +
            s
          
        
        
        ∀
        s
        ,
        t
        ≥
        0
      
    
    {\displaystyle \theta _{t}\circ \theta _{s}=\theta _{t+s}\quad \forall s,t\geq 0}
  
 (semigroup property),

  
    
      
        
          X
          
            t
          
        
        ∘
        
          θ
          
            s
          
        
        =
        
          X
          
            t
            +
            s
          
        
        
        ∀
        s
        ,
        t
        ≥
        0
      
    
    {\displaystyle X_{t}\circ \theta _{s}=X_{t+s}\quad \forall s,t\geq 0}
  
,
then the Markov property is equivalent to:

For all 
  
    
      
        t
        ≥
        0
      
    
    {\displaystyle t\geq 0}
  
 and 
  
    
      
        Λ
        ∈
        
          
            
              F
            
          
          
            0
          
          ′
        
      
    
    {\displaystyle \Lambda \in {\mathcal {F}}_{0}'}
  

  
    
      
        P
        (
        
          θ
          
            t
          
          
            −
            1
          
        
        (
        Λ
        )
        ∣
        
          
            
              F
            
          
          
            t
          
        
        )
        =
        P
        (
        
          θ
          
            t
          
          
            −
            1
          
        
        (
        Λ
        )
        ∣
        
          X
          
            t
          
        
        )
      
    
    {\displaystyle P(\theta _{t}^{-1}(\Lambda )\mid {\mathcal {F}}_{t})=P(\theta _{t}^{-1}(\Lambda )\mid X_{t})}
  
.

For all 
  
    
      
        t
        ≥
        0
      
    
    {\displaystyle t\geq 0}
  
 and 
  
    
      
        Y
        :
        Ω
        →
        
          R
        
      
    
    {\displaystyle Y:\Omega \rightarrow \mathbb {R} }
  
 bounded and 
  
    
      
        
          
            
              F
            
          
          
            0
          
          ′
        
      
    
    {\displaystyle {\mathcal {F}}_{0}'}
  
-measurable

  
    
      
        E
        ⁡
        [
        Y
        ∘
        
          θ
          
            t
          
        
        ∣
        
          
            
              F
            
          
          
            t
          
        
        ]
        =
        E
        ⁡
        [
        Y
        ∘
        
          θ
          
            t
          
        
        ∣
        
          X
          
            t
          
        
        ]
      
    
    {\displaystyle \operatorname {E} [Y\circ \theta _{t}\mid {\mathcal {F}}_{t}]=\operatorname {E} [Y\circ \theta _{t}\mid X_{t}]}
  
.
Depending on the situation, some formulations might be easier to verify or to use than others.


== Strong Markov property ==
Suppose that 
  
    
      
        X
        =
        (
        
          X
          
            t
          
        
        :
        t
        ≥
        0
        )
      
    
    {\displaystyle X=(X_{t}:t\geq 0)}
  
 is a stochastic process on a probability space 
  
    
      
        (
        Ω
        ,
        
          
            F
          
        
        ,
        P
        )
      
    
    {\displaystyle (\Omega ,{\mathcal {F}},P)}
  
 with natural filtration 
  
    
      
        {
        
          
            
              F
            
          
          
            t
          
        
        
          }
          
            t
            ≥
            0
          
        
      
    
    {\displaystyle \{{\mathcal {F}}_{t}\}_{t\geq 0}}
  
. Then for any stopping time 
  
    
      
        τ
      
    
    {\displaystyle \tau }
  
 on 
  
    
      
        Ω
      
    
    {\displaystyle \Omega }
  
, we can define

  
    
      
        
          
            
              F
            
          
          
            τ
          
        
        =
        {
        A
        ∈
        
          
            F
          
        
        :
        ∀
        t
        ≥
        0
        ,
        {
        τ
        ≤
        t
        }
        ∩
        A
        ∈
        
          
            
              F
            
          
          
            t
          
        
        }
      
    
    {\displaystyle {\mathcal {F}}_{\tau }=\{A\in {\mathcal {F}}:\forall t\geq 0,\{\tau \leq t\}\cap A\in {\mathcal {F}}_{t}\}}
  
.
Then 
  
    
      
        X
      
    
    {\displaystyle X}
  
 is said to have the strong Markov property if, for each stopping time 
  
    
      
        τ
      
    
    {\displaystyle \tau }
  
, conditional on the event 
  
    
      
        {
        τ
        <
        ∞
        }
      
    
    {\displaystyle \{\tau <\infty \}}
  
, we have that for each 
  
    
      
        t
        ≥
        0
      
    
    {\displaystyle t\geq 0}
  
, 
  
    
      
        
          X
          
            τ
            +
            t
          
        
      
    
    {\displaystyle X_{\tau +t}}
  
 is independent of 
  
    
      
        
          
            
              F
            
          
          
            τ
          
        
      
    
    {\displaystyle {\mathcal {F}}_{\tau }}
  
 given 
  
    
      
        
          X
          
            τ
          
        
      
    
    {\displaystyle X_{\tau }}
  
. This is equivalent to

  
    
      
        P
        (
        
          X
          
            τ
            +
            t
          
        
        ∈
        A
        ,
        τ
        <
        ∞
        ∣
        
          
            
              F
            
          
          
            τ
          
        
        )
        =
        
          1
          
            {
            τ
            <
            ∞
            }
          
        
        P
        (
        
          X
          
            t
          
        
        ∈
        A
        ∣
        
          X
          
            0
          
        
        =
        
          X
          
            τ
          
        
        )
      
    
    {\displaystyle P(X_{\tau +t}\in A,\tau <\infty \mid {\mathcal {F}}_{\tau })=1_{\{\tau <\infty \}}P(X_{t}\in A\mid X_{0}=X_{\tau })}
  
 for all 
  
    
      
        A
        ∈
        
          
            F
          
        
      
    
    {\displaystyle A\in {\mathcal {F}}}
  
,
where 
  
    
      
        
          1
          
            {
            τ
            <
            ∞
            }
          
        
      
    
    {\displaystyle 1_{\{\tau <\infty \}}}
  
 denotes to indicator function of the set 
  
    
      
        {
        τ
        <
        ∞
        }
      
    
    {\displaystyle \{\tau <\infty \}}
  
.
The strong Markov property implies the ordinary Markov property since by taking the stopping time 
  
    
      
        τ
        =
        t
      
    
    {\displaystyle \tau =t}
  
, the ordinary Markov property can be deduced. The converse is in general not true.
The strong Markov property only leads to non-trivial results in continuous time (i.e., results which do not hold with merely the Markov property), as in the discrete case the strong and the elementary Markov property are equivalent.


== Feller property ==

Although the strong Markov property is in general stronger than the elementary Markov property, it is fulfilled by Markov processes with sufficiently "nice" regularity properties.
A continuous time Markov process is said to have the Feller property, if its transition semigroup 
  
    
      
        (
        
          P
          
            t
          
        
        
          )
          
            t
            ≥
            0
          
        
      
    
    {\displaystyle (P_{t})_{t\geq 0}}
  
 (see above) fulfills

  
    
      
        
          P
          
            t
          
        
        f
        :=
        ∫
        f
        (
        x
        )
        
          P
          
            t
          
        
        (
        ⋅
        ,
        d
        x
        )
        ∈
        
          C
          
            0
          
        
        (
        S
        )
      
    
    {\displaystyle P_{t}f:=\int f(x)P_{t}(\cdot ,dx)\in C_{0}(S)}
  
 for all 
  
    
      
        f
        ∈
        
          C
          
            0
          
        
        (
        S
        )
      
    
    {\displaystyle f\in C_{0}(S)}
  
,

  
    
      
        
          lim
          
            t
            →
            0
          
        
        
          |
        
        
          |
        
        
          P
          
            t
          
        
        f
        −
        f
        
          |
        
        
          
            |
          
          
            ∞
          
        
        =
        0
      
    
    {\displaystyle \lim _{t\to 0}||P_{t}f-f||_{\infty }=0}
  
 for all 
  
    
      
        f
        ∈
        
          C
          
            0
          
        
        (
        S
        )
      
    
    {\displaystyle f\in C_{0}(S)}
  
,
where 
  
    
      
        
          C
          
            0
          
        
        (
        S
        )
      
    
    {\displaystyle C_{0}(S)}
  
 denotes the set of continuous functions vanishing at infinity and 
  
    
      
        
          |
        
        
          |
        
        ⋅
        
          |
        
        
          
            |
          
          
            ∞
          
        
      
    
    {\displaystyle ||\cdot ||_{\infty }}
  
 the sup norm. Then one can show that (if the filtration is augmented) such a process has a version with right-continuous (even càdlàg) paths, which in turn fulfills the strong Markov property.


== Examples ==


=== Intuitive example ===
Assume that an urn contains two red balls and one green ball. One ball was drawn yesterday, one ball was drawn today, and the final ball will be drawn tomorrow. All of the draws are "without replacement".
Suppose you know that today's ball was red, but you have no information about yesterday's ball. The chance that tomorrow's ball will be red is 1/2. That's because the only two remaining outcomes for this random experiment are:

On the other hand, if you know that both today and yesterday's balls were red, then you are guaranteed to get a green ball tomorrow.
This discrepancy shows that the probability distribution for tomorrow's color depends not only on the present value, but is also affected by information about the past. This stochastic process of observed colors doesn't have the Markov property. Using the same experiment above, if sampling "without replacement" is changed to sampling "with replacement," the process of observed colors will have the Markov property.


=== Stochastic processes ===
Many prominent stochastic processes are Markov processes: The Brownian motion, the Brownian bridge, the stochastic exponential, the Ornstein-Uhlenbeck process and the Poisson process have the Markov property.
More generally, any semimartingale 
  
    
      
        X
      
    
    {\displaystyle X}
  
 with values in 
  
    
      
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle \mathbb {R} ^{n}}
  
 that is given by the stochastic differential equation

  
    
      
        
          X
          
            t
          
        
        =
        
          X
          
            0
          
        
        +
        
          ∑
          
            i
            =
            1
          
          
            d
          
        
        
          
            [
          
        
        
          ∫
          
            0
          
          
            t
          
        
        
          g
          
            i
          
        
        (
        
          X
          
            s
          
        
        )
        d
        s
        +
        
          ∫
          
            0
          
          
            t
          
        
        
          f
          
            i
          
        
        (
        
          X
          
            s
          
        
        )
        d
        
          B
          
            s
          
          
            i
          
        
        
          
            ]
          
        
      
    
    {\displaystyle X_{t}=X_{0}+\sum _{i=1}^{d}{\Big [}\int _{0}^{t}g_{i}(X_{s})ds+\int _{0}^{t}f_{i}(X_{s})dB_{s}^{i}{\Big ]}}
  
,
where 
  
    
      
        B
        =
        (
        
          B
          
            1
          
        
        ,
        .
        .
        .
        ,
        
          B
          
            d
          
        
        )
      
    
    {\displaystyle B=(B^{1},...,B^{d})}
  
 is a 
  
    
      
        d
      
    
    {\displaystyle d}
  
-dimensional Brownian motion and 
  
    
      
        
          f
          
            1
          
        
        ,
        .
        .
        .
        ,
        
          f
          
            d
          
        
        ,
        
          g
          
            1
          
        
        ,
        .
        .
        .
        ,
        
          g
          
            d
          
        
        :
        
          
            R
          
          
            n
          
        
        →
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle f_{1},...,f_{d},g_{1},...,g_{d}:\mathbb {R} ^{n}\to \mathbb {R} ^{n}}
  
 are autonomous (i.e., they do not depend on time) Lipschitz functions, is time-homogeneous and has the strong Markov property. If
  
    
      
        
          f
          
            1
          
        
        ,
        .
        .
        .
        ,
        
          f
          
            d
          
        
        ,
        
          g
          
            1
          
        
        ,
        .
        .
        .
        ,
        
          g
          
            d
          
        
      
    
    {\displaystyle f_{1},...,f_{d},g_{1},...,g_{d}}
  
 are not autonomous, then 
  
    
      
        X
      
    
    {\displaystyle X}
  
 still has the elementary Markov property.


== Applications ==


=== Forecasting ===
In the fields of predictive modelling and probabilistic forecasting, the Markov property is considered desirable since it may enable the reasoning and resolution of the problem that otherwise would not be possible to be resolved because of its intractability. Such a model is known as a Markov model.


=== Markov Chain Monte Carlo ===
An application of the Markov property in a generalized form is in Markov chain Monte Carlo computations in the context of Bayesian statistics.


== See also ==
Causal Markov condition
Chapman–Kolmogorov equation
Hysteresis
Markov blanket
Markov chain
Markov decision process
Markov model


== References ==