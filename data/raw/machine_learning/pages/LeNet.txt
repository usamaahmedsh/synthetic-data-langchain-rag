LeNet is a series of convolutional neural network architectures created by a research group in AT&T Bell Laboratories during the 1988 to 1998 period, centered around Yann LeCun. They were designed for reading small grayscale images of handwritten digits and letters, and were used in ATM for reading cheques.
Convolutional neural networks are a kind of feed-forward neural network whose artificial neurons can respond to a part of the surrounding cells in the coverage range and perform well in large-scale image processing. LeNet-5 was one of the earliest convolutional neural networks and was historically important during the development of deep learning.
In general, when LeNet is referred to without a number, it refers to the 1998 version, the most well-known version. It is also sometimes called LeNet-5.


== Development history ==

In 1988, LeCun joined the Adaptive Systems Research Department at AT&T Bell Laboratories in Holmdel, New Jersey, headed by Lawrence D. Jackel.In 1988, LeCun et al. published a neural network design that recognize handwritten zip code. However, its convolutional kernels were hand-designed.
In 1989, Yann LeCun et al. at Bell Labs first applied the backpropagation algorithm to practical applications, and believed that the ability to learn network generalization could be greatly enhanced by providing constraints from the task's domain. He combined a convolutional neural network trained by backpropagation algorithms to read handwritten numbers and successfully applied it in identifying handwritten zip code numbers provided by the US Postal Service. This was the prototype of what later came to be called LeNet-1. In the same year, LeCun described a small handwritten digit recognition problem in another paper, and showed that even though the problem is linearly separable, single-layer networks exhibited poor generalization capabilities. When using shift-invariant feature detectors on a multi-layered, constrained network, the model could perform very well. He believed that these results proved that minimizing the number of free parameters in the neural network could enhance the generalization ability of the neural network.
In 1990, their paper described the application of backpropagation networks in handwritten digit recognition again. They only performed minimal preprocessing on the data, and the model was carefully designed for this task and it was highly constrained. The input data consisted of images, each containing a number, and the test results on the postal code digital data provided by the US Postal Service showed that the model had an error rate of only 1% and a rejection rate of about 9%.
Their research continued for the next four years, and in 1994 the MNIST database was developed, for which LeNet-1 was too small, hence a new LeNet-4 was trained on it.
A year later the AT&T Bell Labs collective reviewed various methods on handwritten character recognition in paper, using standard handwritten digits to identify benchmark tasks. These models were compared and the results showed that the latest network outperformed other models.
By 1998 Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner were able to provide examples of practical applications of neural networks, such as two systems for recognizing handwritten characters online and models that could read millions of checks per day, which includes a description of LeNet-5.
The research achieved great success and aroused the interest of scholars in the study of neural networks. While the architecture of the best performing neural networks today are not the same as that of LeNet, the network was the starting point for a large number of neural network architectures, and also brought inspiration to the field.


== Architecture ==

LeNet has several common motifs of modern convolutional neural networks, such as convolutional layer, pooling layer and full connection layer.

Every convolutional layer includes three parts: convolution, pooling, and nonlinear activation functions
Using convolution to extract spatial features (Convolution was called receptive fields originally)
Subsampling average pooling layer
tanh activation function
fully connected layers in the final layers for classification
Sparse connection between layers to reduce the complexity of computation
In 1989, LeCun et al. published a report, which contained "Net-1" to "Net-5". There were many subsequent refinements, up to 1998, and the naming is inconsistent. Generally, when people speak of "LeNet" they refer to the 1998 LeNet, also known as "LeNet-5".
LeNet-1, 4, 5 had been referred to in, but it is unclear what LeNet-2, LeNet-3 might refer to.


=== 1988 Net ===
The first neural network published by the LeCun research group was in 1988. It was a hybrid approach. The first stage scaled, deskewed, and skeletonized the input image. The second stage was a convolutional layer with 18 hand-designed kernels. The third stage was a fully connected network with one hidden layer.
The dataset was a collection of handwritten digit images extracted from actual U.S. Mail, which was the same dataset used in the famed 1989 report.


=== Net-1 to Net-5 ===
Net-1 to Net-5 were published in a 1989 report. The last layer of all of them were fully connected. The original paper does not explain the padding strategy. All cells have an independent bias, including the output cells of convolutional layers.

Net-1: No hidden layer. Fully connected. 
  
    
      
        (
        16
        ×
        16
        )
        →
        10
      
    
    {\displaystyle (16\times 16)\to 10}
  
.
Net-2: One hidden fully connected layer with 12 hidden units. 
  
    
      
        (
        16
        ×
        16
        )
        →
        12
        →
        10
      
    
    {\displaystyle (16\times 16)\to 12\to 10}
  
.
Net-3: Two hidden convolutional layers. 
  
    
      
        (
        16
        ×
        16
        )
        →
        (
        8
        ×
        8
        )
        →
        (
        4
        ×
        4
        )
        →
        10
      
    
    {\displaystyle (16\times 16)\to (8\times 8)\to (4\times 4)\to 10}
  
. Both are locally connected layers with input shape 
  
    
      
        3
        ×
        3
      
    
    {\displaystyle 3\times 3}
  
 and stride 2.
Net-4: Two hidden layers, the first is a convolution, the second is locally connected. 
  
    
      
        (
        16
        ×
        16
        )
        →
        (
        8
        ×
        8
        ×
        2
        )
        →
        (
        4
        ×
        4
        )
        →
        10
      
    
    {\displaystyle (16\times 16)\to (8\times 8\times 2)\to (4\times 4)\to 10}
  
. The convolution layer has 2 kernels of shape 
  
    
      
        3
        ×
        3
      
    
    {\displaystyle 3\times 3}
  
 and stride 2. The locally connected layer has input shape 
  
    
      
        5
        ×
        5
        ×
        2
      
    
    {\displaystyle 5\times 5\times 2}
  
 and stride 1.
Net-5: Two convolutional hidden layers. 
  
    
      
        (
        16
        ×
        16
        )
        →
        (
        8
        ×
        8
        ×
        2
        )
        →
        (
        4
        ×
        4
        ×
        4
        )
        →
        10
      
    
    {\displaystyle (16\times 16)\to (8\times 8\times 2)\to (4\times 4\times 4)\to 10}
  
. The first convolution layer has 2 kernels of shape 
  
    
      
        3
        ×
        3
      
    
    {\displaystyle 3\times 3}
  
 and stride 2. The second convolutional layer has 4 kernels of shape 
  
    
      
        5
        ×
        5
        ×
        2
      
    
    {\displaystyle 5\times 5\times 2}
  
 and stride 1.
The dataset contained 480 binary images, each sized 16×16 pixels. Originally, 12 examples of each digit were hand-drawn on a 16×13 bitmap using a mouse, resulting in 120 images. Then, each image was shifted horizontally in 4 consecutive positions to generate a 16×16 version, yielding the 480 images.
From these, 320 images (32 per digit) were randomly selected for training and the remaining 160 images (16 per digit) were used for testing. Performance on training set is 100% for all networks, but they differ in test set performance.


=== 1989 LeNet ===
The LeNet published in 1989 has 3 hidden layers (H1-H3) and an output layer. It has 1256 units, 64660 connections, and 9760 independent parameters.

H1 (Convolutional): 
  
    
      
        16
        ×
        16
        →
        12
        ×
        8
        ×
        8
      
    
    {\displaystyle 16\times 16\to 12\times 8\times 8}
  
 with 
  
    
      
        5
        ×
        5
      
    
    {\displaystyle 5\times 5}
  
 kernels.
H2 (Convolutional): 
  
    
      
        12
        ×
        8
        ×
        8
        →
        12
        ×
        4
        ×
        4
      
    
    {\displaystyle 12\times 8\times 8\to 12\times 4\times 4}
  
 with 
  
    
      
        8
        ×
        5
        ×
        5
      
    
    {\displaystyle 8\times 5\times 5}
  
 kernels.
H3: 30 units fully connected to H2.
Output: 10 units fully connected to H3, representing the 10 digit classes (0-9).
The connection pattern between H1 and H2 is described in . There were no separate pooling layer, as it was deemed too computationally expensive.
The dataset was called the "US Postal Service database", and it was 9298 grayscale images of resolution 16×16, digitized from handwritten zip codes that appeared on U.S. mail passing through the Buffalo, New York post office. The training set had 7291 data points, and test set had 2007. Both training and test set contained ambiguous, unclassifiable, and misclassified data. The task is rather difficult. On the test set, two humans (Jane Bromley and Eduard Säckinger) made errors at an average rate of 2.5%.
Training took 3 days on a Sun-4/260 using a diagonal Hessian approximation of Newton's method. It was implemented in the SN Neural Network Simulator. It took 23 epochs over the training set.
Compared to the previous 1988 architecture, there was no skeletonization, and the convolutional kernels were learned automatically by backpropagation.


=== 1990 LeNet ===
A later version of 1989 LeNet has four hidden layers (H1-H4) and an output layer. It takes a 28x28 pixel image as input, though the active region is 16x16 to avoid boundary effects.

H1 (Convolutional): 
  
    
      
        28
        ×
        28
        →
        4
        ×
        24
        ×
        24
      
    
    {\displaystyle 28\times 28\to 4\times 24\times 24}
  
 with 
  
    
      
        5
        ×
        5
      
    
    {\displaystyle 5\times 5}
  
 kernels. This layer has 
  
    
      
        104
      
    
    {\displaystyle 104}
  
 trainable parameters (100 from kernels, 4 from biases).
H2 (Pooling): 
  
    
      
        4
        ×
        24
        ×
        24
        →
        4
        ×
        12
        ×
        12
      
    
    {\displaystyle 4\times 24\times 24\to 4\times 12\times 12}
  
 by 
  
    
      
        2
        ×
        2
      
    
    {\displaystyle 2\times 2}
  
 average pooling.
H3 (Convolutional): 
  
    
      
        4
        ×
        12
        ×
        12
        →
        12
        ×
        8
        ×
        8
      
    
    {\displaystyle 4\times 12\times 12\to 12\times 8\times 8}
  
 with 
  
    
      
        5
        ×
        5
      
    
    {\displaystyle 5\times 5}
  
 kernels. Some kernels take input from 1 feature map, while others take inputs from 2 feature maps.
H4 (Pooling): 
  
    
      
        12
        ×
        8
        ×
        8
        →
        12
        ×
        4
        ×
        4
      
    
    {\displaystyle 12\times 8\times 8\to 12\times 4\times 4}
  
 by 
  
    
      
        2
        ×
        2
      
    
    {\displaystyle 2\times 2}
  
 average pooling.
Output: 10 units fully connected to H4, representing the 10 digit classes (0-9).
The network 4635 units, 98442 connections, and 2578 trainable parameters. Its architecture was designed by beginning with the 1989 LeNet, then pruning the parameter count by 4x via Optimal Brain Damage. One forward pass requires about 140,000 multiply-add operations. Its size is 50 kB in memory. It was also called LeNet-1. On a SPARCstation 10, it took 0.5 weeks to train, and 0.015 seconds to classify one image.


=== 1994 LeNet ===
1994 LeNet was a larger version of 1989 LeNet designed to fit the larger MNIST database. It was also called LeNet-4. It had more feature maps in its convolutional layers, and had an additional layer of hidden units, fully connected to both the last convolutional layer and to the output units. It has 2 convolutions, 2 average poolings, and 2 fully connected layers. It has about 17000 trainable parameters.
One forward pass requires about 260,000 multiply-adds. Its size is 60 kB in memory. On a SPARCstation 10, it took 2 weeks to train, and 0.03 seconds to classify one image.


=== 1998 LeNet ===
1998 LeNet is similar to 1994 LeNet, but with more fully connected layers. Its architecture is shown in the image on the right. It has 2 convolutions, 2 subsamplings, and 3 fully connected layers. It is usually called LeNet-5. It has around 60000 trainable parameters.
Specifically, it has the following layers:

Input: (Implicitly 
  
    
      
        1
        ×
        32
        ×
        32
      
    
    {\displaystyle 1\times 32\times 32}
  
 pixel image)
C1 (Convolutional): 
  
    
      
        1
        ×
        32
        ×
        32
        →
        6
        ×
        28
        ×
        28
      
    
    {\displaystyle 1\times 32\times 32\to 6\times 28\times 28}
  
 with 
  
    
      
        5
        ×
        5
      
    
    {\displaystyle 5\times 5}
  
 kernels, with 156 trainable parameters.
S2 (Subsampling): 
  
    
      
        6
        ×
        28
        ×
        28
        →
        6
        ×
        14
        ×
        14
      
    
    {\displaystyle 6\times 28\times 28\to 6\times 14\times 14}
  
 with 
  
    
      
        2
        ×
        2
      
    
    {\displaystyle 2\times 2}
  
 pooling, with 12 trainable parameters.
C3 (Convolutional): 
  
    
      
        6
        ×
        14
        ×
        14
        →
        16
        ×
        10
        ×
        10
      
    
    {\displaystyle 6\times 14\times 14\to 16\times 10\times 10}
  
 with 
  
    
      
        k
        ×
        5
        ×
        5
      
    
    {\displaystyle k\times 5\times 5}
  
 kernels for various values of 
  
    
      
        k
      
    
    {\displaystyle k}
  
 (see Table below for exact connections in this layer), with 1516 trainable parameters.
S4 (Subsampling): 
  
    
      
        16
        ×
        10
        ×
        10
        →
        16
        ×
        5
        ×
        5
      
    
    {\displaystyle 16\times 10\times 10\to 16\times 5\times 5}
  
 with 
  
    
      
        2
        ×
        2
      
    
    {\displaystyle 2\times 2}
  
 pooling, with 32 trainable parameters.
C5 (Convolutional): 
  
    
      
        16
        ×
        5
        ×
        5
        →
        120
        ×
        1
        ×
        1
      
    
    {\displaystyle 16\times 5\times 5\to 120\times 1\times 1}
  
 with 
  
    
      
        16
        ×
        5
        ×
        5
      
    
    {\displaystyle 16\times 5\times 5}
  
 kernels, with 48120 trainable parameters.
F6: 
  
    
      
        84
      
    
    {\displaystyle 84}
  
 units fully connected to C5, with trainable parameters, with 10164 trainable parameters.
Output: 
  
    
      
        10
      
    
    {\displaystyle 10}
  
 units (RBF) fully connected to F6, representing the 10 digit classes.
Each of the "subsampling" layers is not precisely an average pooling, but with trainable parameters. Specifically, consider a single cell in S2. It takes as input 4 cells in C1. Let these have values 
  
    
      
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            4
          
        
      
    
    {\displaystyle x_{1},\dots ,x_{4}}
  
, then the cell in S2 has value 
  
    
      
        σ
        
          (
          
            w
            
              (
              
                
                  ∑
                  
                    i
                    =
                    1
                  
                  
                    4
                  
                
                
                  x
                  
                    i
                  
                
              
              )
            
            +
            b
          
          )
        
      
    
    {\displaystyle \sigma \left(w\left(\sum _{i=1}^{4}x_{i}\right)+b\right)}
  
, where 
  
    
      
        w
        ,
        b
        ∈
        
          R
        
      
    
    {\displaystyle w,b\in \mathbb {R} }
  
 are trainable parameters, and 
  
    
      
        σ
      
    
    {\displaystyle \sigma }
  
 is a sigmoidal activation function, sometimes called "LeCun's tanh". It is a scaled version of the hyperbolic tangent activation function: 
  
    
      
        1.7159
        tanh
        ⁡
        (
        2
        x
        
          /
        
        3
        )
      
    
    {\displaystyle 1.7159\tanh(2x/3)}
  
. It was designed so that it maps the interval 
  
    
      
        [
        −
        1
        ,
        +
        1
        ]
      
    
    {\displaystyle [-1,+1]}
  
 to itself, thus ensuring that the overall gain is around 1 in "normal operating conditions", and that 
  
    
      
        
          |
        
        
          f
          ″
        
        (
        x
        )
        
          |
        
      
    
    {\displaystyle |f''(x)|}
  
 is at maximum when 
  
    
      
        x
        =
        −
        1
        ,
        +
        1
      
    
    {\displaystyle x=-1,+1}
  
, which improves convergence at the end of training.
In the following table, each column indicates which of the 6 feature maps in S2 are combined by the units in each of the 15 feature maps of C3.

Even though C5 has output shape 
  
    
      
        120
        ×
        1
        ×
        1
      
    
    {\displaystyle 120\times 1\times 1}
  
, it is not a fully connected layer, because the network is designed to be able to take in input shapes of arbitrary height and width, much larger than the 
  
    
      
        1
        ×
        32
        ×
        32
      
    
    {\displaystyle 1\times 32\times 32}
  
 that the network is trained on. In those cases, the output shape of C5 would be larger. Similarly, the output of F6 would also be larger than 
  
    
      
        84
        ×
        1
        ×
        1
      
    
    {\displaystyle 84\times 1\times 1}
  
. Indeed, in modern language, the layer F6 is better described as a 
  
    
      
        1
        ×
        1
      
    
    {\displaystyle 1\times 1}
  
 convolution.
The output of the convolutional part of the network has 84 neurons, and this is not a coincidence. It was designed such, because 84 = 7×12, meaning that the output of the network can be viewed as a small 7×12 grayscale image.
The output layer has RBF units, similar to RBF networks. Each of the 10 units has 84 parameters, which might either be hand-designed and fixed, or trained. When hand-designed, it was designed so that, when viewed as a 7×12 grayscale image, it looks like the digit to be recognized.
1998 LeNet was trained with stochastic Levenberg–Marquardt algorithm with diagonal approximation of the Hessian. It was trained for about 20 epoches over MNIST. It took 2 to 3 days of CPU time on a Silicon Graphics Origin 2000 server, using a single 200 MHz R10000 processor.


=== LeNet7 ===
A certain "LeNet7" was mentioned in 2005. It was benchmarked on the NYU Object Recognition Benchmark (NORB) as being superior to SVM. It had 90,857 trainable parameters and 4.66 million connections. One forward pass requires 3,896,920 multiply-adds. It was trained by the same method as 1998 LeNet, for about 250,000 parameter updates.


== Application ==
Recognizing simple digit images is the most classic application of LeNet as it was created because of that. After the development of 1989 LeNet, as a demonstration for real-time application, they loaded the neural network into a AT&T DSP-32C digital signal processor with a peak performance of 12.5 million multiply-add operations per second. It could normalize-and-classify 10 digits a second, or classify 30 normalized digits a second. Shortly afterwards, the research group started working with a development group and a product group at NCR (acquired by AT&T in 1991). It resulted in ATM machines that could read the numerical amounts on checks using a LeNet loaded on DSP-32C.
Later, NCR deployed a similar system in large cheque reading machines in bank back offices since June 1996, and as of 2001, it was estimated to read 20 million checks a day, or 10% of all the checks in the US. It was a "graph transformer", with a main component being the LeNet as reported in 1998 with ~60000 trainable parameters. According to a draft report, the system was called HCAR50 (Holmdel Courtesy Amount Reader). There were two previous versions, HCAR30 and HCAR40.


== Subsequent work ==
The LeNet-5 means the emergence of CNN and defines the basic components of CNN. But it was not popular at that time because of the lack of hardware, especially GPUs, and since other algorithms, such as SVM, could achieve similar effects or even exceed LeNet.
Since the success of AlexNet in 2012, CNN has become the best choice for computer vision applications and many different types of CNN have been created, such as the R-CNN series. Nowadays, CNN models are quite different from LeNet, but they are all developed on the basis of LeNet.
A three-layer tree architecture imitating LeNet-5 and consisting of only one convolutional layer, has achieved a similar success rate on the CIFAR-10 dataset.
Increasing the number of filters for the LeNet architecture results in a power law decay of the error rate. These results indicate that a shallow network can achieve the same performance as deep learning architectures.


== References ==


== External links ==
LeNet-5, convolutional neural networks. An online project page for LeNet maintained by Yann LeCun, containing animations and bibliography.
projects:lush [leon.bottou.org]. Lush, an object-oriented programming language. It contains SN, a neural network simulator. The LeNet series was written in SN.