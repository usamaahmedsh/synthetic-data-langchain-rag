In futurology, a singleton is a hypothetical world order in which there is a single decision-making agency at the highest level, capable of exerting effective control over its domain, and permanently preventing both internal and external threats to its supremacy. The term was first defined by Nick Bostrom.


== Overview ==
According to Nick Bostrom, a singleton is an abstract concept that could be implemented in various ways:

a singleton could be democracy, a tyranny, a single dominant AI, a strong set of global norms that include effective provisions for their own enforcement, or even an alien overlord—its defining characteristic being simply that it is some form of agency that can solve all major global coordination problems. It may, but need not, resemble any familiar form of human governance.
Bostrom argues that a superintelligence could form a singleton. Technologies for surveillance and mind control could also facilitate the creation of a singleton.
A singleton has both potential risks and potential benefits. Notably, a suitable singleton could solve world coordination problems that would not otherwise be solvable, opening up otherwise unavailable developmental trajectories for civilization. For example, Ben Goertzel, an AGI researcher, suggests humans may instead decide to create an "AI Nanny" with "mildly superhuman intelligence and surveillance powers", to protect the human race from existential risks like nanotechnology and to delay the development of other (unfriendly) artificial intelligences until and unless the safety issues are solved. A singleton could set "very strict limitations on its own exercise of power (e.g. punctiliously confining itself to ensuring that certain treaty-specified international rules—or libertarian principles—are respected)". Furthermore, Bostrom suggests that a singleton could hold Darwinian evolutionary pressures in check, preventing agents interested only in reproduction from coming to dominate.
Yet Bostrom also regards the possibility of a stable, repressive, totalitarian global regime as a serious existential risk. The very stability of a singleton makes the installation of a bad singleton especially catastrophic, since the consequences can never be undone. Bryan Caplan writes that "perhaps an eternity of totalitarianism would be worse than extinction".
Similarly Hans Morgenthau stressed that the mechanical development of weapons, transportation, and communication makes "the conquest of the world technically possible, and they make it technically possible to keep the world in that conquered state". Its lack was the reason why great ancient empires, though vast, failed to complete universal conquest of their world and perpetuate the conquest. Now, however, this is possible. Technology undoes both geographic and climatic barriers. "Today no technological obstacle stands in the way of a world-wide empire", as "modern technology makes it possible to extend the control of mind and action to every corner of the globe regardless of geography and season."


== See also ==
AI takeover
Singularity
Accelerationism
Existential risk
Friendly artificial intelligence
Superintelligence
Superpower


== References ==