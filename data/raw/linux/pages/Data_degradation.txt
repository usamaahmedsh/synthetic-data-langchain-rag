Data degradation is the gradual corruption of computer data due to an accumulation of non-critical failures in a data storage device. It is also referred to as data decay, data rot, digital decay, or bit rot. This results in a decline in data quality over time, even when the data is not being utilized. 


== Manifestations ==


=== Primary storages ===
Data degradation in dynamic random-access memory (DRAM) can occur when the electric charge of a bit in DRAM disperses, possibly altering program code or stored data. DRAM may be altered by cosmic rays or other high-energy particles. Such data degradation is known as a soft error. ECC memory can be used to mitigate this type of data degradation.


=== Secondary storages ===
Data degradation results from the gradual decay of storage media over the course of years or longer. Causes vary by medium.


==== Solid-state media ====
EPROMs, flash memory and other solid-state drive store data using electrical charges, which can slowly leak away due to imperfect insulation. Modern flash controller chips account for this leak by trying several lower threshold voltages (until ECC passes), prolonging the age of data. Multi-level cells with much lower distance between voltage levels cannot be considered stable without this functionality.
The chip itself is not affected by this, so reprogramming it approximately once per decade prevents decay. An undamaged copy of the master data is required for the reprogramming. A checksum can be used to assure that the on-chip data is not yet damaged and ready for reprogramming.
The typical SD card, USB stick and M.2 NVMe all have a limited endurance. Power on can usually recover data but error rates will eventually degrade the media to illegibility. Writing zeros to a degraded NAND device can revive the storage to close to new condition for further use. Refresh cycles should be no longer than 6 months to  be sure the device is legible.


==== Magnetic media ====
Magnetic media, such as hard disk drives, floppy disks and magnetic tapes, may experience data decay as bits lose their magnetic orientation. Higher temperature speeds up the rate of magnetic loss. As with solid-state media, re-writing is useful as long as the medium itself is not damaged (see below). Modern hard drives use Giant magnetoresistance and have a higher magnetic lifespan on the order of decades. They also automatically correct any errors detected by ECC through rewriting. The reliance on a servowriter can complicate data recovery if it becomes unrecoverable, however.
Floppy disks and tapes are poorly protected against ambient air. In warm/humid conditions, they are prone to the physical decomposition of the storage medium.


==== Optical media ====
Optical media such as CD-R, DVD-R and BD-R, may experience data decay from the breakdown of the storage medium. This can be mitigated by storing discs in a dark, cool, low humidity location. "Archival quality" discs are available with an extended lifetime, but are still not permanent. However, data integrity scanning that measures the rates of various types of errors is able to predict data decay on optical media well ahead of uncorrectable data loss occurring.
Both the disc dye and the disc backing layer are potentially susceptible to breakdown. Early cyanine-based dyes used in CD-R were notorious for their lack of UV stability. Early CDs also suffered from CD bronzing, and is related to a combination of bad lacquer material and failure of the aluminum reflection layer. Later discs use more stable dyes or forgo them for an inorganic mixture. The aluminum layer is also commonly swapped out for gold or silver alloy.


==== Paper media ====
Paper media, such as punched cards and punched tape, may literally rot. Mylar punched tape is another approach that does not rely on electromagnetic stability. Degradation of books and printing paper is primarily driven by acid hydrolysis of glycosidic bonds within the cellulose molecule as well as by oxidation; degradation of paper is accelerated by high relative humidity, high temperature, as well as by exposure to acids, oxygen, light, and various pollutants, including various volatile organic compounds and nitrogen dioxide.


==== Streaming media ====
Data degradation occurs in streaming media transmission, causing data quality issues.


=== Example ===
One manifestation of data degradation is when one or a few bits are randomly flipped over a long period of time. This is illustrated by several digital images below, all consisting of 326,272 bits. The original photo is displayed first. In the next image, a single bit was changed from 0 to 1. In the next two images, two and three bits were flipped. On Linux systems, the binary difference between files can be revealed using the cmp command (e.g. cmp -b bitrot-original.jpg bitrot-1bit-changed.jpg).


== Causes ==
This deterioration can be caused by a variety of factors that impact the reliability and integrity of digital information, including physical factors, software errors, security breaches, human error, obsolete technology, and unauthorized access incidents.
Most disk, disk controller and higher-level systems are subject to a slight chance of unrecoverable failure. With ever-growing disk capacities, file sizes, and increases in the amount of data stored on a disk, the likelihood of the occurrence of data decay and other forms of uncorrected and undetected data corruption increases.
Low-level disk controllers typically employ error correction codes (ECC) to correct erroneous data.
Higher-level software systems may be employed to mitigate the risk of such underlying failures by increasing redundancy and implementing integrity checking, error correction codes and self-repairing algorithms. The ZFS file system was designed to address many of these data corruption issues. The Btrfs file system also includes data protection and recovery mechanisms, as does ReFS.


== Mitigation ==
There is no solution that completely eliminates the threat of data degradation, but various measures exist that can stave it off. One of these is to replicate the data as backups. Both the original and backed data are then audited for any faults due to storage media errors by checksumming the data or comparing it with that of other copies. This is the only way to detect latent faults proactively, which might otherwise go unnoticed until the data is actually accessed. Current storage systems such as those based on RAID already employ such measures internally. Ideally, and especially for data that must be preserved digitally, the replicas should be distributed across multiple administrative sites that function autonomously and deploy various hardware and software, increasing resistance to failure, as well as human error and cyberattacks.


== See also ==


== References ==


== Sources ==
Baker, Mary; Keeton, Kimberly; Martin, Sean (30 June 2005). Why Traditional Storage Systems Don't Help Us Save Stuff Forever (PDF). HotDep'05: Proceedings of the First conference on Hot topics in system dependability. USENIX. Archived from the original (PDF) on 7 September 2006. Retrieved 15 February 2025.
Baker, Mary; Shah, Mehul; Rosenthal, David S. H.; Roussopoulos, Mema; Maniatis, Petros; Giuli, TJ; Bungale, Prashanth (18 April 2006). A fresh look at the reliability of long-term digital storage. EuroSys '06: Proceedings of the 1st ACM SIGOPS/EuroSys European Conference on Computer Systems 2006. Association for Computing Machinery. pp. 221–234. doi:10.1145/1217935.1217957.
Rosenthal, David S. H. (November 2010). "Keeping Bits safe: how hard can it Be?". Communications of the ACM. 53 (11): 47–55. doi:10.1145/1839676.1839692.