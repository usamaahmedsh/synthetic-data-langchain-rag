Common Crawl is a nonprofit 501(c)(3) organization that crawls the web and freely provides its archives and datasets to the public. Common Crawl's web archive consists of petabytes of data collected since 2008. It completes crawls approximately once a month.
Common Crawl was founded by Gil Elbaz. Advisors to the non-profit include Peter Norvig and Joi Ito. It is funded by the Elbaz Family Foundation Trust and significant donations from the AI industry. The organization's crawlers respect nofollow and robots.txt policies. Open source code for processing Common Crawl's data set is publicly available.
The Common Crawl dataset includes copyrighted work and is distributed from the US under fair use claims. Researchers in other countries have made use of techniques such as shuffling sentences or referencing the Common Crawl dataset to work around copyright law in other legal jurisdictions.
Contents archived by Common Crawl are mirrored and made available online in the Wayback Machine. They are used by researchers, as well as AI companies to train large language models.
English is the primary language for 46% of documents in the March 2023 version of the Common Crawl dataset. The next most common primary languages are German, Russian, Japanese, French, Spanish and Chinese, each with less than 6% of documents.
In November 2025, an investigation by The Atlantic revealed that Common Crawl lied when it claimed it respected paywalls in its scraping and requests from publishers to have their content removed from its databases.


== History ==
Amazon Web Services began hosting Common Crawl's archive through its Public Data Sets program in 2012.
The organization began releasing metadata files and the text output of the crawlers alongside .arc files in July 2012. Common Crawl's archives had only included .arc files previously.
In December 2012, blekko donated to Common Crawl search engine metadata blekko had gathered from crawls it conducted from February to October 2012. The donated data helped Common Crawl "improve its crawl while avoiding spam, porn and the influence of excessive SEO."
In 2013, Common Crawl began using the Apache Software Foundation's Nutch webcrawler instead of a custom crawler. Common Crawl switched from using .arc files to .warc files with its November 2013 crawl.
A filtered version of Common Crawl was used to train OpenAI's GPT-3 language model, announced in 2020. In 2023, it began receiving significant financial support from AI companies, including Anthropic and OpenAI, each of which donated $250,000.
In November 2025, an investigation by technology journalist Alex Reisner for The Atlantic revealed that Common Crawl lied when it claimed it respected paywalls in its scraping and requests from publishers to have their content removed from its databases. It included misleading results in the public search function on its website that showed no entries for websites that had requested their archives be removed, when in fact those sites were still included in its scrapes used by AI companies.


== Timeline of Common Crawl data ==

The following data have been collected from the official Common Crawl Blog
and Common Crawl's API.


== Norvig Web Data Science Award ==
In corroboration with SURFsara, Common Crawl sponsors the Norvig Web Data Science Award, a competition open to students and researchers in Benelux. The award is named for Peter Norvig who also chairs the judging committee for the award.


== Colossal Clean Crawled Corpus ==
Google's version of the Common Crawl is called the Colossal Clean Crawled Corpus, or C4 for short. It was constructed for the training of the T5 language model series in 2019. There are some concerns over copyrighted content in the C4.


== References ==


== External links ==
Official website
Common Crawl GitHub Repository with the crawler, libraries and example code